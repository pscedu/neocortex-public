{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8087d12e",
   "metadata": {},
   "source": [
    "# Neocortex: Hands-on FC-MNIST Example\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Welcome notes\n",
    "\n",
    "Welcome to this hands-on example of training a Fully Connected (FC) model for the MNIST dataset! In this exercise, we will explore the fascinating world of deep learning by building a neural network capable of recognizing handwritten digits. MNIST is a widely used dataset in the field of computer vision and serves as an excellent starting point for beginners.\n",
    "\n",
    "The objective of this exercise is to guide you through the process of constructing and training a simple FC model using Python and the TensorFlow deep learning framework running on top of the Cerebras Software stack on the Neocortex system. We will break down the example step by step, ensuring that you gain a clear understanding of the underlying concepts and techniques.\n",
    "\n",
    "By the end of this hands-on example, you will have a trained FC model on a Cerebras CS-2 machine that can accurately classify handwritten digits from the MNIST dataset. You will also gain valuable insights into the fundamentals of deep learning, including model architecture, training data preparation, loss functions, and optimization algorithms.\n",
    "\n",
    "Whether you are new to the Neocortex system or looking to reinforce your knowledge, this exercise will provide you with a solid foundation to explore more advanced implementations using your custom model and dataset. So, let's dive in and embark on this exciting journey of training an FC-MNIST model on the Neocortex system!\n",
    "\n",
    "### Training Description\n",
    "#### Model used\n",
    "Fully Connected (FC) model. Neural Network where each neuron in one layer is connected to every neuron in the next layer, enabling complex pattern recognition and decision-making.\n",
    "\n",
    "<img src=\"img/fcnn.png\" width=\"30%\" alt/>\n",
    "<em>Fully Connected Neural Network</em>\n",
    "\n",
    "#### Dataset used\n",
    "MNIST dataset. Is a collection of 70,000 handwritten digits (0 through 9) widely used in the field of image recognition tasks (Modified National Institute of Standards and Technology database).\n",
    "\n",
    "<img src=\"img/mnist.png\" width=\"30%\" alt/>\n",
    "<em>MNIST dataset</em>\n",
    "\n",
    "#### Training task used: \n",
    "* batch_size: 256\n",
    "* max_steps: 100.000\n",
    "* save_checkpoints_steps: 10.000\n",
    "* keep_checkpoint_max: 2\n",
    "\n",
    "### Neocortex\n",
    "\n",
    "[Neocortex](https://www.cmu.edu/psc/aibd/neocortex/) is a highly innovative resource that targets the acceleration of AI-powered scientific discovery by vastly shortening the time required for deep learning training, featuring two [Cerebras CS-2](https://www.cerebras.net/product-system/) systems and an [HPE Superdome Flex HPC server](https://buy.hpe.com/ca/en/compute/mission-critical-x86-servers/superdome-flex-servers/superdome-flex-server/hpe-superdome-flex-280-server/p/1012865453) (SDF) robustly provisioned to drive the CS-2 systems simultaneously at maximum speed and support the complementary requirements of AI and HPDA workflows.\n",
    "\n",
    "There are four types of applications currently supported on the system, divided into the following individual tracks:\n",
    "\n",
    "* **Track 1**, [Cerebras modelzoo ML models](https://portal.neocortex.psc.edu/docs/supported-applications/track1.html): models already present in version R1.6.0 of the Cerebras modelzoo ML models software.\n",
    "* **Track 2**, [Models similar to the Cerebras modelzoo models](https://portal.neocortex.psc.edu/docs/supported-applications/track2.html): a combination of the building blocks used by modelzoo models and/or the layers supported by Cerebras as listed in their documentation.\n",
    "* **Track 3**, [General purpose SDK](https://portal.neocortex.psc.edu/docs/supported-applications/track3.html): a general purpose SDK that can be used for a variety of things. This track requires you to write low-level code, similar to writing CUDA, for implementing your research.\n",
    "* **Track 4**, [WFA, WSE Field-equation API](https://portal.neocortex.psc.edu/docs/supported-applications/track4.html): for field equations, includes ML inference. This API was recently used for advancing CFP simulations at unprecedented resolution and speed ([more info](https://www.cmu.edu/psc/aibd/neocortex/2023-02-netl-psc-pioneer-first-ever-computational-fluid-dynamics-simulation-on-cerebras-wse.html)).\n",
    "This document is expected to serve as an example of how to train a (Track 1) Cerebras modelzoo ML FC-MNIST model example from scratch. \n",
    "\n",
    "This document is under continuous development. If you have any recommendations for this document, please make sure to share them with the team (see the [Feedback](https://portal.neocortex.psc.edu/docs/providing-feedback.html) page).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38396905",
   "metadata": {},
   "source": [
    "## Setup and Requirements\n",
    "\n",
    "To follow along with this hands-on tutorial on training an FC-MNIST model, you will need the following setup and requirements:\n",
    "\n",
    "1. <u>A PSC account to access the Neocortex system</u>. You should have gotten an email requesting you to create/provide a valid PSC account.\n",
    "2. <u>An SSH terminal client</u>.\n",
    "3. <u>A web browser</u>.\n",
    "4. A development environment with all of the Cerebras software stack libraries. This is already present in the Neocortex system. You don't need to download it separately.\n",
    "5. The Cerebras modelzoo repository using the 1.6.0 release tag (R_1.6.0). This repository will be downloaded as part of the tutorial. You don't need to download it separately.\n",
    "6. MNIST Dataset: The tutorial utilizes the MNIST dataset, which consists of a large collection of handwritten digit images. Fortunately, both TensorFlow and PyTorch provide convenient functions to automatically download and load the MNIST dataset. You don't need to download it separately.\n",
    "\n",
    "With these requirements in place, you are all set to start this tutorial.\n",
    "\n",
    "## Expected Steps\n",
    "\n",
    "The training is composed of different stages. We will be performing the following tasks:\n",
    "\n",
    "1. Define all of the helper variables and commands used across the tutorial steps. This way we can reuse code and focus in the logic behind the steps, i.e. setting the paths to access the Cerebras software stack.\n",
    "2. Procure the Cerebras modelzoo repository. This repository contains the example code we will be using.\n",
    "3. Navigate to the FC-MNIST code location inside the Cerebras modelzoo repository.\n",
    "4. Precompile the code, using Cerebras tools to validate everything looks good code-wise.\n",
    "5. Compile the code. This will generate the executable to use.\n",
    "6. Train the model using the generated executable.\n",
    "\n",
    "Here is a simple flow of the expected steps:\n",
    "\n",
    "```\n",
    "Set helper variables -> Get example code -> Change to FC-MNIST dir -> Validate -> Compile -> Train model\n",
    "```\n",
    "\n",
    "## Step 1: Set helper variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b54d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder path to the Cerebras directory\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "# Create a temporary directory and capture the path\n",
    "tmp_dir = subprocess.check_output(\"mktemp -d\", shell=True).decode().strip()\n",
    "\n",
    "account_id = os.environ.get(\"SLURM_JOB_ACCOUNT\")  # Project allocation to use. The `projects` command shows your projects. i.e. tra250009p\n",
    "username = os.environ.get(\"SLURM_JOB_USER\")\n",
    "\n",
    "# Set Cerebras-related environment variables, such as the base directory containing the development environment\n",
    "local_dir = os.environ.get(\"LOCAL\")\n",
    "cerebras_dir = f\"{local_dir}/cerebras\"\n",
    "os.environ[\"CEREBRAS_DIR\"] = cerebras_dir\n",
    "os.environ['CEREBRAS_CONTAINER'] = f\"{cerebras_dir}/cbcore_latest.sif\"\n",
    "\n",
    "# Set your individual code environment variables, such as the directory to be used for running the compilation\n",
    "project_path = tmp_dir\n",
    "os.environ[\"PROJECT\"] = tmp_dir\n",
    "os.environ[\"YOUR_ENTRY_SCRIPT_LOCATION\"] = f\"{project_path}/modelzoo/modelzoo/fc_mnist/tf\"\n",
    "your_entry_script_location = os.environ[\"YOUR_ENTRY_SCRIPT_LOCATION\"]\n",
    "os.environ['BIND_LOCATIONS'] = f\"/local1/cerebras/data,/local2/cerebras/data,/local3/cerebras/data,/local4/cerebras/data,{project_path}\"\n",
    "\n",
    "# Set Slurm-related environment variables and command arguments to use for running this example\n",
    "os.environ['SLURM_GRES_ARGUMENT'] = \"--gres=cs:cerebras:1\"\n",
    "os.environ['SLURM_ARGUMENTS'] = f\"--ntasks=7 --time=0-00:15 --cpus-per-task=28 --account={account_id}\"\n",
    "\n",
    "# Define a method we will use to get some required arguments for the model training.\n",
    "def set_cs_ip_addr_value():\n",
    "    \"\"\"\n",
    "    Runs a SLURM command to retrieve the CS IP address and compute node ID to use, and sets them as environment\n",
    "    variables in the system.\n",
    "    \"\"\"\n",
    "    # Run a job while requesting a CS machine, get the assigned value for the CS_IP_ADDR environment variable.\n",
    "    cs_ip_addr_output = !salloc ${SLURM_GRES_ARGUMENT} ${SLURM_ARGUMENTS} --ntasks=1 srun /bin/bash -c set -o posix | grep CS_IP_ADDR\n",
    "    cs_ip_addr_output = [item for item in cs_ip_addr_output if item.startswith(\"CS_IP_ADDR\")]\n",
    "    os.environ[\"CS_IP_ADDR\"] = cs_ip_addr_output[0].split(\"=\")[1]\n",
    "    cs_ip_addr = os.environ[\"CS_IP_ADDR\"]\n",
    "    \n",
    "    # Execute sacct to figure out the compute node is (the SDF partition) assigned to driving that specific CS machine\n",
    "    node_id_output = !sacct --allocations --format=NodeList,AllocTRES --state=COMPLETED --parsable2 --starttime=now-1hours --endtime=now | grep \"gres/cs=1\" | tail --lines 1\n",
    "    print(node_id_output)\n",
    "    os.environ[\"NODE_ID\"] = node_id_output[0].split(\"|\")[0]\n",
    "    node_id = os.environ[\"NODE_ID\"]\n",
    "    \n",
    "    print(f\"The CS_IP_ADDR ({cs_ip_addr}) and NODE_ID ({node_id}) environment variables have been set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7a36e",
   "metadata": {},
   "source": [
    "## Step 2: Get the example code\n",
    "### Procure the Cerebras modelzoo examples repository\n",
    "\n",
    "The [Cerebras Model Zoo GitHub repository](https://github.com/Cerebras/modelzoo/tree/R_1.6.0) is public and contains examples of common deep learning models that can be trained on Cerebras hardware.\n",
    "\n",
    "Please clone the repository and then check out the R_1.6.0 tag (the current version running on Neocortex system) using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a59adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmp.VT9bug2LBf/modelzoo'...\n",
      "remote: Enumerating objects: 5129, done.\u001b[K\n",
      "remote: Counting objects: 100% (411/411), done.\u001b[K\n",
      "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
      "remote: Total 5129 (delta 255), reused 189 (delta 176), pack-reused 4718 (from 2)\u001b[K\n",
      "Receiving objects: 100% (5129/5129), 25.20 MiB | 38.74 MiB/s, done.\n",
      "Resolving deltas: 100% (3284/3284), done.\n",
      "Note: checking out 'tags/R_1.6.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b new_branch_name\n",
      "\n",
      "HEAD is now at 886a438... R_1.6.0\n",
      "\n",
      "OK: The reference modelzoo folder has been cloned into the /tmp/tmp.VT9bug2LBf directory.\n",
      "total 72K\n",
      "   0 drwxr-xr-x 5 julian pscstaff  226 Apr  9 12:56 .\n",
      "   0 drwx------ 3 julian pscstaff   22 Apr  9 12:56 ..\n",
      "   0 drwxr-xr-x 8 julian pscstaff  163 Apr  9 12:56 .git\n",
      "4.0K -rw-r--r-- 1 julian pscstaff   94 Apr  9 12:56 .gitignore\n",
      " 12K -rw-r--r-- 1 julian pscstaff  12K Apr  9 12:56 LICENSE\n",
      "   0 drwxr-xr-x 6 julian pscstaff   87 Apr  9 12:56 modelzoo\n",
      "8.0K -rw-r--r-- 1 julian pscstaff 4.5K Apr  9 12:56 PYTHON-SETUP.md\n",
      " 12K -rw-r--r-- 1 julian pscstaff 8.2K Apr  9 12:56 README.md\n",
      " 28K -rw-r--r-- 1 julian pscstaff  27K Apr  9 12:56 RELEASE-NOTES.md\n",
      "4.0K -rw-r--r-- 1 julian pscstaff 1.2K Apr  9 12:56 requirements_pytorch_gpu.txt\n",
      "4.0K -rw-r--r-- 1 julian pscstaff 1.3K Apr  9 12:56 requirements_tensorflow_gpu.txt\n",
      "   0 drwxr-xr-x 2 julian pscstaff   82 Apr  9 12:56 user_scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Check if the repository already exists\n",
    "repository_exists = os.path.isdir(f\"{project_path}/modelzoo\")\n",
    "\n",
    "if repository_exists:\n",
    "    !rm -rf {project_path}/modelzoo\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/Cerebras/modelzoo.git {project_path}/modelzoo\n",
    "\n",
    "# Change to the repository directory\n",
    "os.chdir(f\"{project_path}/modelzoo\")\n",
    "\n",
    "# Checkout the specific tag\n",
    "!git checkout tags/R_1.6.0\n",
    "\n",
    "# Confirm the operations\n",
    "print(f\"\\nOK: The reference modelzoo folder has been cloned into the {project_path} directory.\")\n",
    "\n",
    "# List the contents of the directory\n",
    "!ls -lash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363b026",
   "metadata": {},
   "source": [
    "---\n",
    "Example of the expected output (final lines):\n",
    "\n",
    "    Cloning into '/ocean/projects/ACCOUNT_ID/USERNAME/modelzoo'...\n",
    "    [--- OUTPUT SNIPPED FOR KEEPING THIS EXAMPLE SHORT ---]\n",
    "    HEAD is now at 886a438... R_1.6.0\n",
    "\n",
    "    OK: The reference modelzoo folder has been cloned into the /ocean/projects/ACCOUNT_ID/USERNAME directory.\n",
    "---\n",
    "\n",
    "You should have the freshly checked-out folder now and it should have a modelzoo subdirectory inside as well as some other files required for running the examples. See it pointed out in the following output:\n",
    "\n",
    "    total 92K\n",
    "    4.0K drwxr-xr-x 8 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 .git\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID   94 Jun  13 12:29 .gitignore\n",
    "     12K -rw-r--r-- 1 USERNAME ACCOUNT_ID  12K Jun  13 12:29 LICENSE\n",
    "    4.0K drwxr-xr-x 6 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 modelzoo  # <- This is the one we will be using\n",
    "    8.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 4.5K Jun  13 12:29 PYTHON-SETUP.md\n",
    "     12K -rw-r--r-- 1 USERNAME ACCOUNT_ID 8.2K Jun  13 12:29 README.md\n",
    "     28K -rw-r--r-- 1 USERNAME ACCOUNT_ID  27K Jun  13 12:29 RELEASE-NOTES.md\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.2K Jun  13 12:29 requirements_pytorch_gpu.txt\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.3K Jun  13 12:29 requirements_tensorflow_gpu.txt\n",
    "    4.0K drwxr-xr-x 2 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 user_scripts\n",
    "    \n",
    "You can check if the contents of that directory look the same by running the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee5572",
   "metadata": {},
   "source": [
    "## Step 3: Change into the FC-MIST Example folder\n",
    "\n",
    "<img src=\"img/step3.png\" width=\"30%\" alt/>\n",
    "\n",
    "We will now change directories to the `modelzoo/fc_mnist/tf` folder for running the FC-MNIST example. We need to do it as that location has the entry script and all of the code for running the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2db49be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp.VT9bug2LBf/modelzoo/modelzoo/fc_mnist/tf\n",
      "total 44K\r\n",
      "   0 drwxr-xr-x 3 julian pscstaff  165 Apr  9 12:56 .\r\n",
      "   0 drwxr-xr-x 5 julian pscstaff   64 Apr  9 12:56 ..\r\n",
      "   0 drwxr-xr-x 2 julian pscstaff   25 Apr  9 12:56 configs\r\n",
      "4.0K -rw-r--r-- 1 julian pscstaff 2.6K Apr  9 12:56 data.py\r\n",
      "   0 -rw-r--r-- 1 julian pscstaff    0 Apr  9 12:56 __init__.py\r\n",
      "8.0K -rw-r--r-- 1 julian pscstaff 4.3K Apr  9 12:56 model.py\r\n",
      "4.0K -rw-r--r-- 1 julian pscstaff 1.2K Apr  9 12:56 prepare_data.py\r\n",
      "8.0K -rw-r--r-- 1 julian pscstaff 6.4K Apr  9 12:56 README.md\r\n",
      "4.0K -rw-r--r-- 1 julian pscstaff 4.0K Apr  9 12:56 run-appliance.py\r\n",
      " 12K -rw-r--r-- 1 julian pscstaff 8.5K Apr  9 12:56 run.py\r\n",
      "4.0K -rw-r--r-- 1 julian pscstaff 1.5K Apr  9 12:56 utils.py\r\n",
      "\r\n",
      "OK: Successfully listed directories in the FC-MNIST example folder\r\n"
     ]
    }
   ],
   "source": [
    "%cd \"$your_entry_script_location\"\n",
    "!ls -lash && echo -e \"\\nOK: Successfully listed directories in the FC-MNIST example folder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210b971",
   "metadata": {},
   "source": [
    "## Structure of the code\n",
    "\n",
    "The following is the base structure that Cerebras uses for their code (the code template). If you want to run your research on their system, the suggested way to add your model and dataset is to take one of these base examples and start changing specific components.\n",
    "\n",
    "The files for this specific TensorFlow FC-MNIST directory we just switched to should look like this:\n",
    "\n",
    "    /ocean/projects/ACCOUNT_ID/USERNAME/modelzoo/modelzoo/fc_mnist/tf\n",
    "    total 56K\n",
    "    4.0K drwxr-xr-x 2 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 configs\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 2.6K Jun  13 12:29 data.py\n",
    "       0 -rw-r--r-- 1 USERNAME ACCOUNT_ID    0 Jun  13 12:29 __init__.py\n",
    "    8.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 4.3K Jun  13 12:29 model.py\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.2K Jun  13 12:29 prepare_data.py\n",
    "    8.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 6.4K Jun  13 12:29 README.md\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 run-appliance.py\n",
    "     12K -rw-r--r-- 1 USERNAME ACCOUNT_ID 8.5K Jun  13 12:29 run.py\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.5K Jun  13 12:29 utils.py\n",
    "\n",
    "Let's go over the main files in this location:\n",
    "\n",
    "* **configs/params.yaml:** YAML file containing the model configuration and the training hyperparameter settings.\n",
    "* **data.py:** where the input data pipeline is called. Additional data processor modules may be defined elsewhere (e.g., in the input folder) for data pipeline implementation.\n",
    "* **model.py:** It contains the model function definition. For information about the layers supported, please visit the Cerebras Documentation.\n",
    "* **run.py:** it contains the training/compilation/evaluation script.\n",
    "* **utils.py:** it contains the helper scripts.\n",
    "\n",
    "If you would like to modify an example to integrate your code, model, or dataset, the suggested order of files to modify is: **model.py** > **data.py** > **utils.py** > **configs/params.yaml** > **run.py**\n",
    "\n",
    "Additionally, the following diagram shows the suggested order in which the modifications should be performed for porting the code. The arrows represent the suggested order for the modification process to perform. This diagram should be read from left to right.\n",
    "\n",
    "![](img/code_migration_workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d26e71",
   "metadata": {},
   "source": [
    "Since the Cerebras modelzoo examples are all ready to be executed, we will not make any modifications for this example as we are now ready to start running it. \n",
    "\n",
    "The execution will happen over three different steps: validate, compile, and train.\n",
    "\n",
    "1. **validate**: this step runs a fast verification (on CPU), running a light-weight compilation up to performing kernel library matching, helping you determine if you are using any TensorFlow layer or functionality that is unsupported by the Cerebras development stack.\n",
    "<br/>The argument used for running this step is `--mode train --validate_only`.\n",
    "\n",
    "2. **compile**: this steps runs the full compilation (on CPU) through all stages of the Cerebras software stack to generate a CS system executable. When the above compilation is successful, the model is guaranteed to run on the CS system. <br/>The argument used for running this step is `--mode train --compile_only`.\n",
    "\n",
    "3. **train**: this step runs the actual training job on the CS system using the compiled executable.\n",
    "<br/>The argument used for running this step is simply `--mode train` (without additional \"`_only`\" arguments).\n",
    "\n",
    "For more information, please visit the [Cerebras TensorFlow Quickstart Documentation v1.6.0](https://docs.cerebras.net/en/1.6.0/getting-started/cs-tf-quickstart.html) page.\n",
    "\n",
    "## Running the example\n",
    "\n",
    "As the development environment uses custom Cerebras libraries and it would need to be configured beforehand, Cerebras kindly offers a pre-made container with everything preconfigured and ready to execute the Cerebras modelzoo examples.\n",
    "\n",
    "The actual example code is then executed from that [Singularity/Apptainer](https://apptainer.org/) container, and multi-user access to the CS systems is handled by Slurm.\n",
    "\n",
    "The actual validation, compilation and training commands can be found below.\n",
    "\n",
    "### Step 4: Validate de code\n",
    "\n",
    "Run the code validation using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d9f388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salloc: Granted job allocation 320727\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {}\n",
      "INFO:root:Running None on CS-2\n",
      "INFO:absl:Generating dataset mnist (./tfds/mnist/3.0.1)\n",
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ./tfds/mnist/3.0.1...\u001b[0m\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AINFO:absl:Downloading https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz into tfds/downloads/cvdf-datasets_mnist_t10k-images-idx3-ubytedDnaEPiC58ZczHNOp6ks9L4_JLids_rpvUj38kJNGMc.gz.tmp.70f56cb777744e08b49c75cc4c44e845...\n",
      "Dl Completed...:   0%|                                  | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AINFO:absl:Downloading https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz into tfds/downloads/cvdf-datasets_mnist_t10k-labels-idx1-ubyte4Mqf5UL1fRrpd5pIeeAh8c8ZzsY2gbIPBuKwiyfSD_I.gz.tmp.9969b0c72cd04bd396f37f0c81929e6c...\n",
      "Dl Completed...:   0%|                                  | 0/2 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AINFO:absl:Downloading https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz into tfds/downloads/cvdf-datasets_mnist_train-images-idx3-ubyteJAsxAi0QnOBEygBw_XW2X7zp-LBZAIqqYSHN8ru4ZO4.gz.tmp.4b23b68f330e45bb8bb1bd5c7e72ea94...\n",
      "Dl Completed...:   0%|                                  | 0/3 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AINFO:absl:Downloading https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz into tfds/downloads/cvdf-datasets_mnist_train-labels-idx1-ubytedcDWkl3FO9T-WMEH1f1Xt51eIRmePRIMAk6X147Qw8w.gz.tmp.c3c1e4540ff04b93a7922e84594c487f...\n",
      "Dl Completed...:   0%|                                  | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                  | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                  | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                  | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██████▌                   | 1/4 [00:00<00:00, 12.13 url/s]\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██████▌                   | 1/4 [00:00<00:00, 11.93 url/s]\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██████▌                   | 1/4 [00:00<00:00, 11.73 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 22.34 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 22.03 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 21.69 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                       | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|█████████████████| 2/2 [00:00<00:00, 21.69 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 15.59 url/s]\u001b[A\n",
      "Dl Size...:  11%|███▍                           | 1/9 [00:00<00:01,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 14.63 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  22%|██████▉                        | 2/9 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 13.79 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  33%|██████████▎                    | 3/9 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 12.96 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|█████████                     | 3/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 12.02 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  40%|████████████                  | 4/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 11.43 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  50%|███████████████               | 5/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 10.91 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  60%|██████████████████            | 6/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 10.46 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  70%|█████████████████████         | 7/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00, 10.03 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  80%|████████████████████████      | 8/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00,  9.48 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  90%|███████████████████████████   | 9/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00, 13.90 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  90%|███████████████████████████   | 9/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00, 13.90 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  90%|███████████████████████████   | 9/10 [00:00<00:00,  7.81 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  67%|███████████▎     | 2/3 [00:00<00:00,  9.21 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00, 13.90 url/s]\u001b[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00, 10.64 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00, 13.90 url/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00, 10.64 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00, 13.90 url/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00, 10.64 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  50%|████████▌        | 2/4 [00:00<00:00,  7.51 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00, 13.90 url/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00, 10.64 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  75%|████████████▊    | 3/4 [00:00<00:00,  9.02 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00, 13.90 url/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00, 10.64 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|█████████████████| 4/4 [00:00<00:00,  6.40 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00, 16.01 MiB/s]\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00,  6.40 url/s]\n",
      "Generating splits...:   0%|                          | 0/2 [00:00<?, ? splits/s]\n",
      "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A2025-04-09 12:57:21.017597: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2025-04-09 12:57:21.022706: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2700000000 Hz\n",
      "2025-04-09 12:57:21.022935: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8417f00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 12:57:21.022950: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\n",
      "Generating train examples...: 1 examples [00:00,  9.08 examples/s]\u001b[A\n",
      "Generating train examples...: 188 examples [00:00, 12.94 examples/s]\u001b[A\n",
      "Generating train examples...: 377 examples [00:00, 18.44 examples/s]\u001b[A\n",
      "Generating train examples...: 567 examples [00:00, 26.23 examples/s]\u001b[A\n",
      "Generating train examples...: 757 examples [00:00, 37.25 examples/s]\u001b[A\n",
      "Generating train examples...: 946 examples [00:00, 52.76 examples/s]\u001b[A\n",
      "Generating train examples...: 1136 examples [00:00, 74.48 examples/s]\u001b[A\n",
      "Generating train examples...: 1325 examples [00:00, 104.64 examples/s]\u001b[A\n",
      "Generating train examples...: 1515 examples [00:00, 146.03 examples/s]\u001b[A\n",
      "Generating train examples...: 1705 examples [00:01, 201.95 examples/s]\u001b[A\n",
      "Generating train examples...: 1895 examples [00:01, 275.90 examples/s]\u001b[A\n",
      "Generating train examples...: 2085 examples [00:01, 370.98 examples/s]\u001b[A\n",
      "Generating train examples...: 2275 examples [00:01, 488.87 examples/s]\u001b[A\n",
      "Generating train examples...: 2465 examples [00:01, 628.88 examples/s]\u001b[A\n",
      "Generating train examples...: 2655 examples [00:01, 786.38 examples/s]\u001b[A\n",
      "Generating train examples...: 2845 examples [00:01, 953.81 examples/s]\u001b[A\n",
      "Generating train examples...: 3034 examples [00:01, 1119.88 examples/s]\u001b[A\n",
      "Generating train examples...: 3225 examples [00:01, 1278.16 examples/s]\u001b[A\n",
      "Generating train examples...: 3415 examples [00:01, 1416.85 examples/s]\u001b[A\n",
      "Generating train examples...: 3605 examples [00:02, 1533.84 examples/s]\u001b[A\n",
      "Generating train examples...: 3795 examples [00:02, 1624.06 examples/s]\u001b[A\n",
      "Generating train examples...: 3985 examples [00:02, 1697.85 examples/s]\u001b[A\n",
      "Generating train examples...: 4175 examples [00:02, 1751.53 examples/s]\u001b[A\n",
      "Generating train examples...: 4365 examples [00:02, 1791.93 examples/s]\u001b[A\n",
      "Generating train examples...: 4555 examples [00:02, 1822.24 examples/s]\u001b[A\n",
      "Generating train examples...: 4745 examples [00:02, 1843.15 examples/s]\u001b[A\n",
      "Generating train examples...: 4935 examples [00:02, 1854.55 examples/s]\u001b[A\n",
      "Generating train examples...: 5125 examples [00:02, 1866.15 examples/s]\u001b[A\n",
      "Generating train examples...: 5315 examples [00:02, 1874.62 examples/s]\u001b[A\n",
      "Generating train examples...: 5505 examples [00:03, 1879.08 examples/s]\u001b[A\n",
      "Generating train examples...: 5695 examples [00:03, 1883.41 examples/s]\u001b[A\n",
      "Generating train examples...: 5885 examples [00:03, 1887.54 examples/s]\u001b[A\n",
      "Generating train examples...: 6075 examples [00:03, 1888.86 examples/s]\u001b[A\n",
      "Generating train examples...: 6265 examples [00:03, 1890.73 examples/s]\u001b[A\n",
      "Generating train examples...: 6455 examples [00:03, 1891.72 examples/s]\u001b[A\n",
      "Generating train examples...: 6645 examples [00:03, 1893.59 examples/s]\u001b[A\n",
      "Generating train examples...: 6835 examples [00:03, 1894.33 examples/s]\u001b[A\n",
      "Generating train examples...: 7026 examples [00:03, 1897.24 examples/s]\u001b[A\n",
      "Generating train examples...: 7217 examples [00:03, 1898.43 examples/s]\u001b[A\n",
      "Generating train examples...: 7408 examples [00:04, 1899.66 examples/s]\u001b[A\n",
      "Generating train examples...: 7599 examples [00:04, 1899.33 examples/s]\u001b[A\n",
      "Generating train examples...: 7790 examples [00:04, 1900.35 examples/s]\u001b[A\n",
      "Generating train examples...: 7981 examples [00:04, 1898.02 examples/s]\u001b[A\n",
      "Generating train examples...: 8171 examples [00:04, 1896.80 examples/s]\u001b[A\n",
      "Generating train examples...: 8361 examples [00:04, 1896.89 examples/s]\u001b[A\n",
      "Generating train examples...: 8551 examples [00:04, 1895.44 examples/s]\u001b[A\n",
      "Generating train examples...: 8741 examples [00:04, 1892.48 examples/s]\u001b[A\n",
      "Generating train examples...: 8931 examples [00:04, 1894.44 examples/s]\u001b[A\n",
      "Generating train examples...: 9121 examples [00:04, 1894.83 examples/s]\u001b[A\n",
      "Generating train examples...: 9311 examples [00:05, 1895.94 examples/s]\u001b[A\n",
      "Generating train examples...: 9501 examples [00:05, 1893.91 examples/s]\u001b[A\n",
      "Generating train examples...: 9692 examples [00:05, 1896.21 examples/s]\u001b[A\n",
      "Generating train examples...: 9882 examples [00:05, 1891.28 examples/s]\u001b[A\n",
      "Generating train examples...: 10072 examples [00:05, 1890.80 examples/s]\u001b[A\n",
      "Generating train examples...: 10262 examples [00:05, 1889.80 examples/s]\u001b[A\n",
      "Generating train examples...: 10451 examples [00:05, 1887.22 examples/s]\u001b[A\n",
      "Generating train examples...: 10640 examples [00:05, 1884.63 examples/s]\u001b[A\n",
      "Generating train examples...: 10830 examples [00:05, 1886.34 examples/s]\u001b[A\n",
      "Generating train examples...: 11020 examples [00:05, 1889.16 examples/s]\u001b[A\n",
      "Generating train examples...: 11209 examples [00:06, 1888.27 examples/s]\u001b[A\n",
      "Generating train examples...: 11399 examples [00:06, 1888.82 examples/s]\u001b[A\n",
      "Generating train examples...: 11588 examples [00:06, 1888.26 examples/s]\u001b[A\n",
      "Generating train examples...: 11778 examples [00:06, 1890.55 examples/s]\u001b[A\n",
      "Generating train examples...: 11968 examples [00:06, 1890.92 examples/s]\u001b[A\n",
      "Generating train examples...: 12158 examples [00:06, 1889.55 examples/s]\u001b[A\n",
      "Generating train examples...: 12347 examples [00:06, 1888.54 examples/s]\u001b[A\n",
      "Generating train examples...: 12536 examples [00:06, 1885.92 examples/s]\u001b[A\n",
      "Generating train examples...: 12725 examples [00:06, 1885.22 examples/s]\u001b[A\n",
      "Generating train examples...: 12914 examples [00:06, 1885.32 examples/s]\u001b[A\n",
      "Generating train examples...: 13103 examples [00:07, 1885.73 examples/s]\u001b[A\n",
      "Generating train examples...: 13292 examples [00:07, 1886.37 examples/s]\u001b[A\n",
      "Generating train examples...: 13481 examples [00:07, 1887.26 examples/s]\u001b[A\n",
      "Generating train examples...: 13670 examples [00:07, 1885.70 examples/s]\u001b[A\n",
      "Generating train examples...: 13860 examples [00:07, 1887.58 examples/s]\u001b[A\n",
      "Generating train examples...: 14049 examples [00:07, 1887.00 examples/s]\u001b[A\n",
      "Generating train examples...: 14238 examples [00:07, 1887.61 examples/s]\u001b[A\n",
      "Generating train examples...: 14427 examples [00:07, 1885.04 examples/s]\u001b[A\n",
      "Generating train examples...: 14618 examples [00:07, 1887.50 examples/s]\u001b[A\n",
      "Generating train examples...: 14808 examples [00:07, 1889.88 examples/s]\u001b[A\n",
      "Generating train examples...: 14998 examples [00:08, 1889.97 examples/s]\u001b[A\n",
      "Generating train examples...: 15188 examples [00:08, 1891.49 examples/s]\u001b[A\n",
      "Generating train examples...: 15378 examples [00:08, 1893.58 examples/s]\u001b[A\n",
      "Generating train examples...: 15568 examples [00:08, 1894.00 examples/s]\u001b[A\n",
      "Generating train examples...: 15758 examples [00:08, 1894.57 examples/s]\u001b[A\n",
      "Generating train examples...: 15948 examples [00:08, 1895.14 examples/s]\u001b[A\n",
      "Generating train examples...: 16139 examples [00:08, 1897.26 examples/s]\u001b[A\n",
      "Generating train examples...: 16329 examples [00:08, 1894.63 examples/s]\u001b[A\n",
      "Generating train examples...: 16519 examples [00:08, 1894.58 examples/s]\u001b[A\n",
      "Generating train examples...: 16709 examples [00:08, 1893.90 examples/s]\u001b[A\n",
      "Generating train examples...: 16899 examples [00:09, 1893.46 examples/s]\u001b[A\n",
      "Generating train examples...: 17090 examples [00:09, 1895.91 examples/s]\u001b[A\n",
      "Generating train examples...: 17280 examples [00:09, 1895.23 examples/s]\u001b[A\n",
      "Generating train examples...: 17470 examples [00:09, 1895.61 examples/s]\u001b[A\n",
      "Generating train examples...: 17660 examples [00:09, 1895.59 examples/s]\u001b[A\n",
      "Generating train examples...: 17850 examples [00:09, 1896.62 examples/s]\u001b[A\n",
      "Generating train examples...: 18041 examples [00:09, 1898.09 examples/s]\u001b[A\n",
      "Generating train examples...: 18231 examples [00:09, 1897.22 examples/s]\u001b[A\n",
      "Generating train examples...: 18422 examples [00:09, 1900.95 examples/s]\u001b[A\n",
      "Generating train examples...: 18613 examples [00:09, 1899.55 examples/s]\u001b[A\n",
      "Generating train examples...: 18803 examples [00:10, 1897.32 examples/s]\u001b[A\n",
      "Generating train examples...: 18993 examples [00:10, 1897.22 examples/s]\u001b[A\n",
      "Generating train examples...: 19184 examples [00:10, 1898.59 examples/s]\u001b[A\n",
      "Generating train examples...: 19374 examples [00:10, 1895.72 examples/s]\u001b[A\n",
      "Generating train examples...: 19564 examples [00:10, 1895.64 examples/s]\u001b[A\n",
      "Generating train examples...: 19754 examples [00:10, 1895.03 examples/s]\u001b[A\n",
      "Generating train examples...: 19944 examples [00:10, 1893.21 examples/s]\u001b[A\n",
      "Generating train examples...: 20134 examples [00:10, 1889.96 examples/s]\u001b[A\n",
      "Generating train examples...: 20324 examples [00:10, 1892.08 examples/s]\u001b[A\n",
      "Generating train examples...: 20514 examples [00:10, 1892.86 examples/s]\u001b[A\n",
      "Generating train examples...: 20704 examples [00:11, 1893.08 examples/s]\u001b[A\n",
      "Generating train examples...: 20895 examples [00:11, 1895.76 examples/s]\u001b[A\n",
      "Generating train examples...: 21085 examples [00:11, 1896.09 examples/s]\u001b[A\n",
      "Generating train examples...: 21275 examples [00:11, 1895.13 examples/s]\u001b[A\n",
      "Generating train examples...: 21465 examples [00:11, 1894.09 examples/s]\u001b[A\n",
      "Generating train examples...: 21656 examples [00:11, 1896.01 examples/s]\u001b[A\n",
      "Generating train examples...: 21846 examples [00:11, 1896.59 examples/s]\u001b[A\n",
      "Generating train examples...: 22036 examples [00:11, 1894.61 examples/s]\u001b[A\n",
      "Generating train examples...: 22226 examples [00:11, 1894.70 examples/s]\u001b[A\n",
      "Generating train examples...: 22416 examples [00:11, 1895.49 examples/s]\u001b[A\n",
      "Generating train examples...: 22606 examples [00:12, 1895.59 examples/s]\u001b[A\n",
      "Generating train examples...: 22796 examples [00:12, 1895.23 examples/s]\u001b[A\n",
      "Generating train examples...: 22986 examples [00:12, 1895.69 examples/s]\u001b[A\n",
      "Generating train examples...: 23176 examples [00:12, 1894.61 examples/s]\u001b[A\n",
      "Generating train examples...: 23366 examples [00:12, 1896.12 examples/s]\u001b[A\n",
      "Generating train examples...: 23557 examples [00:12, 1897.54 examples/s]\u001b[A\n",
      "Generating train examples...: 23747 examples [00:12, 1898.06 examples/s]\u001b[A\n",
      "Generating train examples...: 23937 examples [00:12, 1893.84 examples/s]\u001b[A\n",
      "Generating train examples...: 24127 examples [00:12, 1894.53 examples/s]\u001b[A\n",
      "Generating train examples...: 24317 examples [00:12, 1896.14 examples/s]\u001b[A\n",
      "Generating train examples...: 24508 examples [00:13, 1897.52 examples/s]\u001b[A\n",
      "Generating train examples...: 24698 examples [00:13, 1897.32 examples/s]\u001b[A\n",
      "Generating train examples...: 24889 examples [00:13, 1899.17 examples/s]\u001b[A\n",
      "Generating train examples...: 25080 examples [00:13, 1901.04 examples/s]\u001b[A\n",
      "Generating train examples...: 25271 examples [00:13, 1898.58 examples/s]\u001b[A\n",
      "Generating train examples...: 25461 examples [00:13, 1896.63 examples/s]\u001b[A\n",
      "Generating train examples...: 25651 examples [00:13, 1895.83 examples/s]\u001b[A\n",
      "Generating train examples...: 25842 examples [00:13, 1897.22 examples/s]\u001b[A\n",
      "Generating train examples...: 26033 examples [00:13, 1898.24 examples/s]\u001b[A\n",
      "Generating train examples...: 26223 examples [00:13, 1897.92 examples/s]\u001b[A\n",
      "Generating train examples...: 26414 examples [00:14, 1898.72 examples/s]\u001b[A\n",
      "Generating train examples...: 26604 examples [00:14, 1897.92 examples/s]\u001b[A\n",
      "Generating train examples...: 26794 examples [00:14, 1897.01 examples/s]\u001b[A\n",
      "Generating train examples...: 26984 examples [00:14, 1896.48 examples/s]\u001b[A\n",
      "Generating train examples...: 27174 examples [00:14, 1896.17 examples/s]\u001b[A\n",
      "Generating train examples...: 27364 examples [00:14, 1895.53 examples/s]\u001b[A\n",
      "Generating train examples...: 27554 examples [00:14, 1894.91 examples/s]\u001b[A\n",
      "Generating train examples...: 27744 examples [00:14, 1892.54 examples/s]\u001b[A\n",
      "Generating train examples...: 27934 examples [00:14, 1892.47 examples/s]\u001b[A\n",
      "Generating train examples...: 28124 examples [00:14, 1891.57 examples/s]\u001b[A\n",
      "Generating train examples...: 28314 examples [00:15, 1889.91 examples/s]\u001b[A\n",
      "Generating train examples...: 28504 examples [00:15, 1890.00 examples/s]\u001b[A\n",
      "Generating train examples...: 28694 examples [00:15, 1890.65 examples/s]\u001b[A\n",
      "Generating train examples...: 28884 examples [00:15, 1891.88 examples/s]\u001b[A\n",
      "Generating train examples...: 29074 examples [00:15, 1890.43 examples/s]\u001b[A\n",
      "Generating train examples...: 29264 examples [00:15, 1891.15 examples/s]\u001b[A\n",
      "Generating train examples...: 29454 examples [00:15, 1890.22 examples/s]\u001b[A\n",
      "Generating train examples...: 29644 examples [00:15, 1891.40 examples/s]\u001b[A\n",
      "Generating train examples...: 29834 examples [00:15, 1893.30 examples/s]\u001b[A\n",
      "Generating train examples...: 30025 examples [00:15, 1895.56 examples/s]\u001b[A\n",
      "Generating train examples...: 30215 examples [00:16, 1895.94 examples/s]\u001b[A\n",
      "Generating train examples...: 30405 examples [00:16, 1892.48 examples/s]\u001b[A\n",
      "Generating train examples...: 30595 examples [00:16, 1891.77 examples/s]\u001b[A\n",
      "Generating train examples...: 30785 examples [00:16, 1893.15 examples/s]\u001b[A\n",
      "Generating train examples...: 30975 examples [00:16, 1892.93 examples/s]\u001b[A\n",
      "Generating train examples...: 31165 examples [00:16, 1892.27 examples/s]\u001b[A\n",
      "Generating train examples...: 31355 examples [00:16, 1893.47 examples/s]\u001b[A\n",
      "Generating train examples...: 31545 examples [00:16, 1892.00 examples/s]\u001b[A\n",
      "Generating train examples...: 31735 examples [00:16, 1894.28 examples/s]\u001b[A\n",
      "Generating train examples...: 31925 examples [00:16, 1895.69 examples/s]\u001b[A\n",
      "Generating train examples...: 32115 examples [00:17, 1894.59 examples/s]\u001b[A\n",
      "Generating train examples...: 32305 examples [00:17, 1895.67 examples/s]\u001b[A\n",
      "Generating train examples...: 32495 examples [00:17, 1896.02 examples/s]\u001b[A\n",
      "Generating train examples...: 32685 examples [00:17, 1893.83 examples/s]\u001b[A\n",
      "Generating train examples...: 32875 examples [00:17, 1893.88 examples/s]\u001b[A\n",
      "Generating train examples...: 33065 examples [00:17, 1893.01 examples/s]\u001b[A\n",
      "Generating train examples...: 33256 examples [00:17, 1896.80 examples/s]\u001b[A\n",
      "Generating train examples...: 33446 examples [00:17, 1886.75 examples/s]\u001b[A\n",
      "Generating train examples...: 33636 examples [00:17, 1888.76 examples/s]\u001b[A\n",
      "Generating train examples...: 33826 examples [00:17, 1891.12 examples/s]\u001b[A\n",
      "Generating train examples...: 34016 examples [00:18, 1892.59 examples/s]\u001b[A\n",
      "Generating train examples...: 34206 examples [00:18, 1889.21 examples/s]\u001b[A\n",
      "Generating train examples...: 34395 examples [00:18, 1883.79 examples/s]\u001b[A\n",
      "Generating train examples...: 34584 examples [00:18, 1879.31 examples/s]\u001b[A\n",
      "Generating train examples...: 34772 examples [00:18, 1879.46 examples/s]\u001b[A\n",
      "Generating train examples...: 34962 examples [00:18, 1882.72 examples/s]\u001b[A\n",
      "Generating train examples...: 35152 examples [00:18, 1886.97 examples/s]\u001b[A\n",
      "Generating train examples...: 35342 examples [00:18, 1888.83 examples/s]\u001b[A\n",
      "Generating train examples...: 35532 examples [00:18, 1889.52 examples/s]\u001b[A\n",
      "Generating train examples...: 35722 examples [00:18, 1890.93 examples/s]\u001b[A\n",
      "Generating train examples...: 35912 examples [00:19, 1889.68 examples/s]\u001b[A\n",
      "Generating train examples...: 36102 examples [00:19, 1890.90 examples/s]\u001b[A\n",
      "Generating train examples...: 36292 examples [00:19, 1890.14 examples/s]\u001b[A\n",
      "Generating train examples...: 36482 examples [00:19, 1889.97 examples/s]\u001b[A\n",
      "Generating train examples...: 36671 examples [00:19, 1889.35 examples/s]\u001b[A\n",
      "Generating train examples...: 36860 examples [00:19, 1889.25 examples/s]\u001b[A\n",
      "Generating train examples...: 37050 examples [00:19, 1890.78 examples/s]\u001b[A\n",
      "Generating train examples...: 37240 examples [00:19, 1893.14 examples/s]\u001b[A\n",
      "Generating train examples...: 37430 examples [00:19, 1892.87 examples/s]\u001b[A\n",
      "Generating train examples...: 37620 examples [00:19, 1894.46 examples/s]\u001b[A\n",
      "Generating train examples...: 37810 examples [00:20, 1890.44 examples/s]\u001b[A\n",
      "Generating train examples...: 38000 examples [00:20, 1891.48 examples/s]\u001b[A\n",
      "Generating train examples...: 38190 examples [00:20, 1892.20 examples/s]\u001b[A\n",
      "Generating train examples...: 38380 examples [00:20, 1891.41 examples/s]\u001b[A\n",
      "Generating train examples...: 38571 examples [00:20, 1895.23 examples/s]\u001b[A\n",
      "Generating train examples...: 38761 examples [00:20, 1893.37 examples/s]\u001b[A\n",
      "Generating train examples...: 38951 examples [00:20, 1893.45 examples/s]\u001b[A\n",
      "Generating train examples...: 39141 examples [00:20, 1892.25 examples/s]\u001b[A\n",
      "Generating train examples...: 39331 examples [00:20, 1892.73 examples/s]\u001b[A\n",
      "Generating train examples...: 39521 examples [00:20, 1894.25 examples/s]\u001b[A\n",
      "Generating train examples...: 39711 examples [00:21, 1885.97 examples/s]\u001b[A\n",
      "Generating train examples...: 39901 examples [00:21, 1888.14 examples/s]\u001b[A\n",
      "Generating train examples...: 40091 examples [00:21, 1889.97 examples/s]\u001b[A\n",
      "Generating train examples...: 40281 examples [00:21, 1890.38 examples/s]\u001b[A\n",
      "Generating train examples...: 40471 examples [00:21, 1892.53 examples/s]\u001b[A\n",
      "Generating train examples...: 40661 examples [00:21, 1893.05 examples/s]\u001b[A\n",
      "Generating train examples...: 40851 examples [00:21, 1893.45 examples/s]\u001b[A\n",
      "Generating train examples...: 41042 examples [00:21, 1896.50 examples/s]\u001b[A\n",
      "Generating train examples...: 41232 examples [00:21, 1896.23 examples/s]\u001b[A\n",
      "Generating train examples...: 41422 examples [00:21, 1896.14 examples/s]\u001b[A\n",
      "Generating train examples...: 41612 examples [00:22, 1896.56 examples/s]\u001b[A\n",
      "Generating train examples...: 41803 examples [00:22, 1898.50 examples/s]\u001b[A\n",
      "Generating train examples...: 41993 examples [00:22, 1896.76 examples/s]\u001b[A\n",
      "Generating train examples...: 42183 examples [00:22, 1896.06 examples/s]\u001b[A\n",
      "Generating train examples...: 42373 examples [00:22, 1895.85 examples/s]\u001b[A\n",
      "Generating train examples...: 42563 examples [00:22, 1895.04 examples/s]\u001b[A\n",
      "Generating train examples...: 42753 examples [00:22, 1892.58 examples/s]\u001b[A\n",
      "Generating train examples...: 42943 examples [00:22, 1888.68 examples/s]\u001b[A\n",
      "Generating train examples...: 43134 examples [00:22, 1893.75 examples/s]\u001b[A\n",
      "Generating train examples...: 43324 examples [00:22, 1894.34 examples/s]\u001b[A\n",
      "Generating train examples...: 43514 examples [00:23, 1892.38 examples/s]\u001b[A\n",
      "Generating train examples...: 43704 examples [00:23, 1892.57 examples/s]\u001b[A\n",
      "Generating train examples...: 43894 examples [00:23, 1891.15 examples/s]\u001b[A\n",
      "Generating train examples...: 44084 examples [00:23, 1892.11 examples/s]\u001b[A\n",
      "Generating train examples...: 44274 examples [00:23, 1892.75 examples/s]\u001b[A\n",
      "Generating train examples...: 44464 examples [00:23, 1894.13 examples/s]\u001b[A\n",
      "Generating train examples...: 44654 examples [00:23, 1893.02 examples/s]\u001b[A\n",
      "Generating train examples...: 44844 examples [00:23, 1893.84 examples/s]\u001b[A\n",
      "Generating train examples...: 45034 examples [00:23, 1893.94 examples/s]\u001b[A\n",
      "Generating train examples...: 45224 examples [00:23, 1895.58 examples/s]\u001b[A\n",
      "Generating train examples...: 45414 examples [00:24, 1895.14 examples/s]\u001b[A\n",
      "Generating train examples...: 45604 examples [00:24, 1894.39 examples/s]\u001b[A\n",
      "Generating train examples...: 45794 examples [00:24, 1893.09 examples/s]\u001b[A\n",
      "Generating train examples...: 45984 examples [00:24, 1892.93 examples/s]\u001b[A\n",
      "Generating train examples...: 46175 examples [00:24, 1895.20 examples/s]\u001b[A\n",
      "Generating train examples...: 46365 examples [00:24, 1895.54 examples/s]\u001b[A\n",
      "Generating train examples...: 46555 examples [00:24, 1895.39 examples/s]\u001b[A\n",
      "Generating train examples...: 46745 examples [00:24, 1892.50 examples/s]\u001b[A\n",
      "Generating train examples...: 46935 examples [00:24, 1891.16 examples/s]\u001b[A\n",
      "Generating train examples...: 47125 examples [00:25, 1892.48 examples/s]\u001b[A\n",
      "Generating train examples...: 47315 examples [00:25, 1893.60 examples/s]\u001b[A\n",
      "Generating train examples...: 47505 examples [00:25, 1894.14 examples/s]\u001b[A\n",
      "Generating train examples...: 47695 examples [00:25, 1892.61 examples/s]\u001b[A\n",
      "Generating train examples...: 47885 examples [00:25, 1894.01 examples/s]\u001b[A\n",
      "Generating train examples...: 48075 examples [00:25, 1894.53 examples/s]\u001b[A\n",
      "Generating train examples...: 48265 examples [00:25, 1895.18 examples/s]\u001b[A\n",
      "Generating train examples...: 48455 examples [00:25, 1891.19 examples/s]\u001b[A\n",
      "Generating train examples...: 48646 examples [00:25, 1894.37 examples/s]\u001b[A\n",
      "Generating train examples...: 48836 examples [00:25, 1893.61 examples/s]\u001b[A\n",
      "Generating train examples...: 49027 examples [00:26, 1895.78 examples/s]\u001b[A\n",
      "Generating train examples...: 49217 examples [00:26, 1894.13 examples/s]\u001b[A\n",
      "Generating train examples...: 49407 examples [00:26, 1892.46 examples/s]\u001b[A\n",
      "Generating train examples...: 49597 examples [00:26, 1891.34 examples/s]\u001b[A\n",
      "Generating train examples...: 49787 examples [00:26, 1888.12 examples/s]\u001b[A\n",
      "Generating train examples...: 49977 examples [00:26, 1889.73 examples/s]\u001b[A\n",
      "Generating train examples...: 50167 examples [00:26, 1891.12 examples/s]\u001b[A\n",
      "Generating train examples...: 50357 examples [00:26, 1888.83 examples/s]\u001b[A\n",
      "Generating train examples...: 50546 examples [00:26, 1888.81 examples/s]\u001b[A\n",
      "Generating train examples...: 50736 examples [00:26, 1890.01 examples/s]\u001b[A\n",
      "Generating train examples...: 50926 examples [00:27, 1892.15 examples/s]\u001b[A\n",
      "Generating train examples...: 51116 examples [00:27, 1892.61 examples/s]\u001b[A\n",
      "Generating train examples...: 51306 examples [00:27, 1893.48 examples/s]\u001b[A\n",
      "Generating train examples...: 51496 examples [00:27, 1892.75 examples/s]\u001b[A\n",
      "Generating train examples...: 51686 examples [00:27, 1892.28 examples/s]\u001b[A\n",
      "Generating train examples...: 51876 examples [00:27, 1893.43 examples/s]\u001b[A\n",
      "Generating train examples...: 52066 examples [00:27, 1894.42 examples/s]\u001b[A\n",
      "Generating train examples...: 52256 examples [00:27, 1894.36 examples/s]\u001b[A\n",
      "Generating train examples...: 52448 examples [00:27, 1896.55 examples/s]\u001b[A\n",
      "Generating train examples...: 52639 examples [00:27, 1899.06 examples/s]\u001b[A\n",
      "Generating train examples...: 52830 examples [00:28, 1900.25 examples/s]\u001b[A\n",
      "Generating train examples...: 53021 examples [00:28, 1897.58 examples/s]\u001b[A\n",
      "Generating train examples...: 53211 examples [00:28, 1896.37 examples/s]\u001b[A\n",
      "Generating train examples...: 53401 examples [00:28, 1895.03 examples/s]\u001b[A\n",
      "Generating train examples...: 53591 examples [00:28, 1895.24 examples/s]\u001b[A\n",
      "Generating train examples...: 53781 examples [00:28, 1895.23 examples/s]\u001b[A\n",
      "Generating train examples...: 53971 examples [00:28, 1895.34 examples/s]\u001b[A\n",
      "Generating train examples...: 54161 examples [00:28, 1891.84 examples/s]\u001b[A\n",
      "Generating train examples...: 54352 examples [00:28, 1895.55 examples/s]\u001b[A\n",
      "Generating train examples...: 54542 examples [00:28, 1893.96 examples/s]\u001b[A\n",
      "Generating train examples...: 54732 examples [00:29, 1894.80 examples/s]\u001b[A\n",
      "Generating train examples...: 54922 examples [00:29, 1894.48 examples/s]\u001b[A\n",
      "Generating train examples...: 55112 examples [00:29, 1894.71 examples/s]\u001b[A\n",
      "Generating train examples...: 55302 examples [00:29, 1893.23 examples/s]\u001b[A\n",
      "Generating train examples...: 55492 examples [00:29, 1894.77 examples/s]\u001b[A\n",
      "Generating train examples...: 55682 examples [00:29, 1894.98 examples/s]\u001b[A\n",
      "Generating train examples...: 55872 examples [00:29, 1893.95 examples/s]\u001b[A\n",
      "Generating train examples...: 56062 examples [00:29, 1891.87 examples/s]\u001b[A\n",
      "Generating train examples...: 56253 examples [00:29, 1894.03 examples/s]\u001b[A\n",
      "Generating train examples...: 56444 examples [00:29, 1897.68 examples/s]\u001b[A\n",
      "Generating train examples...: 56634 examples [00:30, 1896.09 examples/s]\u001b[A\n",
      "Generating train examples...: 56824 examples [00:30, 1896.54 examples/s]\u001b[A\n",
      "Generating train examples...: 57015 examples [00:30, 1897.90 examples/s]\u001b[A\n",
      "Generating train examples...: 57205 examples [00:30, 1897.62 examples/s]\u001b[A\n",
      "Generating train examples...: 57395 examples [00:30, 1896.17 examples/s]\u001b[A\n",
      "Generating train examples...: 57585 examples [00:30, 1895.31 examples/s]\u001b[A\n",
      "Generating train examples...: 57775 examples [00:30, 1896.59 examples/s]\u001b[A\n",
      "Generating train examples...: 57965 examples [00:30, 1894.69 examples/s]\u001b[A\n",
      "Generating train examples...: 58156 examples [00:30, 1898.70 examples/s]\u001b[A\n",
      "Generating train examples...: 58346 examples [00:30, 1896.23 examples/s]\u001b[A\n",
      "Generating train examples...: 58536 examples [00:31, 1894.23 examples/s]\u001b[A\n",
      "Generating train examples...: 58727 examples [00:31, 1896.55 examples/s]\u001b[A\n",
      "Generating train examples...: 58917 examples [00:31, 1896.04 examples/s]\u001b[A\n",
      "Generating train examples...: 59107 examples [00:31, 1894.24 examples/s]\u001b[A\n",
      "Generating train examples...: 59297 examples [00:31, 1894.52 examples/s]\u001b[A\n",
      "Generating train examples...: 59487 examples [00:31, 1893.08 examples/s]\u001b[A\n",
      "Generating train examples...: 59677 examples [00:31, 1893.22 examples/s]\u001b[A\n",
      "Generating train examples...: 59867 examples [00:31, 1893.91 examples/s]\u001b[A\n",
      "                                                                        \u001b[A\n",
      "Shuffling mnist-train.tfrecord...:   0%|       | 0/60000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling mnist-train.tfrecord...:  47%|▍| 27923/60000 [00:00<00:00, 279226.27 e\u001b[A\n",
      "                                                                                \u001b[AINFO:absl:Done writing mnist-train.tfrecord. Number of examples: 60000 (shards: [60000])\n",
      "Generating splits...:  50%|█████████         | 1/2 [00:31<00:31, 31.99s/ splits]\n",
      "Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Generating test examples...: 158 examples [00:00, 1579.70 examples/s]\u001b[A\n",
      "Generating test examples...: 349 examples [00:00, 1664.54 examples/s]\u001b[A\n",
      "Generating test examples...: 539 examples [00:00, 1727.96 examples/s]\u001b[A\n",
      "Generating test examples...: 730 examples [00:00, 1776.32 examples/s]\u001b[A\n",
      "Generating test examples...: 920 examples [00:00, 1811.58 examples/s]\u001b[A\n",
      "Generating test examples...: 1111 examples [00:00, 1838.92 examples/s]\u001b[A\n",
      "Generating test examples...: 1301 examples [00:00, 1855.86 examples/s]\u001b[A\n",
      "Generating test examples...: 1490 examples [00:00, 1865.83 examples/s]\u001b[A\n",
      "Generating test examples...: 1680 examples [00:00, 1875.25 examples/s]\u001b[A\n",
      "Generating test examples...: 1871 examples [00:01, 1883.74 examples/s]\u001b[A\n",
      "Generating test examples...: 2061 examples [00:01, 1888.16 examples/s]\u001b[A\n",
      "Generating test examples...: 2251 examples [00:01, 1890.85 examples/s]\u001b[A\n",
      "Generating test examples...: 2442 examples [00:01, 1896.03 examples/s]\u001b[A\n",
      "Generating test examples...: 2633 examples [00:01, 1897.42 examples/s]\u001b[A\n",
      "Generating test examples...: 2823 examples [00:01, 1898.15 examples/s]\u001b[A\n",
      "Generating test examples...: 3014 examples [00:01, 1900.63 examples/s]\u001b[A\n",
      "Generating test examples...: 3205 examples [00:01, 1901.68 examples/s]\u001b[A\n",
      "Generating test examples...: 3395 examples [00:01, 1898.78 examples/s]\u001b[A\n",
      "Generating test examples...: 3585 examples [00:01, 1898.90 examples/s]\u001b[A\n",
      "Generating test examples...: 3776 examples [00:02, 1900.11 examples/s]\u001b[A\n",
      "Generating test examples...: 3966 examples [00:02, 1897.79 examples/s]\u001b[A\n",
      "Generating test examples...: 4157 examples [00:02, 1899.58 examples/s]\u001b[A\n",
      "Generating test examples...: 4348 examples [00:02, 1901.58 examples/s]\u001b[A\n",
      "Generating test examples...: 4539 examples [00:02, 1901.10 examples/s]\u001b[A\n",
      "Generating test examples...: 4730 examples [00:02, 1901.57 examples/s]\u001b[A\n",
      "Generating test examples...: 4921 examples [00:02, 1902.94 examples/s]\u001b[A\n",
      "Generating test examples...: 5112 examples [00:02, 1903.02 examples/s]\u001b[A\n",
      "Generating test examples...: 5303 examples [00:02, 1899.79 examples/s]\u001b[A\n",
      "Generating test examples...: 5493 examples [00:02, 1897.88 examples/s]\u001b[A\n",
      "Generating test examples...: 5684 examples [00:03, 1898.99 examples/s]\u001b[A\n",
      "Generating test examples...: 5874 examples [00:03, 1898.12 examples/s]\u001b[A\n",
      "Generating test examples...: 6065 examples [00:03, 1899.11 examples/s]\u001b[A\n",
      "Generating test examples...: 6255 examples [00:03, 1897.30 examples/s]\u001b[A\n",
      "Generating test examples...: 6445 examples [00:03, 1896.33 examples/s]\u001b[A\n",
      "Generating test examples...: 6635 examples [00:03, 1896.69 examples/s]\u001b[A\n",
      "Generating test examples...: 6826 examples [00:03, 1898.06 examples/s]\u001b[A\n",
      "Generating test examples...: 7017 examples [00:03, 1900.29 examples/s]\u001b[A\n",
      "Generating test examples...: 7208 examples [00:03, 1901.27 examples/s]\u001b[A\n",
      "Generating test examples...: 7399 examples [00:03, 1900.47 examples/s]\u001b[A\n",
      "Generating test examples...: 7591 examples [00:04, 1903.50 examples/s]\u001b[A\n",
      "Generating test examples...: 7782 examples [00:04, 1901.33 examples/s]\u001b[A\n",
      "Generating test examples...: 7973 examples [00:04, 1898.64 examples/s]\u001b[A\n",
      "Generating test examples...: 8163 examples [00:04, 1897.03 examples/s]\u001b[A\n",
      "Generating test examples...: 8353 examples [00:04, 1897.84 examples/s]\u001b[A\n",
      "Generating test examples...: 8543 examples [00:04, 1896.56 examples/s]\u001b[A\n",
      "Generating test examples...: 8733 examples [00:04, 1895.07 examples/s]\u001b[A\n",
      "Generating test examples...: 8923 examples [00:04, 1893.17 examples/s]\u001b[A\n",
      "Generating test examples...: 9113 examples [00:04, 1893.02 examples/s]\u001b[A\n",
      "Generating test examples...: 9303 examples [00:04, 1892.82 examples/s]\u001b[A\n",
      "Generating test examples...: 9493 examples [00:05, 1893.79 examples/s]\u001b[A\n",
      "Generating test examples...: 9683 examples [00:05, 1895.14 examples/s]\u001b[A\n",
      "Generating test examples...: 9875 examples [00:05, 1898.52 examples/s]\u001b[A\n",
      "                                                                      \u001b[A\n",
      "Shuffling mnist-test.tfrecord...:   0%|        | 0/10000 [00:00<?, ? examples/s]\u001b[A\n",
      "                                                                                \u001b[AINFO:absl:Done writing mnist-test.tfrecord. Number of examples: 10000 (shards: [10000])\n",
      "\u001b[1mDataset mnist downloaded and prepared to ./tfds/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:root:---------- Suggestions to improve input_fn performance ----------\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3_1, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but ShuffleDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but RepeatDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "INFO:root:[input_fn] - batch(): batch_size set to 256\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but BatchDatasetV2 used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Map is called prior to Batch. Consider reversing the order and performing the map function in a batched fashion to increase the performance of the input function \n",
      "WARNING:root:[input_fn] - flat_map(): use map() instead of flat_map() to improve performance and parallelize reads. If you are not calling `flat_map` directly, check if you are using: from_generator, TextLineDataset, TFRecordDataset, or FixedLenthRecordDataset. If so, set `num_parallel_reads` to > 1 or cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE, and map() will be used automatically\n",
      "INFO:root:----------------- End of input_fn suggestions -----------------\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:cerebras.stack.tools.caching_stack:Using lair flow into stack\n",
      "=============== Starting Cerebras Compilation ===============                   \n",
      "Stack:   0%|                                         | 0/10 [00:00s, ?stages/s ]=============== Cerebras Compilation Completed ===============\n",
      "salloc: Relinquishing job allocation 320727\n"
     ]
    }
   ],
   "source": [
    "model_dir_exists = os.path.isdir(\"tutorial_model_dir\")\n",
    "if model_dir_exists:\n",
    "    !rm -rf tutorial_model_dir\n",
    "\n",
    "!salloc ${SLURM_ARGUMENTS} --ntasks=1 singularity exec --bind ${BIND_LOCATIONS} --pwd ${YOUR_ENTRY_SCRIPT_LOCATION} ${CEREBRAS_CONTAINER} python run.py --mode train --validate_only --model_dir tutorial_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21e74e",
   "metadata": {},
   "source": [
    "---\n",
    "Example of the expected output (final lines):\n",
    "\n",
    "    [--- OUTPUT SNIPPED FOR KEEPING THIS EXAMPLE SHORT ---]\n",
    "    =============== Starting Cerebras Compilation ===============                   \n",
    "    Stack:   0%|                      | 0/10 [00:00s, ?stages/s ]\n",
    "    =============== Cerebras Compilation Completed ==============\n",
    "---\n",
    "\n",
    "The command above exectuted a Slurm job using `srun` and `singularity`, both with multiple arguments.\n",
    "\n",
    "For the Slurm arguments, the following were used:\n",
    "* **--ntasks=7**: Specify  the number of tasks to run.\n",
    "* **--time=0-00:15**: Set a limit on the total run time of the job allocation. The format used is \"dd-hh:mm:mm\".\n",
    "* **--cpus-per-task=28**: Request that ncpus be allocated per process. A 28-core CPU processor is being used per task.\n",
    "* **--account=ACCOUNT_ID**: this is the AI tutorial allocation account to which resource utilization is being charged.\n",
    "* **--pty**: Execute task zero in pseudo terminal mode. Meaning, it will provide an interactive session.\n",
    "\n",
    "For the Singularity/Apptainer arguments, the following were used:\n",
    "* **exec**: the \"exec\" mode is an alternative to running the \"run\" or \"shell\" mode, and it allows running a specific command.\n",
    "* **--bind \\${BIND_LOCATIONS}**: it allows specifying the bind paths, or folders to make available to the container apps in the same (or a specific) location. The folder we are requesting to bind is the one in which the FC-MNIST example is located. Other folders with input data should also be mounted as required.\n",
    "* **--pwd \\${YOUR_ENTRY_SCRIPT_LOCATION}**: sets the working directory to use when running the commands with singularity exec. The actual value is `/ocean/projects/ACCOUNT_ID/USERNAME/modelzoo/modelzoo/fc_mnist/tf`\n",
    "* **${CEREBRAS_CONTAINER}**: this is the full path to the latest Cerebras cbcore container. The actual value is `/ocean/neocortex/cerebras/cbcore_latest.sif`.\n",
    "\n",
    "And as for the Python run.py file, the following arguments were used:\n",
    "* **--mode train**: this use the training mode on the Cerebras software stack. Other modes available are the evaluation and prediction mode. More information about this can be found in the [Cerebras Documentation](https://docs.cerebras.net/en/1.6.0/tensorflow-docs/running-a-model/train-eval-predict.html).\n",
    "* **--validate_only**: as mentioned above, this argument allow running a light-weight compilationthat helps determine any unsupported TensorFlow layer or functionality is being used in the code.\n",
    "* **--model_dir tutorial_model_dir**: this argument specifies the target directory to use for the compilation process. Using different folders for this argument allows starting from scratch when something goes wrong.\n",
    "\n",
    "\n",
    "### Step 5: Compile the code\n",
    "\n",
    "Run the full compilation using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735ce091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salloc: Granted job allocation 320728\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {}\n",
      "INFO:root:Running None on CS-2\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:root:---------- Suggestions to improve input_fn performance ----------\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3_1, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but ShuffleDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but RepeatDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "INFO:root:[input_fn] - batch(): batch_size set to 256\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but BatchDatasetV2 used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Map is called prior to Batch. Consider reversing the order and performing the map function in a batched fashion to increase the performance of the input function \n",
      "WARNING:root:[input_fn] - flat_map(): use map() instead of flat_map() to improve performance and parallelize reads. If you are not calling `flat_map` directly, check if you are using: from_generator, TextLineDataset, TFRecordDataset, or FixedLenthRecordDataset. If so, set `num_parallel_reads` to > 1 or cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE, and map() will be used automatically\n",
      "INFO:root:----------------- End of input_fn suggestions -----------------\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:cerebras.stack.tools.caching_stack:Using lair flow into stack\n",
      "=============== Starting Cerebras Compilation ===============                   \n",
      "Estimating performance:  94%|█████████████████▉ | 33/35 [03:10s, 25.27s/stages ]=============== Cerebras Compilation Completed ===============\n",
      "salloc: Relinquishing job allocation 320728\n",
      "salloc: Job allocation 320728 has been revoked.\n"
     ]
    }
   ],
   "source": [
    "!salloc ${SLURM_ARGUMENTS} --ntasks=1 singularity exec --bind ${BIND_LOCATIONS} --pwd ${YOUR_ENTRY_SCRIPT_LOCATION} ${CEREBRAS_CONTAINER} python run.py --mode train --compile_only --model_dir tutorial_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73573bb5",
   "metadata": {},
   "source": [
    "---\n",
    "Example of the expected output (final lines):\n",
    "\n",
    "    [--- OUTPUT SNIPPED FOR KEEPING THIS EXAMPLE SHORT ---]\n",
    "    =============== Starting Cerebras Compilation ===============\n",
    "    Estimating performance:  94%|█████████| 33/35 [00:50s,  3.99s/stages ]\n",
    "    =============== Cerebras Compilation Completed ===============\n",
    "---\n",
    "   \n",
    "Similar to the previous command, another job was executed using `srun` and `singularity`, but now in compilation mode.\n",
    "\n",
    "For the Python run.py file, the following argument changed:\n",
    "* **--compile_only**: as mentioned above, this steps runs the full compilation (on CPU) through all stages of the Cerebras software stack to generate a CS system executable.\n",
    "\n",
    "### Step 6: Run the training on the CS machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea184d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salloc: Granted job allocation 320735\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': ['sdf-1:49600'], 'worker': ['sdf-1:49602', 'sdf-1:49604', 'sdf-1:49606', 'sdf-1:49608', 'sdf-1:49610', 'sdf-1:49612']}, 'task': {'type': 'chief', 'index': 0}}\n",
      "INFO:root:Running train on CS-2\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "2025-04-09 13:17:20.222782: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2025-04-09 13:17:20.257994: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2700000000 Hz\n",
      "2025-04-09 13:17:20.285321: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9532230 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 13:17:20.285405: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:root:---------- Suggestions to improve input_fn performance ----------\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3_1, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but ShuffleDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but RepeatDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "INFO:root:[input_fn] - batch(): batch_size set to 256\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but BatchDatasetV2 used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Map is called prior to Batch. Consider reversing the order and performing the map function in a batched fashion to increase the performance of the input function \n",
      "WARNING:root:[input_fn] - flat_map(): use map() instead of flat_map() to improve performance and parallelize reads. If you are not calling `flat_map` directly, check if you are using: from_generator, TextLineDataset, TFRecordDataset, or FixedLenthRecordDataset. If so, set `num_parallel_reads` to > 1 or cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE, and map() will be used automatically\n",
      "INFO:root:----------------- End of input_fn suggestions -----------------\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:cerebras.stack.tools.caching_stack:Using lair flow into stack\n",
      "Executing incremental compile:  34%|███▍      | 12/35 [00:10s,  3.34s/stages ]        INFO:cerebras.stack.tools.caching_stack:IncrementalCompileFlow found a cached compilation for this model configuration\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:root:Programming CS-2 fabric. This may take a couple of minutes. Please do not interrupt.\n",
      "TSKM202 13:18:53  Checkpoint callback registered!\n",
      "TSKM200 13:18:53  Getting block prebuffer numbers from cm.daemon 10.8.88.10:9001\n",
      "TSKM205 13:18:53  Waiting for block prebuffer percentage to rise above 50%, now 0%\n",
      "MSGS088 13:18:54  \u001b[0;33mWARNING: \u001b[0mMessage count for TSKM205 reached maximum 1: further instances will be not be printed\n",
      "TSKM200 13:19:08  Block prebuffer percentage is sufficient: 50\n",
      "TSKM201 13:19:08  Send block sizes:\n",
      "TSKM201 13:19:08> pre-cliff: 196607, post-cliff: 196607; using send block size: 196607\n",
      "TSKM201 13:19:08> Receive block sizes: \n",
      "TSKM201 13:19:08> pre-cliff: 50000, post-cliff: 50000; using receive block size: 50000\n",
      "Cerebras compilation completed:  37%|███▋      | 13/35 [01:19s,  6.11s/stages ]\n",
      "INFO:root:Fabric programmed: this took 51.12531137466431 seconds.\n",
      "INFO:tensorflow:Waiting for 6 streamer(s) to prime the data pipeline\n",
      "INFO:tensorflow:Streamers are ready\n",
      "INFO:root:Chief fully up. Waiting for Streaming (using 0.76% of fabric cores)\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:global step 0: loss = 2.421875 (49.76 steps/sec)\n",
      "INFO:tensorflow:global step 100: loss = 0.267822265625 (718.38 steps/sec)\n",
      "INFO:tensorflow:global step 200: loss = 0.2008056640625 (1074.66 steps/sec)\n",
      "INFO:tensorflow:global step 300: loss = 0.1929931640625 (1375.32 steps/sec)\n",
      "INFO:tensorflow:global step 400: loss = 0.08984375 (1585.39 steps/sec)\n",
      "INFO:tensorflow:global step 500: loss = 0.09063720703125 (1748.88 steps/sec)\n",
      "INFO:tensorflow:global step 600: loss = 0.1158447265625 (1798.53 steps/sec)\n",
      "INFO:tensorflow:global step 700: loss = 0.050628662109375 (1904.66 steps/sec)\n",
      "INFO:tensorflow:global step 800: loss = 0.062408447265625 (1985.58 steps/sec)\n",
      "INFO:tensorflow:global step 900: loss = 0.043243408203125 (2067.76 steps/sec)\n",
      "INFO:tensorflow:global step 1000: loss = 0.0540771484375 (2138.44 steps/sec)\n",
      "INFO:tensorflow:global step 1100: loss = 0.0243988037109375 (2197.16 steps/sec)\n",
      "INFO:tensorflow:global step 1200: loss = 0.04364013671875 (2249.43 steps/sec)\n",
      "INFO:tensorflow:global step 1300: loss = 0.024810791015625 (2314.39 steps/sec)\n",
      "INFO:tensorflow:global step 1400: loss = 0.032745361328125 (2370.28 steps/sec)\n",
      "INFO:tensorflow:global step 1500: loss = 0.02099609375 (2417.08 steps/sec)\n",
      "INFO:tensorflow:global step 1600: loss = 0.0278778076171875 (2447.21 steps/sec)\n",
      "INFO:tensorflow:global step 1700: loss = 0.07861328125 (2484.82 steps/sec)\n",
      "INFO:tensorflow:global step 1800: loss = 0.033782958984375 (2506.61 steps/sec)\n",
      "INFO:tensorflow:global step 1900: loss = 0.017669677734375 (2521.59 steps/sec)\n",
      "INFO:tensorflow:global step 2000: loss = 0.0275726318359375 (2536.56 steps/sec)\n",
      "INFO:tensorflow:global step 2100: loss = 0.0279541015625 (2561.21 steps/sec)\n",
      "INFO:tensorflow:global step 2200: loss = 0.0204010009765625 (2572.49 steps/sec)\n",
      "INFO:tensorflow:global step 2300: loss = 0.0390625 (2587.42 steps/sec)\n",
      "INFO:tensorflow:global step 2400: loss = 0.0160980224609375 (2595.2 steps/sec)\n",
      "INFO:tensorflow:global step 2500: loss = 0.045318603515625 (2608.03 steps/sec)\n",
      "INFO:tensorflow:global step 2600: loss = 0.03997802734375 (2617.23 steps/sec)\n",
      "INFO:tensorflow:global step 2700: loss = 0.01357269287109375 (2630.36 steps/sec)\n",
      "INFO:tensorflow:global step 2800: loss = 0.036224365234375 (2638.07 steps/sec)\n",
      "INFO:tensorflow:global step 2900: loss = 0.0250701904296875 (2648.16 steps/sec)\n",
      "INFO:tensorflow:global step 3000: loss = 0.0171661376953125 (2654.43 steps/sec)\n",
      "INFO:tensorflow:global step 3100: loss = 0.038330078125 (2663.85 steps/sec)\n",
      "INFO:tensorflow:global step 3200: loss = 0.005565643310546875 (2677.91 steps/sec)\n",
      "INFO:tensorflow:global step 3300: loss = 0.006587982177734375 (2692.66 steps/sec)\n",
      "INFO:tensorflow:global step 3400: loss = 0.0032024383544921875 (2706.17 steps/sec)\n",
      "INFO:tensorflow:global step 3500: loss = 0.010162353515625 (2724.7 steps/sec)\n",
      "INFO:tensorflow:global step 3600: loss = 0.007373809814453125 (2733.57 steps/sec)\n",
      "INFO:tensorflow:global step 3700: loss = 0.038116455078125 (2747.38 steps/sec)\n",
      "INFO:tensorflow:global step 3800: loss = 0.017181396484375 (2757.94 steps/sec)\n",
      "INFO:tensorflow:global step 3900: loss = 0.00650787353515625 (2767.69 steps/sec)\n",
      "INFO:tensorflow:global step 4000: loss = 0.023040771484375 (2778.52 steps/sec)\n",
      "INFO:tensorflow:global step 4100: loss = 0.0007638931274414062 (2791.74 steps/sec)\n",
      "INFO:tensorflow:global step 4200: loss = 0.01345062255859375 (2801.95 steps/sec)\n",
      "INFO:tensorflow:global step 4300: loss = 0.01331329345703125 (2811.91 steps/sec)\n",
      "INFO:tensorflow:global step 4400: loss = 0.0092315673828125 (2824.44 steps/sec)\n",
      "INFO:tensorflow:global step 4500: loss = 0.01953125 (2834.77 steps/sec)\n",
      "INFO:tensorflow:global step 4600: loss = 0.0298309326171875 (2845.46 steps/sec)\n",
      "INFO:tensorflow:global step 4700: loss = 0.012359619140625 (2853.66 steps/sec)\n",
      "INFO:tensorflow:global step 4800: loss = 0.005260467529296875 (2864.07 steps/sec)\n",
      "INFO:tensorflow:global step 4900: loss = 0.0126800537109375 (2870.5 steps/sec)\n",
      "INFO:tensorflow:global step 5000: loss = 0.00412750244140625 (2880.01 steps/sec)\n",
      "INFO:tensorflow:global step 5100: loss = 0.0213623046875 (2886.18 steps/sec)\n",
      "INFO:tensorflow:global step 5200: loss = 0.009765625 (2894.19 steps/sec)\n",
      "INFO:tensorflow:global step 5300: loss = 0.006122589111328125 (2898.65 steps/sec)\n",
      "INFO:tensorflow:global step 5400: loss = 0.0008878707885742188 (2908.34 steps/sec)\n",
      "INFO:tensorflow:global step 5500: loss = 0.005252838134765625 (2914.3 steps/sec)\n",
      "INFO:tensorflow:global step 5600: loss = 0.00469970703125 (2922.04 steps/sec)\n",
      "INFO:tensorflow:global step 5700: loss = 0.005191802978515625 (2927.01 steps/sec)\n",
      "INFO:tensorflow:global step 5800: loss = 0.0005497932434082031 (2934.6 steps/sec)\n",
      "INFO:tensorflow:global step 5900: loss = 0.0003261566162109375 (2936.6 steps/sec)\n",
      "INFO:tensorflow:global step 6000: loss = 0.0034027099609375 (2942.41 steps/sec)\n",
      "INFO:tensorflow:global step 6100: loss = 0.009429931640625 (2940.43 steps/sec)\n",
      "INFO:tensorflow:global step 6200: loss = 0.006443023681640625 (2947.15 steps/sec)\n",
      "INFO:tensorflow:global step 6300: loss = 0.0037174224853515625 (2951.04 steps/sec)\n",
      "INFO:tensorflow:global step 6400: loss = 0.00791168212890625 (2956.79 steps/sec)\n",
      "INFO:tensorflow:global step 6500: loss = 0.016265869140625 (2960.43 steps/sec)\n",
      "INFO:tensorflow:global step 6600: loss = 0.0018253326416015625 (2966.91 steps/sec)\n",
      "INFO:tensorflow:global step 6700: loss = 0.0040740966796875 (2970.58 steps/sec)\n",
      "INFO:tensorflow:global step 6800: loss = 0.00531005859375 (2977.12 steps/sec)\n",
      "INFO:tensorflow:global step 6900: loss = 0.004863739013671875 (2980.29 steps/sec)\n",
      "INFO:tensorflow:global step 7000: loss = 0.0079803466796875 (2986.9 steps/sec)\n",
      "INFO:tensorflow:global step 7100: loss = 0.00481414794921875 (2990.29 steps/sec)\n",
      "INFO:tensorflow:global step 7200: loss = 0.0009174346923828125 (2996.92 steps/sec)\n",
      "INFO:tensorflow:global step 7300: loss = 0.0189056396484375 (3000.94 steps/sec)\n",
      "INFO:tensorflow:global step 7400: loss = 0.0026836395263671875 (3006.12 steps/sec)\n",
      "INFO:tensorflow:global step 7500: loss = 0.0004837512969970703 (3007.35 steps/sec)\n",
      "INFO:tensorflow:global step 7600: loss = 0.004238128662109375 (3012.09 steps/sec)\n",
      "INFO:tensorflow:global step 7700: loss = 0.01435089111328125 (3012.28 steps/sec)\n",
      "INFO:tensorflow:global step 7800: loss = 0.0235137939453125 (3016.48 steps/sec)\n",
      "INFO:tensorflow:global step 7900: loss = 0.0007710456848144531 (3010.71 steps/sec)\n",
      "INFO:tensorflow:global step 8000: loss = 0.0079498291015625 (3009.31 steps/sec)\n",
      "INFO:tensorflow:global step 8100: loss = 0.01531982421875 (3006.0 steps/sec)\n",
      "INFO:tensorflow:global step 8200: loss = 0.0201873779296875 (3010.52 steps/sec)\n",
      "INFO:tensorflow:global step 8300: loss = 0.0016832351684570312 (3013.2 steps/sec)\n",
      "INFO:tensorflow:global step 8400: loss = 8.761882781982422e-05 (3017.32 steps/sec)\n",
      "INFO:tensorflow:global step 8500: loss = 0.0016307830810546875 (3021.5 steps/sec)\n",
      "INFO:tensorflow:global step 8600: loss = 0.0009984970092773438 (3021.87 steps/sec)\n",
      "INFO:tensorflow:global step 8700: loss = 0.0347900390625 (3025.77 steps/sec)\n",
      "INFO:tensorflow:global step 8800: loss = 0.01515960693359375 (3024.06 steps/sec)\n",
      "INFO:tensorflow:global step 8900: loss = 0.0015306472778320312 (3025.38 steps/sec)\n",
      "INFO:tensorflow:global step 9000: loss = 0.0182342529296875 (3026.47 steps/sec)\n",
      "INFO:tensorflow:global step 9100: loss = 0.001567840576171875 (3025.12 steps/sec)\n",
      "INFO:tensorflow:global step 9200: loss = 0.02593994140625 (3023.67 steps/sec)\n",
      "INFO:tensorflow:global step 9300: loss = 0.0009312629699707031 (3023.52 steps/sec)\n",
      "INFO:tensorflow:global step 9400: loss = 0.0246124267578125 (3024.66 steps/sec)\n",
      "INFO:tensorflow:global step 9500: loss = 0.00627899169921875 (3028.45 steps/sec)\n",
      "INFO:tensorflow:global step 9600: loss = 0.0284881591796875 (3029.04 steps/sec)\n",
      "INFO:tensorflow:global step 9700: loss = 0.0157318115234375 (3032.47 steps/sec)\n",
      "INFO:tensorflow:global step 9800: loss = 0.0016107559204101562 (3033.06 steps/sec)\n",
      "INFO:tensorflow:global step 9900: loss = 0.0199737548828125 (3037.38 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10000...\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10000...\n",
      "INFO:tensorflow:global step 10000: loss = 0.0005517005920410156 (2200.02 steps/sec)\n",
      "INFO:tensorflow:global step 10100: loss = 0.020355224609375 (2205.22 steps/sec)\n",
      "INFO:tensorflow:global step 10200: loss = 0.006099700927734375 (2210.72 steps/sec)\n",
      "INFO:tensorflow:global step 10300: loss = 0.0215301513671875 (2216.84 steps/sec)\n",
      "INFO:tensorflow:global step 10400: loss = 0.001926422119140625 (2222.31 steps/sec)\n",
      "INFO:tensorflow:global step 10500: loss = 0.0021038055419921875 (2229.04 steps/sec)\n",
      "INFO:tensorflow:global step 10600: loss = 0.006633758544921875 (2235.2 steps/sec)\n",
      "INFO:tensorflow:global step 10700: loss = 0.0286712646484375 (2242.94 steps/sec)\n",
      "INFO:tensorflow:global step 10800: loss = 0.00653839111328125 (2250.09 steps/sec)\n",
      "INFO:tensorflow:global step 10900: loss = 0.0028972625732421875 (2257.73 steps/sec)\n",
      "INFO:tensorflow:global step 11000: loss = 0.0005750656127929688 (2264.93 steps/sec)\n",
      "INFO:tensorflow:global step 11100: loss = 0.0005855560302734375 (2270.83 steps/sec)\n",
      "INFO:tensorflow:global step 11200: loss = 0.0090789794921875 (2276.37 steps/sec)\n",
      "INFO:tensorflow:global step 11300: loss = 0.005275726318359375 (2282.45 steps/sec)\n",
      "INFO:tensorflow:global step 11400: loss = 0.0056915283203125 (2287.83 steps/sec)\n",
      "INFO:tensorflow:global step 11500: loss = 0.01049041748046875 (2294.13 steps/sec)\n",
      "INFO:tensorflow:global step 11600: loss = 0.015625 (2299.81 steps/sec)\n",
      "INFO:tensorflow:global step 11700: loss = 0.006526947021484375 (2306.36 steps/sec)\n",
      "INFO:tensorflow:global step 11800: loss = 0.01316070556640625 (2312.6 steps/sec)\n",
      "INFO:tensorflow:global step 11900: loss = 0.003574371337890625 (2319.3 steps/sec)\n",
      "INFO:tensorflow:global step 12000: loss = 0.0003898143768310547 (2325.53 steps/sec)\n",
      "INFO:tensorflow:global step 12100: loss = 0.005138397216796875 (2331.67 steps/sec)\n",
      "INFO:tensorflow:global step 12200: loss = 0.0027408599853515625 (2337.72 steps/sec)\n",
      "INFO:tensorflow:global step 12300: loss = 0.0008406639099121094 (2344.27 steps/sec)\n",
      "INFO:tensorflow:global step 12400: loss = 0.004474639892578125 (2349.84 steps/sec)\n",
      "INFO:tensorflow:global step 12500: loss = 0.0020008087158203125 (2355.16 steps/sec)\n",
      "INFO:tensorflow:global step 12600: loss = 0.004978179931640625 (2360.25 steps/sec)\n",
      "INFO:tensorflow:global step 12700: loss = 0.005054473876953125 (2364.25 steps/sec)\n",
      "INFO:tensorflow:global step 12800: loss = 0.0006661415100097656 (2369.47 steps/sec)\n",
      "INFO:tensorflow:global step 12900: loss = 0.02569580078125 (2373.66 steps/sec)\n",
      "INFO:tensorflow:global step 13000: loss = 0.0004000663757324219 (2378.51 steps/sec)\n",
      "INFO:tensorflow:global step 13100: loss = 0.004749298095703125 (2382.02 steps/sec)\n",
      "INFO:tensorflow:global step 13200: loss = 0.01557159423828125 (2386.77 steps/sec)\n",
      "INFO:tensorflow:global step 13300: loss = 0.00034165382385253906 (2391.71 steps/sec)\n",
      "INFO:tensorflow:global step 13400: loss = 0.001026153564453125 (2397.26 steps/sec)\n",
      "INFO:tensorflow:global step 13500: loss = 0.006572723388671875 (2402.17 steps/sec)\n",
      "INFO:tensorflow:global step 13600: loss = 0.0019741058349609375 (2406.74 steps/sec)\n",
      "INFO:tensorflow:global step 13700: loss = 0.0008840560913085938 (2411.27 steps/sec)\n",
      "INFO:tensorflow:global step 13800: loss = 0.0003287792205810547 (2416.32 steps/sec)\n",
      "INFO:tensorflow:global step 13900: loss = 0.0021800994873046875 (2420.82 steps/sec)\n",
      "INFO:tensorflow:global step 14000: loss = 0.002635955810546875 (2425.66 steps/sec)\n",
      "INFO:tensorflow:global step 14100: loss = 0.0225830078125 (2429.92 steps/sec)\n",
      "INFO:tensorflow:global step 14200: loss = 0.022430419921875 (2434.36 steps/sec)\n",
      "INFO:tensorflow:global step 14300: loss = 0.00708770751953125 (2438.16 steps/sec)\n",
      "INFO:tensorflow:global step 14400: loss = 0.00021386146545410156 (2442.66 steps/sec)\n",
      "INFO:tensorflow:global step 14500: loss = 0.0003871917724609375 (2446.8 steps/sec)\n",
      "INFO:tensorflow:global step 14600: loss = 0.00015926361083984375 (2450.32 steps/sec)\n",
      "INFO:tensorflow:global step 14700: loss = 0.002002716064453125 (2453.51 steps/sec)\n",
      "INFO:tensorflow:global step 14800: loss = 0.00030231475830078125 (2457.21 steps/sec)\n",
      "INFO:tensorflow:global step 14900: loss = 0.011322021484375 (2460.27 steps/sec)\n",
      "INFO:tensorflow:global step 15000: loss = 0.0002655982971191406 (2463.93 steps/sec)\n",
      "INFO:tensorflow:global step 15100: loss = 0.0009174346923828125 (2466.83 steps/sec)\n",
      "INFO:tensorflow:global step 15200: loss = 0.004779815673828125 (2470.51 steps/sec)\n",
      "INFO:tensorflow:global step 15300: loss = 0.00402069091796875 (2473.46 steps/sec)\n",
      "INFO:tensorflow:global step 15400: loss = 0.0007033348083496094 (2476.54 steps/sec)\n",
      "INFO:tensorflow:global step 15500: loss = 0.00031375885009765625 (2479.12 steps/sec)\n",
      "INFO:tensorflow:global step 15600: loss = 0.0017786026000976562 (2482.78 steps/sec)\n",
      "INFO:tensorflow:global step 15700: loss = 0.0022144317626953125 (2486.69 steps/sec)\n",
      "INFO:tensorflow:global step 15800: loss = 0.0036411285400390625 (2490.45 steps/sec)\n",
      "INFO:tensorflow:global step 15900: loss = 0.016571044921875 (2493.76 steps/sec)\n",
      "INFO:tensorflow:global step 16000: loss = 0.00022411346435546875 (2498.32 steps/sec)\n",
      "INFO:tensorflow:global step 16100: loss = 0.00014710426330566406 (2501.53 steps/sec)\n",
      "INFO:tensorflow:global step 16200: loss = 0.0203094482421875 (2505.73 steps/sec)\n",
      "INFO:tensorflow:global step 16300: loss = 0.045806884765625 (2509.53 steps/sec)\n",
      "INFO:tensorflow:global step 16400: loss = 0.0018110275268554688 (2513.46 steps/sec)\n",
      "INFO:tensorflow:global step 16500: loss = 0.0007262229919433594 (2517.55 steps/sec)\n",
      "INFO:tensorflow:global step 16600: loss = 0.0132598876953125 (2521.33 steps/sec)\n",
      "INFO:tensorflow:global step 16700: loss = 0.0006594657897949219 (2525.3 steps/sec)\n",
      "INFO:tensorflow:global step 16800: loss = 0.0026111602783203125 (2529.33 steps/sec)\n",
      "INFO:tensorflow:global step 16900: loss = 0.005413055419921875 (2533.73 steps/sec)\n",
      "INFO:tensorflow:global step 17000: loss = 0.0023326873779296875 (2537.62 steps/sec)\n",
      "INFO:tensorflow:global step 17100: loss = 0.0003006458282470703 (2541.57 steps/sec)\n",
      "INFO:tensorflow:global step 17200: loss = 0.009368896484375 (2545.34 steps/sec)\n",
      "INFO:tensorflow:global step 17300: loss = 0.0009765625 (2549.24 steps/sec)\n",
      "INFO:tensorflow:global step 17400: loss = 0.00794219970703125 (2552.4 steps/sec)\n",
      "INFO:tensorflow:global step 17500: loss = 0.0107421875 (2556.21 steps/sec)\n",
      "INFO:tensorflow:global step 17600: loss = 0.0037212371826171875 (2559.15 steps/sec)\n",
      "INFO:tensorflow:global step 17700: loss = 0.00199127197265625 (2562.67 steps/sec)\n",
      "INFO:tensorflow:global step 17800: loss = 0.0321044921875 (2565.59 steps/sec)\n",
      "INFO:tensorflow:global step 17900: loss = 0.0023040771484375 (2569.11 steps/sec)\n",
      "INFO:tensorflow:global step 18000: loss = 0.004062652587890625 (2572.0 steps/sec)\n",
      "INFO:tensorflow:global step 18100: loss = 0.00042510032653808594 (2575.25 steps/sec)\n",
      "INFO:tensorflow:global step 18200: loss = 4.3451786041259766e-05 (2578.51 steps/sec)\n",
      "INFO:tensorflow:global step 18300: loss = 0.00557708740234375 (2582.28 steps/sec)\n",
      "INFO:tensorflow:global step 18400: loss = 0.00010216236114501953 (2585.61 steps/sec)\n",
      "INFO:tensorflow:global step 18500: loss = 0.002674102783203125 (2589.49 steps/sec)\n",
      "INFO:tensorflow:global step 18600: loss = 0.0103759765625 (2592.22 steps/sec)\n",
      "INFO:tensorflow:global step 18700: loss = 0.0017728805541992188 (2595.86 steps/sec)\n",
      "INFO:tensorflow:global step 18800: loss = 0.00801849365234375 (2599.13 steps/sec)\n",
      "INFO:tensorflow:global step 18900: loss = 0.0016069412231445312 (2602.77 steps/sec)\n",
      "INFO:tensorflow:global step 19000: loss = 9.495019912719727e-05 (2606.13 steps/sec)\n",
      "INFO:tensorflow:global step 19100: loss = 0.0041961669921875 (2608.68 steps/sec)\n",
      "INFO:tensorflow:global step 19200: loss = 0.0005769729614257812 (2611.08 steps/sec)\n",
      "INFO:tensorflow:global step 19300: loss = 0.004764556884765625 (2614.28 steps/sec)\n",
      "INFO:tensorflow:global step 19400: loss = 0.0012960433959960938 (2616.79 steps/sec)\n",
      "INFO:tensorflow:global step 19500: loss = 0.0130615234375 (2620.22 steps/sec)\n",
      "INFO:tensorflow:global step 19600: loss = 0.0080718994140625 (2622.25 steps/sec)\n",
      "INFO:tensorflow:global step 19700: loss = 0.0037555694580078125 (2624.97 steps/sec)\n",
      "INFO:tensorflow:global step 19800: loss = 0.00728607177734375 (2627.18 steps/sec)\n",
      "INFO:tensorflow:global step 19900: loss = 0.0010137557983398438 (2629.95 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 20000...\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into tutorial_model_dir/model.ckpt.\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 20000...\n",
      "INFO:tensorflow:global step 20000: loss = 0.0164794921875 (2193.09 steps/sec)\n",
      "INFO:tensorflow:global step 20100: loss = 0.012786865234375 (2195.79 steps/sec)\n",
      "INFO:tensorflow:global step 20200: loss = 0.004150390625 (2197.98 steps/sec)\n",
      "INFO:tensorflow:global step 20300: loss = 2.9027462005615234e-05 (2200.78 steps/sec)\n",
      "INFO:tensorflow:global step 20400: loss = 6.99162483215332e-05 (2203.14 steps/sec)\n",
      "INFO:tensorflow:global step 20500: loss = 5.418062210083008e-05 (2205.74 steps/sec)\n",
      "INFO:tensorflow:global step 20600: loss = 0.004199981689453125 (2207.62 steps/sec)\n",
      "INFO:tensorflow:global step 20700: loss = 0.001567840576171875 (2210.11 steps/sec)\n",
      "INFO:tensorflow:global step 20800: loss = 0.022857666015625 (2212.25 steps/sec)\n",
      "INFO:tensorflow:global step 20900: loss = 0.00031185150146484375 (2214.44 steps/sec)\n",
      "INFO:tensorflow:global step 21000: loss = 0.0200958251953125 (2216.92 steps/sec)\n",
      "INFO:tensorflow:global step 21100: loss = 0.0004038810729980469 (2219.12 steps/sec)\n",
      "INFO:tensorflow:global step 21200: loss = 0.0200042724609375 (2221.87 steps/sec)\n",
      "INFO:tensorflow:global step 21300: loss = 0.00885009765625 (2224.46 steps/sec)\n",
      "INFO:tensorflow:global step 21400: loss = 0.00011038780212402344 (2227.3 steps/sec)\n",
      "INFO:tensorflow:global step 21500: loss = 0.02471923828125 (2229.81 steps/sec)\n",
      "INFO:tensorflow:global step 21600: loss = 0.002658843994140625 (2231.89 steps/sec)\n",
      "INFO:tensorflow:global step 21700: loss = 0.0007410049438476562 (2234.0 steps/sec)\n",
      "INFO:tensorflow:global step 21800: loss = 0.0006690025329589844 (2236.53 steps/sec)\n",
      "INFO:tensorflow:global step 21900: loss = 0.002811431884765625 (2238.79 steps/sec)\n",
      "INFO:tensorflow:global step 22000: loss = 7.677078247070312e-05 (2241.13 steps/sec)\n",
      "INFO:tensorflow:global step 22100: loss = 0.03302001953125 (2243.22 steps/sec)\n",
      "INFO:tensorflow:global step 22200: loss = 0.0005598068237304688 (2245.7 steps/sec)\n",
      "INFO:tensorflow:global step 22300: loss = 0.0046539306640625 (2247.86 steps/sec)\n",
      "INFO:tensorflow:global step 22400: loss = 0.0024013519287109375 (2250.31 steps/sec)\n",
      "INFO:tensorflow:global step 22500: loss = 0.0013561248779296875 (2252.51 steps/sec)\n",
      "INFO:tensorflow:global step 22600: loss = 0.00118255615234375 (2254.99 steps/sec)\n",
      "INFO:tensorflow:global step 22700: loss = 0.0173797607421875 (2257.06 steps/sec)\n",
      "INFO:tensorflow:global step 22800: loss = 1.0013580322265625e-05 (2259.48 steps/sec)\n",
      "INFO:tensorflow:global step 22900: loss = 0.001434326171875 (2261.58 steps/sec)\n",
      "INFO:tensorflow:global step 23000: loss = 0.0011701583862304688 (2263.92 steps/sec)\n",
      "INFO:tensorflow:global step 23100: loss = 0.0002872943878173828 (2265.73 steps/sec)\n",
      "INFO:tensorflow:global step 23200: loss = 0.0001304149627685547 (2268.14 steps/sec)\n",
      "INFO:tensorflow:global step 23300: loss = 0.0007877349853515625 (2270.26 steps/sec)\n",
      "INFO:tensorflow:global step 23400: loss = 9.655952453613281e-06 (2272.84 steps/sec)\n",
      "INFO:tensorflow:global step 23500: loss = 2.0503997802734375e-05 (2275.13 steps/sec)\n",
      "INFO:tensorflow:global step 23600: loss = 0.0182952880859375 (2277.47 steps/sec)\n",
      "INFO:tensorflow:global step 23700: loss = 0.0004591941833496094 (2279.68 steps/sec)\n",
      "INFO:tensorflow:global step 23800: loss = 0.00026679039001464844 (2282.16 steps/sec)\n",
      "INFO:tensorflow:global step 23900: loss = 5.984306335449219e-05 (2284.36 steps/sec)\n",
      "INFO:tensorflow:global step 24000: loss = 2.771615982055664e-05 (2286.57 steps/sec)\n",
      "INFO:tensorflow:global step 24100: loss = 0.0004901885986328125 (2288.51 steps/sec)\n",
      "INFO:tensorflow:global step 24200: loss = 0.0001327991485595703 (2290.82 steps/sec)\n",
      "INFO:tensorflow:global step 24300: loss = 0.006103515625 (2292.98 steps/sec)\n",
      "INFO:tensorflow:global step 24400: loss = 0.000598907470703125 (2295.34 steps/sec)\n",
      "INFO:tensorflow:global step 24500: loss = 0.0074005126953125 (2297.45 steps/sec)\n",
      "INFO:tensorflow:global step 24600: loss = 0.00028228759765625 (2299.73 steps/sec)\n",
      "INFO:tensorflow:global step 24700: loss = 5.65648078918457e-05 (2301.8 steps/sec)\n",
      "INFO:tensorflow:global step 24800: loss = 6.532669067382812e-05 (2304.07 steps/sec)\n",
      "INFO:tensorflow:global step 24900: loss = 0.0008497238159179688 (2306.14 steps/sec)\n",
      "INFO:tensorflow:global step 25000: loss = 0.00250244140625 (2308.11 steps/sec)\n",
      "INFO:tensorflow:global step 25100: loss = 0.0010290145874023438 (2310.22 steps/sec)\n",
      "INFO:tensorflow:global step 25200: loss = 0.050140380859375 (2312.17 steps/sec)\n",
      "INFO:tensorflow:global step 25300: loss = 0.00060272216796875 (2314.52 steps/sec)\n",
      "INFO:tensorflow:global step 25400: loss = 0.0028362274169921875 (2316.61 steps/sec)\n",
      "INFO:tensorflow:global step 25500: loss = 0.0017337799072265625 (2318.9 steps/sec)\n",
      "INFO:tensorflow:global step 25600: loss = 1.6927719116210938e-05 (2320.68 steps/sec)\n",
      "INFO:tensorflow:global step 25700: loss = 3.62396240234375e-05 (2322.81 steps/sec)\n",
      "INFO:tensorflow:global step 25800: loss = 0.022369384765625 (2324.67 steps/sec)\n",
      "INFO:tensorflow:global step 25900: loss = 0.001148223876953125 (2326.81 steps/sec)\n",
      "INFO:tensorflow:global step 26000: loss = 6.318092346191406e-05 (2328.72 steps/sec)\n",
      "INFO:tensorflow:global step 26100: loss = 0.00043487548828125 (2330.66 steps/sec)\n",
      "INFO:tensorflow:global step 26200: loss = 0.0009293556213378906 (2332.53 steps/sec)\n",
      "INFO:tensorflow:global step 26300: loss = 0.0028705596923828125 (2334.59 steps/sec)\n",
      "INFO:tensorflow:global step 26400: loss = 0.00101470947265625 (2336.3 steps/sec)\n",
      "INFO:tensorflow:global step 26500: loss = 2.3603439331054688e-05 (2338.34 steps/sec)\n",
      "INFO:tensorflow:global step 26600: loss = 8.398294448852539e-05 (2340.0 steps/sec)\n",
      "INFO:tensorflow:global step 26700: loss = 1.9252300262451172e-05 (2341.96 steps/sec)\n",
      "INFO:tensorflow:global step 26800: loss = 0.0005311965942382812 (2343.71 steps/sec)\n",
      "INFO:tensorflow:global step 26900: loss = 9.357929229736328e-06 (2345.73 steps/sec)\n",
      "INFO:tensorflow:global step 27000: loss = 0.00015437602996826172 (2347.32 steps/sec)\n",
      "INFO:tensorflow:global step 27100: loss = 0.01490020751953125 (2349.23 steps/sec)\n",
      "INFO:tensorflow:global step 27200: loss = 0.01337432861328125 (2350.93 steps/sec)\n",
      "INFO:tensorflow:global step 27300: loss = 1.8894672393798828e-05 (2352.85 steps/sec)\n",
      "INFO:tensorflow:global step 27400: loss = 0.0010166168212890625 (2354.22 steps/sec)\n",
      "INFO:tensorflow:global step 27500: loss = 0.0005965232849121094 (2356.05 steps/sec)\n",
      "INFO:tensorflow:global step 27600: loss = 0.00019097328186035156 (2357.51 steps/sec)\n",
      "INFO:tensorflow:global step 27700: loss = 3.933906555175781e-06 (2359.26 steps/sec)\n",
      "INFO:tensorflow:global step 27800: loss = 6.079673767089844e-06 (2360.74 steps/sec)\n",
      "INFO:tensorflow:global step 27900: loss = 7.75456428527832e-05 (2362.63 steps/sec)\n",
      "INFO:tensorflow:global step 28000: loss = 6.771087646484375e-05 (2364.24 steps/sec)\n",
      "INFO:tensorflow:global step 28100: loss = 0.0119781494140625 (2365.98 steps/sec)\n",
      "INFO:tensorflow:global step 28200: loss = 0.043304443359375 (2367.62 steps/sec)\n",
      "INFO:tensorflow:global step 28300: loss = 1.9609928131103516e-05 (2369.49 steps/sec)\n",
      "INFO:tensorflow:global step 28400: loss = 0.0009984970092773438 (2371.01 steps/sec)\n",
      "INFO:tensorflow:global step 28500: loss = 0.0004978179931640625 (2372.63 steps/sec)\n",
      "INFO:tensorflow:global step 28600: loss = 5.060434341430664e-05 (2373.65 steps/sec)\n",
      "INFO:tensorflow:global step 28700: loss = 0.01666259765625 (2375.28 steps/sec)\n",
      "INFO:tensorflow:global step 28800: loss = 0.00021719932556152344 (2376.6 steps/sec)\n",
      "INFO:tensorflow:global step 28900: loss = 0.00010228157043457031 (2378.33 steps/sec)\n",
      "INFO:tensorflow:global step 29000: loss = 0.0025882720947265625 (2380.0 steps/sec)\n",
      "INFO:tensorflow:global step 29100: loss = 2.5510787963867188e-05 (2381.5 steps/sec)\n",
      "INFO:tensorflow:global step 29200: loss = 1.138448715209961e-05 (2383.05 steps/sec)\n",
      "INFO:tensorflow:global step 29300: loss = 4.351139068603516e-06 (2384.58 steps/sec)\n",
      "INFO:tensorflow:global step 29400: loss = 0.0007500648498535156 (2386.3 steps/sec)\n",
      "INFO:tensorflow:global step 29500: loss = 1.9073486328125e-06 (2387.85 steps/sec)\n",
      "INFO:tensorflow:global step 29600: loss = 0.026214599609375 (2389.4 steps/sec)\n",
      "INFO:tensorflow:global step 29700: loss = 3.826618194580078e-05 (2390.97 steps/sec)\n",
      "INFO:tensorflow:global step 29800: loss = 4.571676254272461e-05 (2392.77 steps/sec)\n",
      "INFO:tensorflow:global step 29900: loss = 0.0015354156494140625 (2323.52 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 30000...\n",
      "INFO:tensorflow:Saving checkpoints for 30000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 30000...\n",
      "INFO:tensorflow:global step 30000: loss = 0.00014853477478027344 (2138.09 steps/sec)\n",
      "INFO:tensorflow:global step 30100: loss = 0.0004506111145019531 (2139.88 steps/sec)\n",
      "INFO:tensorflow:global step 30200: loss = 0.0030727386474609375 (2141.99 steps/sec)\n",
      "INFO:tensorflow:global step 30300: loss = 1.704692840576172e-05 (2144.02 steps/sec)\n",
      "INFO:tensorflow:global step 30400: loss = 0.0006251335144042969 (2146.17 steps/sec)\n",
      "INFO:tensorflow:global step 30500: loss = 6.455183029174805e-05 (2148.15 steps/sec)\n",
      "INFO:tensorflow:global step 30600: loss = 6.794929504394531e-06 (2150.19 steps/sec)\n",
      "INFO:tensorflow:global step 30700: loss = 0.0004329681396484375 (2152.08 steps/sec)\n",
      "INFO:tensorflow:global step 30800: loss = 0.00046825408935546875 (2154.1 steps/sec)\n",
      "INFO:tensorflow:global step 30900: loss = 0.0001468658447265625 (2156.09 steps/sec)\n",
      "INFO:tensorflow:global step 31000: loss = 0.006954193115234375 (2158.21 steps/sec)\n",
      "INFO:tensorflow:global step 31100: loss = 1.4901161193847656e-06 (2159.98 steps/sec)\n",
      "INFO:tensorflow:global step 31200: loss = 0.00597381591796875 (2161.95 steps/sec)\n",
      "INFO:tensorflow:global step 31300: loss = 8.553266525268555e-05 (2163.81 steps/sec)\n",
      "INFO:tensorflow:global step 31400: loss = 0.044952392578125 (2165.82 steps/sec)\n",
      "INFO:tensorflow:global step 31500: loss = 0.005725860595703125 (2167.68 steps/sec)\n",
      "INFO:tensorflow:global step 31600: loss = 0.00888824462890625 (2169.71 steps/sec)\n",
      "INFO:tensorflow:global step 31700: loss = 0.00867462158203125 (2171.58 steps/sec)\n",
      "INFO:tensorflow:global step 31800: loss = 0.01092529296875 (2173.53 steps/sec)\n",
      "INFO:tensorflow:global step 31900: loss = 6.854534149169922e-06 (2175.38 steps/sec)\n",
      "INFO:tensorflow:global step 32000: loss = 7.927417755126953e-06 (2177.38 steps/sec)\n",
      "INFO:tensorflow:global step 32100: loss = 0.0006041526794433594 (2179.09 steps/sec)\n",
      "INFO:tensorflow:global step 32200: loss = 3.039836883544922e-05 (2181.0 steps/sec)\n",
      "INFO:tensorflow:global step 32300: loss = 0.0333251953125 (2182.75 steps/sec)\n",
      "INFO:tensorflow:global step 32400: loss = 0.0 (2184.66 steps/sec)\n",
      "INFO:tensorflow:global step 32500: loss = 9.894371032714844e-06 (2186.43 steps/sec)\n",
      "INFO:tensorflow:global step 32600: loss = 0.0091705322265625 (2188.3 steps/sec)\n",
      "INFO:tensorflow:global step 32700: loss = 0.00020313262939453125 (2189.97 steps/sec)\n",
      "INFO:tensorflow:global step 32800: loss = 0.0007600784301757812 (2191.8 steps/sec)\n",
      "INFO:tensorflow:global step 32900: loss = 0.00020635128021240234 (2193.47 steps/sec)\n",
      "INFO:tensorflow:global step 33000: loss = 0.0015058517456054688 (2195.21 steps/sec)\n",
      "INFO:tensorflow:global step 33100: loss = 0.0022220611572265625 (2196.76 steps/sec)\n",
      "INFO:tensorflow:global step 33200: loss = 1.7642974853515625e-05 (2198.31 steps/sec)\n",
      "INFO:tensorflow:global step 33300: loss = 0.005146026611328125 (2199.76 steps/sec)\n",
      "INFO:tensorflow:global step 33400: loss = 5.7578086853027344e-05 (2201.3 steps/sec)\n",
      "INFO:tensorflow:global step 33500: loss = 0.000576019287109375 (2203.13 steps/sec)\n",
      "INFO:tensorflow:global step 33600: loss = 0.0014133453369140625 (2204.53 steps/sec)\n",
      "INFO:tensorflow:global step 33700: loss = 1.5139579772949219e-05 (2206.32 steps/sec)\n",
      "INFO:tensorflow:global step 33800: loss = 0.018768310546875 (2208.06 steps/sec)\n",
      "INFO:tensorflow:global step 33900: loss = 0.0014858245849609375 (2209.94 steps/sec)\n",
      "INFO:tensorflow:global step 34000: loss = 0.00562286376953125 (2211.67 steps/sec)\n",
      "INFO:tensorflow:global step 34100: loss = 4.0531158447265625e-06 (2213.42 steps/sec)\n",
      "INFO:tensorflow:global step 34200: loss = 0.0006127357482910156 (2215.13 steps/sec)\n",
      "INFO:tensorflow:global step 34300: loss = 0.003391265869140625 (2217.02 steps/sec)\n",
      "INFO:tensorflow:global step 34400: loss = 4.392862319946289e-05 (2218.79 steps/sec)\n",
      "INFO:tensorflow:global step 34500: loss = 0.024658203125 (2220.68 steps/sec)\n",
      "INFO:tensorflow:global step 34600: loss = 0.0008516311645507812 (2222.32 steps/sec)\n",
      "INFO:tensorflow:global step 34700: loss = 0.0032138824462890625 (2224.16 steps/sec)\n",
      "INFO:tensorflow:global step 34800: loss = 0.006877899169921875 (2225.87 steps/sec)\n",
      "INFO:tensorflow:global step 34900: loss = 2.682209014892578e-06 (2227.69 steps/sec)\n",
      "INFO:tensorflow:global step 35000: loss = 0.00026726722717285156 (2229.39 steps/sec)\n",
      "INFO:tensorflow:global step 35100: loss = 0.001163482666015625 (2231.09 steps/sec)\n",
      "INFO:tensorflow:global step 35200: loss = 0.0014791488647460938 (2232.47 steps/sec)\n",
      "INFO:tensorflow:global step 35300: loss = 2.5570392608642578e-05 (2234.04 steps/sec)\n",
      "INFO:tensorflow:global step 35400: loss = 0.00033736228942871094 (2235.42 steps/sec)\n",
      "INFO:tensorflow:global step 35500: loss = 5.364418029785156e-06 (2236.96 steps/sec)\n",
      "INFO:tensorflow:global step 35600: loss = 0.0008392333984375 (2238.27 steps/sec)\n",
      "INFO:tensorflow:global step 35700: loss = 2.384185791015625e-07 (2239.84 steps/sec)\n",
      "INFO:tensorflow:global step 35800: loss = 8.404254913330078e-06 (2241.31 steps/sec)\n",
      "INFO:tensorflow:global step 35900: loss = 0.000232696533203125 (2242.8 steps/sec)\n",
      "INFO:tensorflow:global step 36000: loss = 2.384185791015625e-07 (2244.18 steps/sec)\n",
      "INFO:tensorflow:global step 36100: loss = 0.0198516845703125 (2245.49 steps/sec)\n",
      "INFO:tensorflow:global step 36200: loss = 7.748603820800781e-07 (2246.62 steps/sec)\n",
      "INFO:tensorflow:global step 36300: loss = 8.83340835571289e-05 (2247.95 steps/sec)\n",
      "INFO:tensorflow:global step 36400: loss = 0.00045871734619140625 (2249.12 steps/sec)\n",
      "INFO:tensorflow:global step 36500: loss = 3.2782554626464844e-06 (2250.53 steps/sec)\n",
      "INFO:tensorflow:global step 36600: loss = 8.487701416015625e-05 (2251.54 steps/sec)\n",
      "INFO:tensorflow:global step 36700: loss = 0.031829833984375 (2252.92 steps/sec)\n",
      "INFO:tensorflow:global step 36800: loss = 0.0003426074981689453 (2254.08 steps/sec)\n",
      "INFO:tensorflow:global step 36900: loss = 2.0265579223632812e-06 (2255.48 steps/sec)\n",
      "INFO:tensorflow:global step 37000: loss = 0.04205322265625 (2256.76 steps/sec)\n",
      "INFO:tensorflow:global step 37100: loss = 4.845857620239258e-05 (2258.12 steps/sec)\n",
      "INFO:tensorflow:global step 37200: loss = 0.0037593841552734375 (2259.38 steps/sec)\n",
      "INFO:tensorflow:global step 37300: loss = 0.00011402368545532227 (2260.79 steps/sec)\n",
      "INFO:tensorflow:global step 37400: loss = 2.4139881134033203e-05 (2262.02 steps/sec)\n",
      "INFO:tensorflow:global step 37500: loss = 3.7789344787597656e-05 (2263.33 steps/sec)\n",
      "INFO:tensorflow:global step 37600: loss = 2.384185791015625e-06 (2264.66 steps/sec)\n",
      "INFO:tensorflow:global step 37700: loss = 0.035552978515625 (2265.94 steps/sec)\n",
      "INFO:tensorflow:global step 37800: loss = 0.01380157470703125 (2267.33 steps/sec)\n",
      "INFO:tensorflow:global step 37900: loss = 1.245737075805664e-05 (2268.49 steps/sec)\n",
      "INFO:tensorflow:global step 38000: loss = 0.0001004934310913086 (2269.91 steps/sec)\n",
      "INFO:tensorflow:global step 38100: loss = 0.003322601318359375 (2271.07 steps/sec)\n",
      "INFO:tensorflow:global step 38200: loss = 5.245208740234375e-06 (2272.42 steps/sec)\n",
      "INFO:tensorflow:global step 38300: loss = 0.00769805908203125 (2273.51 steps/sec)\n",
      "INFO:tensorflow:global step 38400: loss = 1.6570091247558594e-05 (2274.79 steps/sec)\n",
      "INFO:tensorflow:global step 38500: loss = 2.9265880584716797e-05 (2275.96 steps/sec)\n",
      "INFO:tensorflow:global step 38600: loss = 0.01010894775390625 (2276.91 steps/sec)\n",
      "INFO:tensorflow:global step 38700: loss = 0.0003342628479003906 (2278.04 steps/sec)\n",
      "INFO:tensorflow:global step 38800: loss = 6.210803985595703e-05 (2279.31 steps/sec)\n",
      "INFO:tensorflow:global step 38900: loss = 4.827976226806641e-06 (2280.45 steps/sec)\n",
      "INFO:tensorflow:global step 39000: loss = 5.835294723510742e-05 (2281.68 steps/sec)\n",
      "INFO:tensorflow:global step 39100: loss = 0.00614166259765625 (2282.5 steps/sec)\n",
      "INFO:tensorflow:global step 39200: loss = 0.003986358642578125 (2283.72 steps/sec)\n",
      "INFO:tensorflow:global step 39300: loss = 0.0003075599670410156 (2284.8 steps/sec)\n",
      "INFO:tensorflow:global step 39400: loss = 0.0011243820190429688 (2286.03 steps/sec)\n",
      "INFO:tensorflow:global step 39500: loss = 0.0111083984375 (2287.13 steps/sec)\n",
      "INFO:tensorflow:global step 39600: loss = 4.231929779052734e-06 (2288.29 steps/sec)\n",
      "INFO:tensorflow:global step 39700: loss = 0.0004963874816894531 (2289.39 steps/sec)\n",
      "INFO:tensorflow:global step 39800: loss = 7.832050323486328e-05 (2290.61 steps/sec)\n",
      "INFO:tensorflow:global step 39900: loss = 1.4841556549072266e-05 (2291.71 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 40000...\n",
      "INFO:tensorflow:Saving checkpoints for 40000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 40000...\n",
      "INFO:tensorflow:global step 40000: loss = 0.0013256072998046875 (2282.3 steps/sec)\n",
      "INFO:tensorflow:global step 40100: loss = 0.005092620849609375 (2283.34 steps/sec)\n",
      "INFO:tensorflow:global step 40200: loss = 0.0001379251480102539 (2284.67 steps/sec)\n",
      "INFO:tensorflow:global step 40300: loss = 0.0045013427734375 (2285.74 steps/sec)\n",
      "INFO:tensorflow:global step 40400: loss = 0.0007328987121582031 (2287.03 steps/sec)\n",
      "INFO:tensorflow:global step 40500: loss = 0.015777587890625 (2288.22 steps/sec)\n",
      "INFO:tensorflow:global step 40600: loss = 0.0032405853271484375 (2289.47 steps/sec)\n",
      "INFO:tensorflow:global step 40700: loss = 4.076957702636719e-05 (2290.6 steps/sec)\n",
      "INFO:tensorflow:global step 40800: loss = 0.00032138824462890625 (2291.9 steps/sec)\n",
      "INFO:tensorflow:global step 40900: loss = 0.0088348388671875 (2292.95 steps/sec)\n",
      "INFO:tensorflow:global step 41000: loss = 0.00363922119140625 (2294.11 steps/sec)\n",
      "INFO:tensorflow:global step 41100: loss = 1.0848045349121094e-05 (2295.05 steps/sec)\n",
      "INFO:tensorflow:global step 41200: loss = 0.0001533031463623047 (2296.32 steps/sec)\n",
      "INFO:tensorflow:global step 41300: loss = 0.0008020401000976562 (2297.18 steps/sec)\n",
      "INFO:tensorflow:global step 41400: loss = 1.5676021575927734e-05 (2298.27 steps/sec)\n",
      "INFO:tensorflow:global step 41500: loss = 6.151199340820312e-05 (2299.34 steps/sec)\n",
      "INFO:tensorflow:global step 41600: loss = 0.000705718994140625 (2300.64 steps/sec)\n",
      "INFO:tensorflow:global step 41700: loss = 0.0055999755859375 (2301.91 steps/sec)\n",
      "INFO:tensorflow:global step 41800: loss = 1.1920928955078125e-06 (2303.21 steps/sec)\n",
      "INFO:tensorflow:global step 41900: loss = 0.00019347667694091797 (2304.64 steps/sec)\n",
      "INFO:tensorflow:global step 42000: loss = 0.0625 (2305.96 steps/sec)\n",
      "INFO:tensorflow:global step 42100: loss = 0.0002639293670654297 (2307.29 steps/sec)\n",
      "INFO:tensorflow:global step 42200: loss = 1.4901161193847656e-06 (2308.57 steps/sec)\n",
      "INFO:tensorflow:global step 42300: loss = 0.00011372566223144531 (2309.99 steps/sec)\n",
      "INFO:tensorflow:global step 42400: loss = 0.0015993118286132812 (2311.27 steps/sec)\n",
      "INFO:tensorflow:global step 42500: loss = 0.005481719970703125 (2312.63 steps/sec)\n",
      "INFO:tensorflow:global step 42600: loss = 0.01446533203125 (2313.85 steps/sec)\n",
      "INFO:tensorflow:global step 42700: loss = 8.219480514526367e-05 (2315.25 steps/sec)\n",
      "INFO:tensorflow:global step 42800: loss = 0.00014591217041015625 (2316.46 steps/sec)\n",
      "INFO:tensorflow:global step 42900: loss = 5.602836608886719e-06 (2317.82 steps/sec)\n",
      "INFO:tensorflow:global step 43000: loss = 6.693601608276367e-05 (2319.06 steps/sec)\n",
      "INFO:tensorflow:global step 43100: loss = 0.0005764961242675781 (2320.16 steps/sec)\n",
      "INFO:tensorflow:global step 43200: loss = 0.01071929931640625 (2321.3 steps/sec)\n",
      "INFO:tensorflow:global step 43300: loss = 0.0150604248046875 (2322.53 steps/sec)\n",
      "INFO:tensorflow:global step 43400: loss = 0.0006008148193359375 (2323.74 steps/sec)\n",
      "INFO:tensorflow:global step 43500: loss = 0.01465606689453125 (2325.07 steps/sec)\n",
      "INFO:tensorflow:global step 43600: loss = 2.9087066650390625e-05 (2326.21 steps/sec)\n",
      "INFO:tensorflow:global step 43700: loss = 0.00015807151794433594 (2327.49 steps/sec)\n",
      "INFO:tensorflow:global step 43800: loss = 0.0165863037109375 (2328.65 steps/sec)\n",
      "INFO:tensorflow:global step 43900: loss = 0.01512908935546875 (2329.94 steps/sec)\n",
      "INFO:tensorflow:global step 44000: loss = 0.0028362274169921875 (2331.06 steps/sec)\n",
      "INFO:tensorflow:global step 44100: loss = 0.003143310546875 (2332.2 steps/sec)\n",
      "INFO:tensorflow:global step 44200: loss = 0.00011420249938964844 (2333.29 steps/sec)\n",
      "INFO:tensorflow:global step 44300: loss = 0.0005116462707519531 (2334.59 steps/sec)\n",
      "INFO:tensorflow:global step 44400: loss = 0.0310821533203125 (2335.15 steps/sec)\n",
      "INFO:tensorflow:global step 44500: loss = 0.00028252601623535156 (2336.18 steps/sec)\n",
      "INFO:tensorflow:global step 44600: loss = 0.00011050701141357422 (2337.01 steps/sec)\n",
      "INFO:tensorflow:global step 44700: loss = 1.138448715209961e-05 (2338.2 steps/sec)\n",
      "INFO:tensorflow:global step 44800: loss = 2.467632293701172e-05 (2339.26 steps/sec)\n",
      "INFO:tensorflow:global step 44900: loss = 0.0008716583251953125 (2340.35 steps/sec)\n",
      "INFO:tensorflow:global step 45000: loss = 2.562999725341797e-05 (2341.37 steps/sec)\n",
      "INFO:tensorflow:global step 45100: loss = 5.960464477539063e-08 (2342.34 steps/sec)\n",
      "INFO:tensorflow:global step 45200: loss = 0.00130462646484375 (2343.43 steps/sec)\n",
      "INFO:tensorflow:global step 45300: loss = 0.000530242919921875 (2344.45 steps/sec)\n",
      "INFO:tensorflow:global step 45400: loss = 1.1324882507324219e-06 (2345.42 steps/sec)\n",
      "INFO:tensorflow:global step 45500: loss = 1.3172626495361328e-05 (2346.59 steps/sec)\n",
      "INFO:tensorflow:global step 45600: loss = 0.0455322265625 (2347.33 steps/sec)\n",
      "INFO:tensorflow:global step 45700: loss = 8.940696716308594e-07 (2348.38 steps/sec)\n",
      "INFO:tensorflow:global step 45800: loss = 2.9206275939941406e-06 (2349.27 steps/sec)\n",
      "INFO:tensorflow:global step 45900: loss = 6.556510925292969e-07 (2350.18 steps/sec)\n",
      "INFO:tensorflow:global step 46000: loss = 7.49826431274414e-05 (2351.21 steps/sec)\n",
      "INFO:tensorflow:global step 46100: loss = 0.005718231201171875 (2352.04 steps/sec)\n",
      "INFO:tensorflow:global step 46200: loss = 1.0728836059570312e-06 (2353.16 steps/sec)\n",
      "INFO:tensorflow:global step 46300: loss = 0.0006241798400878906 (2354.05 steps/sec)\n",
      "INFO:tensorflow:global step 46400: loss = 0.001285552978515625 (2355.15 steps/sec)\n",
      "INFO:tensorflow:global step 46500: loss = 0.034942626953125 (2356.11 steps/sec)\n",
      "INFO:tensorflow:global step 46600: loss = 0.0003371238708496094 (2357.08 steps/sec)\n",
      "INFO:tensorflow:global step 46700: loss = 0.007518768310546875 (2357.89 steps/sec)\n",
      "INFO:tensorflow:global step 46800: loss = 1.1920928955078125e-06 (2358.86 steps/sec)\n",
      "INFO:tensorflow:global step 46900: loss = 4.410743713378906e-06 (2359.8 steps/sec)\n",
      "INFO:tensorflow:global step 47000: loss = 0.0004363059997558594 (2360.73 steps/sec)\n",
      "INFO:tensorflow:global step 47100: loss = 1.3053417205810547e-05 (2361.54 steps/sec)\n",
      "INFO:tensorflow:global step 47200: loss = 8.565187454223633e-05 (2362.65 steps/sec)\n",
      "INFO:tensorflow:global step 47300: loss = 0.037841796875 (2363.52 steps/sec)\n",
      "INFO:tensorflow:global step 47400: loss = 2.1457672119140625e-06 (2364.52 steps/sec)\n",
      "INFO:tensorflow:global step 47500: loss = 3.731250762939453e-05 (2365.24 steps/sec)\n",
      "INFO:tensorflow:global step 47600: loss = 9.495019912719727e-05 (2366.52 steps/sec)\n",
      "INFO:tensorflow:global step 47700: loss = 0.002201080322265625 (2367.7 steps/sec)\n",
      "INFO:tensorflow:global step 47800: loss = 3.874301910400391e-06 (2369.22 steps/sec)\n",
      "INFO:tensorflow:global step 47900: loss = 4.1484832763671875e-05 (2370.38 steps/sec)\n",
      "INFO:tensorflow:global step 48000: loss = 2.0742416381835938e-05 (2371.67 steps/sec)\n",
      "INFO:tensorflow:global step 48100: loss = 0.000209808349609375 (2372.84 steps/sec)\n",
      "INFO:tensorflow:global step 48200: loss = 3.1948089599609375e-05 (2374.37 steps/sec)\n",
      "INFO:tensorflow:global step 48300: loss = 4.5299530029296875e-06 (2375.78 steps/sec)\n",
      "INFO:tensorflow:global step 48400: loss = 0.0007815361022949219 (2377.2 steps/sec)\n",
      "INFO:tensorflow:global step 48500: loss = 6.556510925292969e-07 (2378.55 steps/sec)\n",
      "INFO:tensorflow:global step 48600: loss = 3.5762786865234375e-07 (2380.08 steps/sec)\n",
      "INFO:tensorflow:global step 48700: loss = 6.145238876342773e-05 (2381.28 steps/sec)\n",
      "INFO:tensorflow:global step 48800: loss = 0.0005440711975097656 (2382.6 steps/sec)\n",
      "INFO:tensorflow:global step 48900: loss = 2.4437904357910156e-06 (2384.05 steps/sec)\n",
      "INFO:tensorflow:global step 49000: loss = 0.0004341602325439453 (2385.58 steps/sec)\n",
      "INFO:tensorflow:global step 49100: loss = 7.480382919311523e-05 (2386.73 steps/sec)\n",
      "INFO:tensorflow:global step 49200: loss = 1.9669532775878906e-06 (2388.29 steps/sec)\n",
      "INFO:tensorflow:global step 49300: loss = 4.190206527709961e-05 (2389.6 steps/sec)\n",
      "INFO:tensorflow:global step 49400: loss = 0.00011330842971801758 (2390.89 steps/sec)\n",
      "INFO:tensorflow:global step 49500: loss = 2.682209014892578e-06 (2392.25 steps/sec)\n",
      "INFO:tensorflow:global step 49600: loss = 0.0013580322265625 (2393.39 steps/sec)\n",
      "INFO:tensorflow:global step 49700: loss = 0.0006432533264160156 (2394.69 steps/sec)\n",
      "INFO:tensorflow:global step 49800: loss = 4.941225051879883e-05 (2396.0 steps/sec)\n",
      "INFO:tensorflow:global step 49900: loss = 9.5367431640625e-07 (2397.42 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 50000...\n",
      "INFO:tensorflow:Saving checkpoints for 50000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 50000...\n",
      "INFO:tensorflow:global step 50000: loss = 0.0045623779296875 (2321.51 steps/sec)\n",
      "INFO:tensorflow:global step 50100: loss = 2.3603439331054688e-05 (2322.32 steps/sec)\n",
      "INFO:tensorflow:global step 50200: loss = 1.2874603271484375e-05 (2323.28 steps/sec)\n",
      "INFO:tensorflow:global step 50300: loss = 0.00010305643081665039 (2324.39 steps/sec)\n",
      "INFO:tensorflow:global step 50400: loss = 7.569789886474609e-06 (2325.27 steps/sec)\n",
      "INFO:tensorflow:global step 50500: loss = 0.00022649765014648438 (2326.27 steps/sec)\n",
      "INFO:tensorflow:global step 50600: loss = 0.032318115234375 (2327.02 steps/sec)\n",
      "INFO:tensorflow:global step 50700: loss = 0.00018596649169921875 (2327.96 steps/sec)\n",
      "INFO:tensorflow:global step 50800: loss = 9.000301361083984e-06 (2328.67 steps/sec)\n",
      "INFO:tensorflow:global step 50900: loss = 1.537799835205078e-05 (2329.62 steps/sec)\n",
      "INFO:tensorflow:global step 51000: loss = 1.901388168334961e-05 (2330.5 steps/sec)\n",
      "INFO:tensorflow:global step 51100: loss = 0.0021495819091796875 (2330.95 steps/sec)\n",
      "INFO:tensorflow:global step 51200: loss = 0.055023193359375 (2331.61 steps/sec)\n",
      "INFO:tensorflow:global step 51300: loss = 0.0300140380859375 (2332.63 steps/sec)\n",
      "INFO:tensorflow:global step 51400: loss = 6.335973739624023e-05 (2333.43 steps/sec)\n",
      "INFO:tensorflow:global step 51500: loss = 0.0171966552734375 (2334.45 steps/sec)\n",
      "INFO:tensorflow:global step 51600: loss = 0.00016248226165771484 (2335.29 steps/sec)\n",
      "INFO:tensorflow:global step 51700: loss = 8.404254913330078e-06 (2336.3 steps/sec)\n",
      "INFO:tensorflow:global step 51800: loss = 4.410743713378906e-06 (2337.08 steps/sec)\n",
      "INFO:tensorflow:global step 51900: loss = 0.00113677978515625 (2337.91 steps/sec)\n",
      "INFO:tensorflow:global step 52000: loss = 0.0157623291015625 (2338.75 steps/sec)\n",
      "INFO:tensorflow:global step 52100: loss = 2.0265579223632812e-06 (2339.76 steps/sec)\n",
      "INFO:tensorflow:global step 52200: loss = 0.0004248619079589844 (2340.65 steps/sec)\n",
      "INFO:tensorflow:global step 52300: loss = 1.0132789611816406e-06 (2341.81 steps/sec)\n",
      "INFO:tensorflow:global step 52400: loss = 0.0001493692398071289 (2343.17 steps/sec)\n",
      "INFO:tensorflow:global step 52500: loss = 7.718801498413086e-05 (2344.4 steps/sec)\n",
      "INFO:tensorflow:global step 52600: loss = 0.0001499652862548828 (2345.39 steps/sec)\n",
      "INFO:tensorflow:global step 52700: loss = 0.01126861572265625 (2346.58 steps/sec)\n",
      "INFO:tensorflow:global step 52800: loss = 0.0310211181640625 (2347.43 steps/sec)\n",
      "INFO:tensorflow:global step 52900: loss = 3.7550926208496094e-06 (2348.69 steps/sec)\n",
      "INFO:tensorflow:global step 53000: loss = 9.5367431640625e-07 (2349.65 steps/sec)\n",
      "INFO:tensorflow:global step 53100: loss = 9.47713851928711e-06 (2350.75 steps/sec)\n",
      "INFO:tensorflow:global step 53200: loss = 5.960464477539063e-08 (2351.94 steps/sec)\n",
      "INFO:tensorflow:global step 53300: loss = 0.0015106201171875 (2353.17 steps/sec)\n",
      "INFO:tensorflow:global step 53400: loss = 0.01910400390625 (2354.26 steps/sec)\n",
      "INFO:tensorflow:global step 53500: loss = 0.0014934539794921875 (2355.52 steps/sec)\n",
      "INFO:tensorflow:global step 53600: loss = 0.00014710426330566406 (2356.74 steps/sec)\n",
      "INFO:tensorflow:global step 53700: loss = 0.0004317760467529297 (2358.05 steps/sec)\n",
      "INFO:tensorflow:global step 53800: loss = 1.1205673217773438e-05 (2359.29 steps/sec)\n",
      "INFO:tensorflow:global step 53900: loss = 5.960464477539063e-08 (2360.55 steps/sec)\n",
      "INFO:tensorflow:global step 54000: loss = 8.58306884765625e-06 (2361.8 steps/sec)\n",
      "INFO:tensorflow:global step 54100: loss = 6.61611557006836e-05 (2362.82 steps/sec)\n",
      "INFO:tensorflow:global step 54200: loss = 0.0015125274658203125 (2364.13 steps/sec)\n",
      "INFO:tensorflow:global step 54300: loss = 0.0 (2365.34 steps/sec)\n",
      "INFO:tensorflow:global step 54400: loss = 0.0013551712036132812 (2366.55 steps/sec)\n",
      "INFO:tensorflow:global step 54500: loss = 3.409385681152344e-05 (2367.83 steps/sec)\n",
      "INFO:tensorflow:global step 54600: loss = 0.0001697540283203125 (2369.14 steps/sec)\n",
      "INFO:tensorflow:global step 54700: loss = 0.00269317626953125 (2370.47 steps/sec)\n",
      "INFO:tensorflow:global step 54800: loss = 0.0002613067626953125 (2371.92 steps/sec)\n",
      "INFO:tensorflow:global step 54900: loss = 7.331371307373047e-06 (2373.28 steps/sec)\n",
      "INFO:tensorflow:global step 55000: loss = 0.00018334388732910156 (2374.71 steps/sec)\n",
      "INFO:tensorflow:global step 55100: loss = 1.7881393432617188e-07 (2375.73 steps/sec)\n",
      "INFO:tensorflow:global step 55200: loss = 0.0109710693359375 (2376.97 steps/sec)\n",
      "INFO:tensorflow:global step 55300: loss = 0.029815673828125 (2378.09 steps/sec)\n",
      "INFO:tensorflow:global step 55400: loss = 0.0032634735107421875 (2379.24 steps/sec)\n",
      "INFO:tensorflow:global step 55500: loss = 1.6093254089355469e-06 (2380.34 steps/sec)\n",
      "INFO:tensorflow:global step 55600: loss = 0.002239227294921875 (2381.47 steps/sec)\n",
      "INFO:tensorflow:global step 55700: loss = 0.0003421306610107422 (2382.58 steps/sec)\n",
      "INFO:tensorflow:global step 55800: loss = 1.1622905731201172e-05 (2383.82 steps/sec)\n",
      "INFO:tensorflow:global step 55900: loss = 2.5033950805664062e-06 (2385.02 steps/sec)\n",
      "INFO:tensorflow:global step 56000: loss = 3.4570693969726562e-06 (2386.22 steps/sec)\n",
      "INFO:tensorflow:global step 56100: loss = 0.007091522216796875 (2387.33 steps/sec)\n",
      "INFO:tensorflow:global step 56200: loss = 0.004459381103515625 (2388.59 steps/sec)\n",
      "INFO:tensorflow:global step 56300: loss = 4.470348358154297e-06 (2389.82 steps/sec)\n",
      "INFO:tensorflow:global step 56400: loss = 1.2278556823730469e-05 (2391.16 steps/sec)\n",
      "INFO:tensorflow:global step 56500: loss = 6.794929504394531e-06 (2392.43 steps/sec)\n",
      "INFO:tensorflow:global step 56600: loss = 6.705522537231445e-05 (2393.72 steps/sec)\n",
      "INFO:tensorflow:global step 56700: loss = 0.0010995864868164062 (2394.97 steps/sec)\n",
      "INFO:tensorflow:global step 56800: loss = 0.0004591941833496094 (2396.33 steps/sec)\n",
      "INFO:tensorflow:global step 56900: loss = 3.5762786865234375e-07 (2397.58 steps/sec)\n",
      "INFO:tensorflow:global step 57000: loss = 0.0 (2398.9 steps/sec)\n",
      "INFO:tensorflow:global step 57100: loss = 0.01476287841796875 (2399.92 steps/sec)\n",
      "INFO:tensorflow:global step 57200: loss = 0.00014543533325195312 (2401.23 steps/sec)\n",
      "INFO:tensorflow:global step 57300: loss = 0.00011140108108520508 (2402.48 steps/sec)\n",
      "INFO:tensorflow:global step 57400: loss = 0.0070343017578125 (2403.82 steps/sec)\n",
      "INFO:tensorflow:global step 57500: loss = 0.0006213188171386719 (2405.07 steps/sec)\n",
      "INFO:tensorflow:global step 57600: loss = 0.003673553466796875 (2406.31 steps/sec)\n",
      "INFO:tensorflow:global step 57700: loss = 0.00022804737091064453 (2407.52 steps/sec)\n",
      "INFO:tensorflow:global step 57800: loss = 4.017353057861328e-05 (2408.87 steps/sec)\n",
      "INFO:tensorflow:global step 57900: loss = 0.005039215087890625 (2410.09 steps/sec)\n",
      "INFO:tensorflow:global step 58000: loss = 5.7220458984375e-06 (2411.4 steps/sec)\n",
      "INFO:tensorflow:global step 58100: loss = 0.0003657341003417969 (2412.36 steps/sec)\n",
      "INFO:tensorflow:global step 58200: loss = 1.0728836059570312e-06 (2413.48 steps/sec)\n",
      "INFO:tensorflow:global step 58300: loss = 8.940696716308594e-07 (2414.54 steps/sec)\n",
      "INFO:tensorflow:global step 58400: loss = 4.708766937255859e-06 (2415.51 steps/sec)\n",
      "INFO:tensorflow:global step 58500: loss = 0.01324462890625 (2416.56 steps/sec)\n",
      "INFO:tensorflow:global step 58600: loss = 2.568960189819336e-05 (2417.41 steps/sec)\n",
      "INFO:tensorflow:global step 58700: loss = 0.0227813720703125 (2418.52 steps/sec)\n",
      "INFO:tensorflow:global step 58800: loss = 5.960464477539063e-08 (2419.45 steps/sec)\n",
      "INFO:tensorflow:global step 58900: loss = 0.0072021484375 (2420.51 steps/sec)\n",
      "INFO:tensorflow:global step 59000: loss = 1.8775463104248047e-05 (2421.43 steps/sec)\n",
      "INFO:tensorflow:global step 59100: loss = 0.00791168212890625 (2422.48 steps/sec)\n",
      "INFO:tensorflow:global step 59200: loss = 0.0002696514129638672 (2423.43 steps/sec)\n",
      "INFO:tensorflow:global step 59300: loss = 2.5331974029541016e-05 (2424.49 steps/sec)\n",
      "INFO:tensorflow:global step 59400: loss = 6.556510925292969e-07 (2425.51 steps/sec)\n",
      "INFO:tensorflow:global step 59500: loss = 0.0258941650390625 (2426.81 steps/sec)\n",
      "INFO:tensorflow:global step 59600: loss = 3.8564205169677734e-05 (2427.82 steps/sec)\n",
      "INFO:tensorflow:global step 59700: loss = 1.1920928955078125e-07 (2428.95 steps/sec)\n",
      "INFO:tensorflow:global step 59800: loss = 1.722574234008789e-05 (2429.98 steps/sec)\n",
      "INFO:tensorflow:global step 59900: loss = 3.904104232788086e-05 (2431.12 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 60000...\n",
      "INFO:tensorflow:Saving checkpoints for 60000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 60000...\n",
      "INFO:tensorflow:global step 60000: loss = 0.0031604766845703125 (2318.53 steps/sec)\n",
      "INFO:tensorflow:global step 60100: loss = 3.516674041748047e-05 (2319.24 steps/sec)\n",
      "INFO:tensorflow:global step 60200: loss = 1.9431114196777344e-05 (2319.97 steps/sec)\n",
      "INFO:tensorflow:global step 60300: loss = 0.0026111602783203125 (2320.81 steps/sec)\n",
      "INFO:tensorflow:global step 60400: loss = 1.4543533325195312e-05 (2321.5 steps/sec)\n",
      "INFO:tensorflow:global step 60500: loss = 9.179115295410156e-06 (2322.33 steps/sec)\n",
      "INFO:tensorflow:global step 60600: loss = 4.8279762268066406e-05 (2322.95 steps/sec)\n",
      "INFO:tensorflow:global step 60700: loss = 5.7220458984375e-06 (2323.81 steps/sec)\n",
      "INFO:tensorflow:global step 60800: loss = 4.827976226806641e-06 (2324.46 steps/sec)\n",
      "INFO:tensorflow:global step 60900: loss = 5.960464477539063e-08 (2325.32 steps/sec)\n",
      "INFO:tensorflow:global step 61000: loss = 0.04351806640625 (2326.07 steps/sec)\n",
      "INFO:tensorflow:global step 61100: loss = 0.03662109375 (2326.99 steps/sec)\n",
      "INFO:tensorflow:global step 61200: loss = 0.001354217529296875 (2327.78 steps/sec)\n",
      "INFO:tensorflow:global step 61300: loss = 2.980232238769531e-07 (2328.72 steps/sec)\n",
      "INFO:tensorflow:global step 61400: loss = 0.00872802734375 (2329.51 steps/sec)\n",
      "INFO:tensorflow:global step 61500: loss = 3.6716461181640625e-05 (2330.42 steps/sec)\n",
      "INFO:tensorflow:global step 61600: loss = 0.0008172988891601562 (2331.2 steps/sec)\n",
      "INFO:tensorflow:global step 61700: loss = 0.00043320655822753906 (2332.08 steps/sec)\n",
      "INFO:tensorflow:global step 61800: loss = 1.7881393432617188e-07 (2332.88 steps/sec)\n",
      "INFO:tensorflow:global step 61900: loss = 6.443262100219727e-05 (2333.77 steps/sec)\n",
      "INFO:tensorflow:global step 62000: loss = 1.329183578491211e-05 (2334.68 steps/sec)\n",
      "INFO:tensorflow:global step 62100: loss = 0.00035881996154785156 (2335.46 steps/sec)\n",
      "INFO:tensorflow:global step 62200: loss = 9.512901306152344e-05 (2336.24 steps/sec)\n",
      "INFO:tensorflow:global step 62300: loss = 0.00820159912109375 (2337.12 steps/sec)\n",
      "INFO:tensorflow:global step 62400: loss = 0.04638671875 (2337.87 steps/sec)\n",
      "INFO:tensorflow:global step 62500: loss = 0.004619598388671875 (2338.64 steps/sec)\n",
      "INFO:tensorflow:global step 62600: loss = 0.0030002593994140625 (2339.35 steps/sec)\n",
      "INFO:tensorflow:global step 62700: loss = 9.119510650634766e-06 (2340.07 steps/sec)\n",
      "INFO:tensorflow:global step 62800: loss = 2.47955322265625e-05 (2340.89 steps/sec)\n",
      "INFO:tensorflow:global step 62900: loss = 3.159046173095703e-06 (2341.6 steps/sec)\n",
      "INFO:tensorflow:global step 63000: loss = 0.001384735107421875 (2342.4 steps/sec)\n",
      "INFO:tensorflow:global step 63100: loss = 0.0006265640258789062 (2343.08 steps/sec)\n",
      "INFO:tensorflow:global step 63200: loss = 0.0013914108276367188 (2343.96 steps/sec)\n",
      "INFO:tensorflow:global step 63300: loss = 1.0192394256591797e-05 (2344.65 steps/sec)\n",
      "INFO:tensorflow:global step 63400: loss = 0.00010335445404052734 (2345.44 steps/sec)\n",
      "INFO:tensorflow:global step 63500: loss = 0.00034356117248535156 (2346.15 steps/sec)\n",
      "INFO:tensorflow:global step 63600: loss = 8.702278137207031e-06 (2346.87 steps/sec)\n",
      "INFO:tensorflow:global step 63700: loss = 4.76837158203125e-07 (2347.6 steps/sec)\n",
      "INFO:tensorflow:global step 63800: loss = 0.0002815723419189453 (2348.36 steps/sec)\n",
      "INFO:tensorflow:global step 63900: loss = 3.5762786865234375e-07 (2349.1 steps/sec)\n",
      "INFO:tensorflow:global step 64000: loss = 7.510185241699219e-06 (2349.94 steps/sec)\n",
      "INFO:tensorflow:global step 64100: loss = 3.2782554626464844e-06 (2350.66 steps/sec)\n",
      "INFO:tensorflow:global step 64200: loss = 0.0 (2351.51 steps/sec)\n",
      "INFO:tensorflow:global step 64300: loss = 1.3470649719238281e-05 (2352.27 steps/sec)\n",
      "INFO:tensorflow:global step 64400: loss = 0.00022399425506591797 (2353.15 steps/sec)\n",
      "INFO:tensorflow:global step 64500: loss = 4.470348358154297e-06 (2353.91 steps/sec)\n",
      "INFO:tensorflow:global step 64600: loss = 0.0010890960693359375 (2354.71 steps/sec)\n",
      "INFO:tensorflow:global step 64700: loss = 3.7550926208496094e-06 (2355.48 steps/sec)\n",
      "INFO:tensorflow:global step 64800: loss = 3.218650817871094e-05 (2356.35 steps/sec)\n",
      "INFO:tensorflow:global step 64900: loss = 1.6093254089355469e-06 (2357.08 steps/sec)\n",
      "INFO:tensorflow:global step 65000: loss = 3.0159950256347656e-05 (2357.95 steps/sec)\n",
      "INFO:tensorflow:global step 65100: loss = 0.00019180774688720703 (2358.65 steps/sec)\n",
      "INFO:tensorflow:global step 65200: loss = 8.285045623779297e-06 (2359.42 steps/sec)\n",
      "INFO:tensorflow:global step 65300: loss = 2.9802322387695312e-06 (2360.06 steps/sec)\n",
      "INFO:tensorflow:global step 65400: loss = 9.435415267944336e-05 (2360.78 steps/sec)\n",
      "INFO:tensorflow:global step 65500: loss = 3.7789344787597656e-05 (2361.39 steps/sec)\n",
      "INFO:tensorflow:global step 65600: loss = 1.1920928955078125e-07 (2362.08 steps/sec)\n",
      "INFO:tensorflow:global step 65700: loss = 0.0001837015151977539 (2362.76 steps/sec)\n",
      "INFO:tensorflow:global step 65800: loss = 0.00847625732421875 (2363.5 steps/sec)\n",
      "INFO:tensorflow:global step 65900: loss = 2.384185791015625e-07 (2364.06 steps/sec)\n",
      "INFO:tensorflow:global step 66000: loss = 5.841255187988281e-06 (2364.84 steps/sec)\n",
      "INFO:tensorflow:global step 66100: loss = 1.1324882507324219e-06 (2365.13 steps/sec)\n",
      "INFO:tensorflow:global step 66200: loss = 0.01214599609375 (2365.87 steps/sec)\n",
      "INFO:tensorflow:global step 66300: loss = 4.3332576751708984e-05 (2366.58 steps/sec)\n",
      "INFO:tensorflow:global step 66400: loss = 0.025634765625 (2367.45 steps/sec)\n",
      "INFO:tensorflow:global step 66500: loss = 0.00020515918731689453 (2368.15 steps/sec)\n",
      "INFO:tensorflow:global step 66600: loss = 2.4199485778808594e-05 (2368.83 steps/sec)\n",
      "INFO:tensorflow:global step 66700: loss = 0.00044918060302734375 (2369.47 steps/sec)\n",
      "INFO:tensorflow:global step 66800: loss = 1.7285346984863281e-06 (2370.1 steps/sec)\n",
      "INFO:tensorflow:global step 66900: loss = 3.7729740142822266e-05 (2370.86 steps/sec)\n",
      "INFO:tensorflow:global step 67000: loss = 0.04888916015625 (2371.55 steps/sec)\n",
      "INFO:tensorflow:global step 67100: loss = 0.0146942138671875 (2372.25 steps/sec)\n",
      "INFO:tensorflow:global step 67200: loss = 1.5556812286376953e-05 (2372.93 steps/sec)\n",
      "INFO:tensorflow:global step 67300: loss = 0.0 (2373.8 steps/sec)\n",
      "INFO:tensorflow:global step 67400: loss = 0.0 (2374.49 steps/sec)\n",
      "INFO:tensorflow:global step 67500: loss = 0.00044035911560058594 (2375.14 steps/sec)\n",
      "INFO:tensorflow:global step 67600: loss = 1.5974044799804688e-05 (2375.65 steps/sec)\n",
      "INFO:tensorflow:global step 67700: loss = 0.0003943443298339844 (2376.37 steps/sec)\n",
      "INFO:tensorflow:global step 67800: loss = 1.9669532775878906e-06 (2376.98 steps/sec)\n",
      "INFO:tensorflow:global step 67900: loss = 0.0023479461669921875 (2377.71 steps/sec)\n",
      "INFO:tensorflow:global step 68000: loss = 2.5272369384765625e-05 (2378.34 steps/sec)\n",
      "INFO:tensorflow:global step 68100: loss = 0.00214385986328125 (2379.02 steps/sec)\n",
      "INFO:tensorflow:global step 68200: loss = 0.0039215087890625 (2379.68 steps/sec)\n",
      "INFO:tensorflow:global step 68300: loss = 2.390146255493164e-05 (2380.47 steps/sec)\n",
      "INFO:tensorflow:global step 68400: loss = 0.03350830078125 (2381.07 steps/sec)\n",
      "INFO:tensorflow:global step 68500: loss = 1.3828277587890625e-05 (2381.79 steps/sec)\n",
      "INFO:tensorflow:global step 68600: loss = 0.0010547637939453125 (2382.41 steps/sec)\n",
      "INFO:tensorflow:global step 68700: loss = 1.8358230590820312e-05 (2383.21 steps/sec)\n",
      "INFO:tensorflow:global step 68800: loss = 0.0011186599731445312 (2383.87 steps/sec)\n",
      "INFO:tensorflow:global step 68900: loss = 0.0018062591552734375 (2384.63 steps/sec)\n",
      "INFO:tensorflow:global step 69000: loss = 2.384185791015625e-06 (2385.23 steps/sec)\n",
      "INFO:tensorflow:global step 69100: loss = 3.0994415283203125e-06 (2385.82 steps/sec)\n",
      "INFO:tensorflow:global step 69200: loss = 5.960464477539063e-08 (2386.39 steps/sec)\n",
      "INFO:tensorflow:global step 69300: loss = 3.5762786865234375e-07 (2387.12 steps/sec)\n",
      "INFO:tensorflow:global step 69400: loss = 0.0018138885498046875 (2387.7 steps/sec)\n",
      "INFO:tensorflow:global step 69500: loss = 8.940696716308594e-07 (2388.38 steps/sec)\n",
      "INFO:tensorflow:global step 69600: loss = 0.008209228515625 (2388.88 steps/sec)\n",
      "INFO:tensorflow:global step 69700: loss = 1.5795230865478516e-05 (2389.63 steps/sec)\n",
      "INFO:tensorflow:global step 69800: loss = 1.6093254089355469e-06 (2390.25 steps/sec)\n",
      "INFO:tensorflow:global step 69900: loss = 2.6166439056396484e-05 (2390.97 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 70000...\n",
      "INFO:tensorflow:Saving checkpoints for 70000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 70000...\n",
      "INFO:tensorflow:global step 70000: loss = 0.0158538818359375 (2284.81 steps/sec)\n",
      "INFO:tensorflow:global step 70100: loss = 5.984306335449219e-05 (2285.37 steps/sec)\n",
      "INFO:tensorflow:global step 70200: loss = 0.0 (2285.99 steps/sec)\n",
      "INFO:tensorflow:global step 70300: loss = 2.0623207092285156e-05 (2286.7 steps/sec)\n",
      "INFO:tensorflow:global step 70400: loss = 0.0001538991928100586 (2287.41 steps/sec)\n",
      "INFO:tensorflow:global step 70500: loss = 0.0117950439453125 (2288.05 steps/sec)\n",
      "INFO:tensorflow:global step 70600: loss = 1.1742115020751953e-05 (2288.73 steps/sec)\n",
      "INFO:tensorflow:global step 70700: loss = 0.0 (2289.49 steps/sec)\n",
      "INFO:tensorflow:global step 70800: loss = 0.0003790855407714844 (2290.18 steps/sec)\n",
      "INFO:tensorflow:global step 70900: loss = 1.3113021850585938e-06 (2290.95 steps/sec)\n",
      "INFO:tensorflow:global step 71000: loss = 1.7881393432617188e-07 (2291.74 steps/sec)\n",
      "INFO:tensorflow:global step 71100: loss = 2.562999725341797e-06 (2292.19 steps/sec)\n",
      "INFO:tensorflow:global step 71200: loss = 0.0003619194030761719 (2292.98 steps/sec)\n",
      "INFO:tensorflow:global step 71300: loss = 5.960464477539063e-08 (2293.68 steps/sec)\n",
      "INFO:tensorflow:global step 71400: loss = 2.9206275939941406e-06 (2294.5 steps/sec)\n",
      "INFO:tensorflow:global step 71500: loss = 0.0 (2295.21 steps/sec)\n",
      "INFO:tensorflow:global step 71600: loss = 0.008392333984375 (2295.83 steps/sec)\n",
      "INFO:tensorflow:global step 71700: loss = 6.973743438720703e-06 (2296.48 steps/sec)\n",
      "INFO:tensorflow:global step 71800: loss = 4.76837158203125e-06 (2297.27 steps/sec)\n",
      "INFO:tensorflow:global step 71900: loss = 0.0036029815673828125 (2297.91 steps/sec)\n",
      "INFO:tensorflow:global step 72000: loss = 0.010498046875 (2298.73 steps/sec)\n",
      "INFO:tensorflow:global step 72100: loss = 5.841255187988281e-06 (2299.42 steps/sec)\n",
      "INFO:tensorflow:global step 72200: loss = 8.344650268554688e-06 (2300.24 steps/sec)\n",
      "INFO:tensorflow:global step 72300: loss = 3.6954879760742188e-06 (2301.05 steps/sec)\n",
      "INFO:tensorflow:global step 72400: loss = 2.586841583251953e-05 (2301.86 steps/sec)\n",
      "INFO:tensorflow:global step 72500: loss = 0.004734039306640625 (2302.61 steps/sec)\n",
      "INFO:tensorflow:global step 72600: loss = 5.602836608886719e-06 (2303.41 steps/sec)\n",
      "INFO:tensorflow:global step 72700: loss = 0.00015795230865478516 (2304.18 steps/sec)\n",
      "INFO:tensorflow:global step 72800: loss = 1.7642974853515625e-05 (2305.0 steps/sec)\n",
      "INFO:tensorflow:global step 72900: loss = 2.5212764739990234e-05 (2305.76 steps/sec)\n",
      "INFO:tensorflow:global step 73000: loss = 0.0225982666015625 (2306.59 steps/sec)\n",
      "INFO:tensorflow:global step 73100: loss = 5.960464477539062e-07 (2307.31 steps/sec)\n",
      "INFO:tensorflow:global step 73200: loss = 0.0006499290466308594 (2308.13 steps/sec)\n",
      "INFO:tensorflow:global step 73300: loss = 2.6226043701171875e-06 (2308.8 steps/sec)\n",
      "INFO:tensorflow:global step 73400: loss = 0.00021147727966308594 (2309.53 steps/sec)\n",
      "INFO:tensorflow:global step 73500: loss = 0.0 (2310.15 steps/sec)\n",
      "INFO:tensorflow:global step 73600: loss = 3.731250762939453e-05 (2310.85 steps/sec)\n",
      "INFO:tensorflow:global step 73700: loss = 0.0013866424560546875 (2311.49 steps/sec)\n",
      "INFO:tensorflow:global step 73800: loss = 1.7881393432617188e-07 (2312.25 steps/sec)\n",
      "INFO:tensorflow:global step 73900: loss = 0.025299072265625 (2312.89 steps/sec)\n",
      "INFO:tensorflow:global step 74000: loss = 6.794929504394531e-06 (2313.59 steps/sec)\n",
      "INFO:tensorflow:global step 74100: loss = 4.410743713378906e-05 (2314.28 steps/sec)\n",
      "INFO:tensorflow:global step 74200: loss = 1.7881393432617188e-06 (2315.37 steps/sec)\n",
      "INFO:tensorflow:global step 74300: loss = 8.225440979003906e-06 (2316.19 steps/sec)\n",
      "INFO:tensorflow:global step 74400: loss = 0.0002837181091308594 (2317.16 steps/sec)\n",
      "INFO:tensorflow:global step 74500: loss = 6.556510925292969e-05 (2318.03 steps/sec)\n",
      "INFO:tensorflow:global step 74600: loss = 0.0001621246337890625 (2318.81 steps/sec)\n",
      "INFO:tensorflow:global step 74700: loss = 2.384185791015625e-06 (2319.53 steps/sec)\n",
      "INFO:tensorflow:global step 74800: loss = 7.748603820800781e-07 (2320.44 steps/sec)\n",
      "INFO:tensorflow:global step 74900: loss = 0.0057830810546875 (2321.14 steps/sec)\n",
      "INFO:tensorflow:global step 75000: loss = 0.0001195073127746582 (2321.84 steps/sec)\n",
      "INFO:tensorflow:global step 75100: loss = 0.0005288124084472656 (2322.4 steps/sec)\n",
      "INFO:tensorflow:global step 75200: loss = 3.993511199951172e-06 (2322.98 steps/sec)\n",
      "INFO:tensorflow:global step 75300: loss = 7.748603820800781e-07 (2323.66 steps/sec)\n",
      "INFO:tensorflow:global step 75400: loss = 0.0 (2324.32 steps/sec)\n",
      "INFO:tensorflow:global step 75500: loss = 1.9073486328125e-05 (2325.08 steps/sec)\n",
      "INFO:tensorflow:global step 75600: loss = 0.00010883808135986328 (2325.55 steps/sec)\n",
      "INFO:tensorflow:global step 75700: loss = 0.0011281967163085938 (2326.23 steps/sec)\n",
      "INFO:tensorflow:global step 75800: loss = 0.0022525787353515625 (2326.8 steps/sec)\n",
      "INFO:tensorflow:global step 75900: loss = 0.0011472702026367188 (2327.48 steps/sec)\n",
      "INFO:tensorflow:global step 76000: loss = 5.960464477539063e-08 (2328.12 steps/sec)\n",
      "INFO:tensorflow:global step 76100: loss = 5.960464477539063e-08 (2328.78 steps/sec)\n",
      "INFO:tensorflow:global step 76200: loss = 6.556510925292969e-07 (2329.46 steps/sec)\n",
      "INFO:tensorflow:global step 76300: loss = 0.00019884109497070312 (2330.2 steps/sec)\n",
      "INFO:tensorflow:global step 76400: loss = 1.0251998901367188e-05 (2330.9 steps/sec)\n",
      "INFO:tensorflow:global step 76500: loss = 1.9550323486328125e-05 (2331.53 steps/sec)\n",
      "INFO:tensorflow:global step 76600: loss = 0.0 (2332.12 steps/sec)\n",
      "INFO:tensorflow:global step 76700: loss = 0.0005383491516113281 (2332.8 steps/sec)\n",
      "INFO:tensorflow:global step 76800: loss = 5.960464477539062e-07 (2333.47 steps/sec)\n",
      "INFO:tensorflow:global step 76900: loss = 1.3113021850585938e-06 (2334.19 steps/sec)\n",
      "INFO:tensorflow:global step 77000: loss = 0.040618896484375 (2334.77 steps/sec)\n",
      "INFO:tensorflow:global step 77100: loss = 5.364418029785156e-07 (2335.36 steps/sec)\n",
      "INFO:tensorflow:global step 77200: loss = 3.0934810638427734e-05 (2335.91 steps/sec)\n",
      "INFO:tensorflow:global step 77300: loss = 1.239776611328125e-05 (2336.54 steps/sec)\n",
      "INFO:tensorflow:global step 77400: loss = 0.00015437602996826172 (2337.08 steps/sec)\n",
      "INFO:tensorflow:global step 77500: loss = 0.00452423095703125 (2337.75 steps/sec)\n",
      "INFO:tensorflow:global step 77600: loss = 1.6689300537109375e-06 (2338.28 steps/sec)\n",
      "INFO:tensorflow:global step 77700: loss = 0.00016617774963378906 (2338.96 steps/sec)\n",
      "INFO:tensorflow:global step 77800: loss = 0.00011360645294189453 (2339.54 steps/sec)\n",
      "INFO:tensorflow:global step 77900: loss = 0.0 (2340.27 steps/sec)\n",
      "INFO:tensorflow:global step 78000: loss = 0.005435943603515625 (2340.89 steps/sec)\n",
      "INFO:tensorflow:global step 78100: loss = 5.829334259033203e-05 (2341.57 steps/sec)\n",
      "INFO:tensorflow:global step 78200: loss = 5.185604095458984e-06 (2342.22 steps/sec)\n",
      "INFO:tensorflow:global step 78300: loss = 0.0 (2342.95 steps/sec)\n",
      "INFO:tensorflow:global step 78400: loss = 1.8477439880371094e-06 (2343.62 steps/sec)\n",
      "INFO:tensorflow:global step 78500: loss = 0.00016570091247558594 (2344.36 steps/sec)\n",
      "INFO:tensorflow:global step 78600: loss = 4.76837158203125e-07 (2344.9 steps/sec)\n",
      "INFO:tensorflow:global step 78700: loss = 2.3543834686279297e-05 (2345.57 steps/sec)\n",
      "INFO:tensorflow:global step 78800: loss = 6.210803985595703e-05 (2346.16 steps/sec)\n",
      "INFO:tensorflow:global step 78900: loss = 0.0006256103515625 (2346.84 steps/sec)\n",
      "INFO:tensorflow:global step 79000: loss = 4.5299530029296875e-06 (2347.44 steps/sec)\n",
      "INFO:tensorflow:global step 79100: loss = 6.616115570068359e-06 (2348.09 steps/sec)\n",
      "INFO:tensorflow:global step 79200: loss = 0.01025390625 (2348.71 steps/sec)\n",
      "INFO:tensorflow:global step 79300: loss = 2.384185791015625e-06 (2349.37 steps/sec)\n",
      "INFO:tensorflow:global step 79400: loss = 1.2874603271484375e-05 (2350.08 steps/sec)\n",
      "INFO:tensorflow:global step 79500: loss = 0.0002027750015258789 (2350.75 steps/sec)\n",
      "INFO:tensorflow:global step 79600: loss = 1.3649463653564453e-05 (2351.38 steps/sec)\n",
      "INFO:tensorflow:global step 79700: loss = 1.4901161193847656e-06 (2352.03 steps/sec)\n",
      "INFO:tensorflow:global step 79800: loss = 0.0 (2352.73 steps/sec)\n",
      "INFO:tensorflow:global step 79900: loss = 2.294778823852539e-05 (2231.53 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 80000...\n",
      "INFO:tensorflow:Saving checkpoints for 80000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 80000...\n",
      "INFO:tensorflow:global step 80000: loss = 7.748603820800781e-07 (2226.87 steps/sec)\n",
      "INFO:tensorflow:global step 80100: loss = 1.1324882507324219e-05 (2227.41 steps/sec)\n",
      "INFO:tensorflow:global step 80200: loss = 8.535385131835938e-05 (2228.11 steps/sec)\n",
      "INFO:tensorflow:global step 80300: loss = 0.0011625289916992188 (2228.71 steps/sec)\n",
      "INFO:tensorflow:global step 80400: loss = 1.1920928955078125e-06 (2229.4 steps/sec)\n",
      "INFO:tensorflow:global step 80500: loss = 0.0 (2230.05 steps/sec)\n",
      "INFO:tensorflow:global step 80600: loss = 0.006542205810546875 (2230.73 steps/sec)\n",
      "INFO:tensorflow:global step 80700: loss = 4.5299530029296875e-06 (2231.49 steps/sec)\n",
      "INFO:tensorflow:global step 80800: loss = 5.704164505004883e-05 (2232.24 steps/sec)\n",
      "INFO:tensorflow:global step 80900: loss = 1.3113021850585938e-05 (2232.95 steps/sec)\n",
      "INFO:tensorflow:global step 81000: loss = 5.781650543212891e-06 (2233.66 steps/sec)\n",
      "INFO:tensorflow:global step 81100: loss = 5.7220458984375e-06 (2234.21 steps/sec)\n",
      "INFO:tensorflow:global step 81200: loss = 4.172325134277344e-07 (2234.92 steps/sec)\n",
      "INFO:tensorflow:global step 81300: loss = 0.00025153160095214844 (2235.51 steps/sec)\n",
      "INFO:tensorflow:global step 81400: loss = 1.1324882507324219e-06 (2236.11 steps/sec)\n",
      "INFO:tensorflow:global step 81500: loss = 0.021759033203125 (2236.64 steps/sec)\n",
      "INFO:tensorflow:global step 81600: loss = 0.00606536865234375 (2237.28 steps/sec)\n",
      "INFO:tensorflow:global step 81700: loss = 0.0 (2237.82 steps/sec)\n",
      "INFO:tensorflow:global step 81800: loss = 2.5272369384765625e-05 (2238.5 steps/sec)\n",
      "INFO:tensorflow:global step 81900: loss = 2.384185791015625e-07 (2239.13 steps/sec)\n",
      "INFO:tensorflow:global step 82000: loss = 5.960464477539062e-07 (2239.87 steps/sec)\n",
      "INFO:tensorflow:global step 82100: loss = 3.5762786865234375e-07 (2240.4 steps/sec)\n",
      "INFO:tensorflow:global step 82200: loss = 7.3909759521484375e-06 (2241.15 steps/sec)\n",
      "INFO:tensorflow:global step 82300: loss = 0.002685546875 (2241.84 steps/sec)\n",
      "INFO:tensorflow:global step 82400: loss = 0.00010001659393310547 (2242.58 steps/sec)\n",
      "INFO:tensorflow:global step 82500: loss = 1.627206802368164e-05 (2243.27 steps/sec)\n",
      "INFO:tensorflow:global step 82600: loss = 6.556510925292969e-07 (2243.96 steps/sec)\n",
      "INFO:tensorflow:global step 82700: loss = 0.0 (2244.62 steps/sec)\n",
      "INFO:tensorflow:global step 82800: loss = 0.00031757354736328125 (2245.37 steps/sec)\n",
      "INFO:tensorflow:global step 82900: loss = 2.664327621459961e-05 (2246.04 steps/sec)\n",
      "INFO:tensorflow:global step 83000: loss = 8.940696716308594e-07 (2246.71 steps/sec)\n",
      "INFO:tensorflow:global step 83100: loss = 0.0 (2247.24 steps/sec)\n",
      "INFO:tensorflow:global step 83200: loss = 4.0471553802490234e-05 (2247.87 steps/sec)\n",
      "INFO:tensorflow:global step 83300: loss = 1.5497207641601562e-06 (2248.41 steps/sec)\n",
      "INFO:tensorflow:global step 83400: loss = 2.980232238769531e-07 (2249.01 steps/sec)\n",
      "INFO:tensorflow:global step 83500: loss = 2.6285648345947266e-05 (2249.71 steps/sec)\n",
      "INFO:tensorflow:global step 83600: loss = 0.0 (2250.32 steps/sec)\n",
      "INFO:tensorflow:global step 83700: loss = 0.0015010833740234375 (2251.03 steps/sec)\n",
      "INFO:tensorflow:global step 83800: loss = 0.00017595291137695312 (2251.68 steps/sec)\n",
      "INFO:tensorflow:global step 83900: loss = 0.00014519691467285156 (2252.4 steps/sec)\n",
      "INFO:tensorflow:global step 84000: loss = 2.2649765014648438e-06 (2253.05 steps/sec)\n",
      "INFO:tensorflow:global step 84100: loss = 1.2218952178955078e-05 (2253.72 steps/sec)\n",
      "INFO:tensorflow:global step 84200: loss = 0.00015163421630859375 (2254.39 steps/sec)\n",
      "INFO:tensorflow:global step 84300: loss = 1.1324882507324219e-06 (2255.15 steps/sec)\n",
      "INFO:tensorflow:global step 84400: loss = 1.0728836059570312e-06 (2255.94 steps/sec)\n",
      "INFO:tensorflow:global step 84500: loss = 0.0 (2256.64 steps/sec)\n",
      "INFO:tensorflow:global step 84600: loss = 9.959936141967773e-05 (2257.23 steps/sec)\n",
      "INFO:tensorflow:global step 84700: loss = 0.0 (2258.12 steps/sec)\n",
      "INFO:tensorflow:global step 84800: loss = 0.030731201171875 (2258.97 steps/sec)\n",
      "INFO:tensorflow:global step 84900: loss = 0.00021767616271972656 (2259.93 steps/sec)\n",
      "INFO:tensorflow:global step 85000: loss = 4.947185516357422e-06 (2260.81 steps/sec)\n",
      "INFO:tensorflow:global step 85100: loss = 5.53131103515625e-05 (2261.56 steps/sec)\n",
      "INFO:tensorflow:global step 85200: loss = 0.0 (2262.33 steps/sec)\n",
      "INFO:tensorflow:global step 85300: loss = 1.1920928955078125e-07 (2263.12 steps/sec)\n",
      "INFO:tensorflow:global step 85400: loss = 0.021636962890625 (2263.88 steps/sec)\n",
      "INFO:tensorflow:global step 85500: loss = 0.0 (2264.7 steps/sec)\n",
      "INFO:tensorflow:global step 85600: loss = 2.980232238769531e-07 (2265.37 steps/sec)\n",
      "INFO:tensorflow:global step 85700: loss = 5.888938903808594e-05 (2266.17 steps/sec)\n",
      "INFO:tensorflow:global step 85800: loss = 0.0 (2266.99 steps/sec)\n",
      "INFO:tensorflow:global step 85900: loss = 7.927417755126953e-06 (2267.79 steps/sec)\n",
      "INFO:tensorflow:global step 86000: loss = 5.364418029785156e-07 (2268.53 steps/sec)\n",
      "INFO:tensorflow:global step 86100: loss = 0.05877685546875 (2269.28 steps/sec)\n",
      "INFO:tensorflow:global step 86200: loss = 5.960464477539063e-08 (2270.07 steps/sec)\n",
      "INFO:tensorflow:global step 86300: loss = 5.2928924560546875e-05 (2270.88 steps/sec)\n",
      "INFO:tensorflow:global step 86400: loss = 6.502866744995117e-05 (2271.73 steps/sec)\n",
      "INFO:tensorflow:global step 86500: loss = 0.0027618408203125 (2272.5 steps/sec)\n",
      "INFO:tensorflow:global step 86600: loss = 5.960464477539063e-08 (2273.16 steps/sec)\n",
      "INFO:tensorflow:global step 86700: loss = 0.00333404541015625 (2273.97 steps/sec)\n",
      "INFO:tensorflow:global step 86800: loss = 5.960464477539063e-08 (2274.68 steps/sec)\n",
      "INFO:tensorflow:global step 86900: loss = 0.0 (2275.5 steps/sec)\n",
      "INFO:tensorflow:global step 87000: loss = 0.00652313232421875 (2276.18 steps/sec)\n",
      "INFO:tensorflow:global step 87100: loss = 0.01256561279296875 (2276.91 steps/sec)\n",
      "INFO:tensorflow:global step 87200: loss = 2.682209014892578e-06 (2277.47 steps/sec)\n",
      "INFO:tensorflow:global step 87300: loss = 0.0 (2278.13 steps/sec)\n",
      "INFO:tensorflow:global step 87400: loss = 0.0004544258117675781 (2278.75 steps/sec)\n",
      "INFO:tensorflow:global step 87500: loss = 9.5367431640625e-07 (2279.41 steps/sec)\n",
      "INFO:tensorflow:global step 87600: loss = 1.1324882507324219e-06 (2280.15 steps/sec)\n",
      "INFO:tensorflow:global step 87700: loss = 3.5703182220458984e-05 (2280.82 steps/sec)\n",
      "INFO:tensorflow:global step 87800: loss = 2.3245811462402344e-06 (2281.57 steps/sec)\n",
      "INFO:tensorflow:global step 87900: loss = 0.014556884765625 (2282.28 steps/sec)\n",
      "INFO:tensorflow:global step 88000: loss = 0.0 (2283.06 steps/sec)\n",
      "INFO:tensorflow:global step 88100: loss = 0.0010118484497070312 (2283.68 steps/sec)\n",
      "INFO:tensorflow:global step 88200: loss = 0.00868988037109375 (2284.43 steps/sec)\n",
      "INFO:tensorflow:global step 88300: loss = 0.0 (2285.27 steps/sec)\n",
      "INFO:tensorflow:global step 88400: loss = 7.748603820800781e-07 (2286.15 steps/sec)\n",
      "INFO:tensorflow:global step 88500: loss = 2.205371856689453e-06 (2286.87 steps/sec)\n",
      "INFO:tensorflow:global step 88600: loss = 5.960464477539063e-08 (2287.6 steps/sec)\n",
      "INFO:tensorflow:global step 88700: loss = 8.106231689453125e-06 (2288.37 steps/sec)\n",
      "INFO:tensorflow:global step 88800: loss = 0.0 (2289.13 steps/sec)\n",
      "INFO:tensorflow:global step 88900: loss = 0.0131378173828125 (2289.8 steps/sec)\n",
      "INFO:tensorflow:global step 89000: loss = 5.960464477539063e-08 (2290.47 steps/sec)\n",
      "INFO:tensorflow:global step 89100: loss = 3.802776336669922e-05 (2291.04 steps/sec)\n",
      "INFO:tensorflow:global step 89200: loss = 3.933906555175781e-06 (2291.71 steps/sec)\n",
      "INFO:tensorflow:global step 89300: loss = 1.4066696166992188e-05 (2292.27 steps/sec)\n",
      "INFO:tensorflow:global step 89400: loss = 0.0 (2292.92 steps/sec)\n",
      "INFO:tensorflow:global step 89500: loss = 3.5762786865234375e-07 (2293.53 steps/sec)\n",
      "INFO:tensorflow:global step 89600: loss = 6.794929504394531e-05 (2294.28 steps/sec)\n",
      "INFO:tensorflow:global step 89700: loss = 3.713369369506836e-05 (2295.0 steps/sec)\n",
      "INFO:tensorflow:global step 89800: loss = 6.80088996887207e-05 (2295.78 steps/sec)\n",
      "INFO:tensorflow:global step 89900: loss = 0.0 (2223.88 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 90000...\n",
      "INFO:tensorflow:Saving checkpoints for 90000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 90000...\n",
      "INFO:tensorflow:global step 90000: loss = 0.00014495849609375 (2220.34 steps/sec)\n",
      "INFO:tensorflow:global step 90100: loss = 0.0 (2220.94 steps/sec)\n",
      "INFO:tensorflow:global step 90200: loss = 2.086162567138672e-06 (2221.84 steps/sec)\n",
      "INFO:tensorflow:global step 90300: loss = 0.007671356201171875 (2222.68 steps/sec)\n",
      "INFO:tensorflow:global step 90400: loss = 0.0 (2223.58 steps/sec)\n",
      "INFO:tensorflow:global step 90500: loss = 2.562999725341797e-06 (2224.42 steps/sec)\n",
      "INFO:tensorflow:global step 90600: loss = 3.159046173095703e-06 (2225.26 steps/sec)\n",
      "INFO:tensorflow:global step 90700: loss = 5.960464477539063e-08 (2226.09 steps/sec)\n",
      "INFO:tensorflow:global step 90800: loss = 0.04345703125 (2227.0 steps/sec)\n",
      "INFO:tensorflow:global step 90900: loss = 7.152557373046875e-07 (2227.84 steps/sec)\n",
      "INFO:tensorflow:global step 91000: loss = 0.0001308917999267578 (2228.75 steps/sec)\n",
      "INFO:tensorflow:global step 91100: loss = 8.082389831542969e-05 (2229.49 steps/sec)\n",
      "INFO:tensorflow:global step 91200: loss = 5.960464477539062e-07 (2230.4 steps/sec)\n",
      "INFO:tensorflow:global step 91300: loss = 2.0265579223632812e-06 (2231.24 steps/sec)\n",
      "INFO:tensorflow:global step 91400: loss = 0.00011307001113891602 (2232.15 steps/sec)\n",
      "INFO:tensorflow:global step 91500: loss = 0.0025348663330078125 (2232.98 steps/sec)\n",
      "INFO:tensorflow:global step 91600: loss = 1.430511474609375e-06 (2233.85 steps/sec)\n",
      "INFO:tensorflow:global step 91700: loss = 0.025787353515625 (2234.68 steps/sec)\n",
      "INFO:tensorflow:global step 91800: loss = 0.006511688232421875 (2235.54 steps/sec)\n",
      "INFO:tensorflow:global step 91900: loss = 9.59634780883789e-06 (2236.44 steps/sec)\n",
      "INFO:tensorflow:global step 92000: loss = 2.9206275939941406e-06 (2237.27 steps/sec)\n",
      "INFO:tensorflow:global step 92100: loss = 1.1920928955078125e-07 (2238.11 steps/sec)\n",
      "INFO:tensorflow:global step 92200: loss = 0.0 (2238.96 steps/sec)\n",
      "INFO:tensorflow:global step 92300: loss = 0.004016876220703125 (2239.83 steps/sec)\n",
      "INFO:tensorflow:global step 92400: loss = 0.0017347335815429688 (2240.66 steps/sec)\n",
      "INFO:tensorflow:global step 92500: loss = 5.364418029785156e-06 (2241.54 steps/sec)\n",
      "INFO:tensorflow:global step 92600: loss = 2.3186206817626953e-05 (2242.31 steps/sec)\n",
      "INFO:tensorflow:global step 92700: loss = 0.00098419189453125 (2243.19 steps/sec)\n",
      "INFO:tensorflow:global step 92800: loss = 5.960464477539063e-08 (2244.02 steps/sec)\n",
      "INFO:tensorflow:global step 92900: loss = 3.170967102050781e-05 (2244.88 steps/sec)\n",
      "INFO:tensorflow:global step 93000: loss = 6.318092346191406e-05 (2245.7 steps/sec)\n",
      "INFO:tensorflow:global step 93100: loss = 0.0 (2246.55 steps/sec)\n",
      "INFO:tensorflow:global step 93200: loss = 3.0994415283203125e-06 (2247.38 steps/sec)\n",
      "INFO:tensorflow:global step 93300: loss = 2.0265579223632812e-06 (2248.27 steps/sec)\n",
      "INFO:tensorflow:global step 93400: loss = 2.4437904357910156e-06 (2249.09 steps/sec)\n",
      "INFO:tensorflow:global step 93500: loss = 0.0 (2249.98 steps/sec)\n",
      "INFO:tensorflow:global step 93600: loss = 0.0 (2250.71 steps/sec)\n",
      "INFO:tensorflow:global step 93700: loss = 0.0 (2251.49 steps/sec)\n",
      "INFO:tensorflow:global step 93800: loss = 1.1920928955078125e-06 (2252.19 steps/sec)\n",
      "INFO:tensorflow:global step 93900: loss = 0.0675048828125 (2252.94 steps/sec)\n",
      "INFO:tensorflow:global step 94000: loss = 2.086162567138672e-06 (2253.66 steps/sec)\n",
      "INFO:tensorflow:global step 94100: loss = 0.0 (2254.38 steps/sec)\n",
      "INFO:tensorflow:global step 94200: loss = 7.450580596923828e-06 (2255.12 steps/sec)\n",
      "INFO:tensorflow:global step 94300: loss = 1.8775463104248047e-05 (2255.84 steps/sec)\n",
      "INFO:tensorflow:global step 94400: loss = 1.1324882507324219e-06 (2256.57 steps/sec)\n",
      "INFO:tensorflow:global step 94500: loss = 5.960464477539063e-08 (2257.38 steps/sec)\n",
      "INFO:tensorflow:global step 94600: loss = 5.960464477539063e-08 (2258.12 steps/sec)\n",
      "INFO:tensorflow:global step 94700: loss = 1.7523765563964844e-05 (2258.97 steps/sec)\n",
      "INFO:tensorflow:global step 94800: loss = 1.615285873413086e-05 (2259.77 steps/sec)\n",
      "INFO:tensorflow:global step 94900: loss = 6.67572021484375e-06 (2260.62 steps/sec)\n",
      "INFO:tensorflow:global step 95000: loss = 0.0214385986328125 (2261.42 steps/sec)\n",
      "INFO:tensorflow:global step 95100: loss = 1.329183578491211e-05 (2262.24 steps/sec)\n",
      "INFO:tensorflow:global step 95200: loss = 5.960464477539063e-08 (2263.05 steps/sec)\n",
      "INFO:tensorflow:global step 95300: loss = 2.384185791015625e-07 (2263.9 steps/sec)\n",
      "INFO:tensorflow:global step 95400: loss = 2.4437904357910156e-06 (2264.7 steps/sec)\n",
      "INFO:tensorflow:global step 95500: loss = 1.6033649444580078e-05 (2265.53 steps/sec)\n",
      "INFO:tensorflow:global step 95600: loss = 5.960464477539062e-07 (2266.24 steps/sec)\n",
      "INFO:tensorflow:global step 95700: loss = 1.1920928955078125e-07 (2267.08 steps/sec)\n",
      "INFO:tensorflow:global step 95800: loss = 0.0 (2267.87 steps/sec)\n",
      "INFO:tensorflow:global step 95900: loss = 5.429983139038086e-05 (2268.61 steps/sec)\n",
      "INFO:tensorflow:global step 96000: loss = 5.960464477539063e-08 (2269.43 steps/sec)\n",
      "INFO:tensorflow:global step 96100: loss = 2.467632293701172e-05 (2270.12 steps/sec)\n",
      "INFO:tensorflow:global step 96200: loss = 0.0017862319946289062 (2270.93 steps/sec)\n",
      "INFO:tensorflow:global step 96300: loss = 5.960464477539063e-08 (2271.68 steps/sec)\n",
      "INFO:tensorflow:global step 96400: loss = 0.0005717277526855469 (2272.5 steps/sec)\n",
      "INFO:tensorflow:global step 96500: loss = 0.002841949462890625 (2273.27 steps/sec)\n",
      "INFO:tensorflow:global step 96600: loss = 0.00019943714141845703 (2274.06 steps/sec)\n",
      "INFO:tensorflow:global step 96700: loss = 1.1920928955078125e-07 (2274.85 steps/sec)\n",
      "INFO:tensorflow:global step 96800: loss = 2.980232238769531e-07 (2275.69 steps/sec)\n",
      "INFO:tensorflow:global step 96900: loss = 1.7881393432617188e-07 (2276.45 steps/sec)\n",
      "INFO:tensorflow:global step 97000: loss = 2.980232238769531e-07 (2277.28 steps/sec)\n",
      "INFO:tensorflow:global step 97100: loss = 2.5033950805664062e-06 (2278.04 steps/sec)\n",
      "INFO:tensorflow:global step 97200: loss = 1.2516975402832031e-06 (2278.87 steps/sec)\n",
      "INFO:tensorflow:global step 97300: loss = 0.0 (2279.64 steps/sec)\n",
      "INFO:tensorflow:global step 97400: loss = 0.0131988525390625 (2280.47 steps/sec)\n",
      "INFO:tensorflow:global step 97500: loss = 6.556510925292969e-07 (2281.26 steps/sec)\n",
      "INFO:tensorflow:global step 97600: loss = 4.470348358154297e-06 (2282.07 steps/sec)\n",
      "INFO:tensorflow:global step 97700: loss = 4.673004150390625e-05 (2282.85 steps/sec)\n",
      "INFO:tensorflow:global step 97800: loss = 0.0 (2283.67 steps/sec)\n",
      "INFO:tensorflow:global step 97900: loss = 0.03375244140625 (2284.44 steps/sec)\n",
      "INFO:tensorflow:global step 98000: loss = 0.0 (2285.25 steps/sec)\n",
      "INFO:tensorflow:global step 98100: loss = 0.010986328125 (2285.99 steps/sec)\n",
      "INFO:tensorflow:global step 98200: loss = 3.516674041748047e-06 (2286.79 steps/sec)\n",
      "INFO:tensorflow:global step 98300: loss = 2.384185791015625e-07 (2287.57 steps/sec)\n",
      "INFO:tensorflow:global step 98400: loss = 0.0036182403564453125 (2288.4 steps/sec)\n",
      "INFO:tensorflow:global step 98500: loss = 0.0 (2289.12 steps/sec)\n",
      "INFO:tensorflow:global step 98600: loss = 0.00012302398681640625 (2289.92 steps/sec)\n",
      "INFO:tensorflow:global step 98700: loss = 6.556510925292969e-07 (2290.69 steps/sec)\n",
      "INFO:tensorflow:global step 98800: loss = 0.00029730796813964844 (2291.5 steps/sec)\n",
      "INFO:tensorflow:global step 98900: loss = 1.1920928955078125e-07 (2292.26 steps/sec)\n",
      "INFO:tensorflow:global step 99000: loss = 0.0 (2293.08 steps/sec)\n",
      "INFO:tensorflow:global step 99100: loss = 0.0 (2293.71 steps/sec)\n",
      "INFO:tensorflow:global step 99200: loss = 0.0 (2294.52 steps/sec)\n",
      "INFO:tensorflow:global step 99300: loss = 2.1457672119140625e-06 (2295.23 steps/sec)\n",
      "INFO:tensorflow:global step 99400: loss = 2.4557113647460938e-05 (2296.04 steps/sec)\n",
      "INFO:tensorflow:global step 99500: loss = 0.00042819976806640625 (2296.76 steps/sec)\n",
      "INFO:tensorflow:global step 99600: loss = 2.2292137145996094e-05 (2297.49 steps/sec)\n",
      "INFO:tensorflow:global step 99700: loss = 0.0 (2298.2 steps/sec)\n",
      "INFO:tensorflow:global step 99800: loss = 5.960464477539063e-08 (2298.99 steps/sec)\n",
      "INFO:tensorflow:global step 99900: loss = 0.0 (2299.68 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 100000...\n",
      "INFO:tensorflow:Saving checkpoints for 100000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 100000...\n",
      "INFO:tensorflow:Training finished with 25600000 samples in 43.47 seconds, 588906.38 samples/second.\n",
      "INFO:tensorflow:Loss for final step: 0.003548.\n",
      "=============== Starting Cerebras Compilation ===============\n",
      "=============== Cerebras Compilation Completed ===============\n",
      "salloc: Relinquishing job allocation 320735\n"
     ]
    }
   ],
   "source": [
    "# set_cs_ip_addr_value()\n",
    "!salloc ${SLURM_GRES_ARGUMENT} ${SLURM_ARGUMENTS} --nodelist=sdf-1 srun /usr/bin/singularity exec --bind ${BIND_LOCATIONS} --pwd ${YOUR_ENTRY_SCRIPT_LOCATION} ${CEREBRAS_CONTAINER} python run.py --mode train --model_dir tutorial_model_dir --cs_ip 10.8.88.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41bb42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example of the expected output (final lines):\n",
    "\n",
    "    SLURM environment variables have been set successfully.\n",
    "    srun: job 1234 queued and waiting for resources\n",
    "    srun: job 1234 has been allocated resources\n",
    "    INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': ['sdf:29231'], 'worker': ['sdf:29233', 'sdf:29235', 'sdf:29237', 'sdf:29239', 'sdf:29241', 'sdf:29243']}, 'task': {'type': 'chief', 'index': 0}}\n",
    "    INFO:root:Running train on CS-2\n",
    "\n",
    "    [--- OUTPUT SNIPPED FOR KEEPING THIS EXAMPLE SHORT ---]\n",
    "\n",
    "    INFO:tensorflow:global step 99700: loss = 4.76837158203125e-07 (2277.17 steps/sec)\n",
    "    INFO:tensorflow:global step 99800: loss = 0.0 (2277.98 steps/sec)\n",
    "    INFO:tensorflow:global step 99900: loss = 5.304813385009766e-06 (2278.76 steps/sec)\n",
    "    INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 100000...\n",
    "    INFO:tensorflow:Saving checkpoints for 100000 into tutorial_model_dir/model.ckpt.\n",
    "    INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 100000...\n",
    "    INFO:tensorflow:Training finished with 25600000 samples in 43.869 seconds, 583554.35 samples/second.\n",
    "    INFO:tensorflow:Loss for final step: 0.06384.\n",
    "    =============== Starting Cerebras Compilation ===============\n",
    "    =============== Cerebras Compilation Completed ===============\n",
    "\n",
    "---\n",
    "\n",
    "This third command also executed a job using `srun` and `singularity`, but now using the actual training mode that utilizes the CS system.\n",
    "\n",
    "For the Singularity/Apptainer arguments, the following argument changed:\n",
    "* **\\${SLURM_GRES_ARGUMENT}**: the actual value is `--gres=cs:cerebras:1`, and it requests a CS machine as a special resource to be used for the Slurm job training the model. If this flag were not to be used, the CS would not be allocated for the job (unavailable).\n",
    "* **--cs_ip ${CS_IP_ADDR}**: this value is set when a CS machine has been requested (using `--gres=cs:cerebras:1`). It will point to the IP address of the CS machine mapped to the specific compute node running the training. This value is dynamic and changes as required by the system administrators.\n",
    "\n",
    "For the Python run.py file, the following argument changed:\n",
    "* **--mode train**: no additional `_only` arguments were used, just the `--mode train` argument to start the training on the CS system using the compiled executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cfcd2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp.VT9bug2LBf/modelzoo/modelzoo/fc_mnist/tf/tutorial_model_dir/cs_0233d1734bc58d9f1ad221730921ebe2bbcc507232f5d3bb32762a5a2baf5a31/plan.json:\"total_utilization\":1.71281689019136\n",
      "/tmp/tmp.VT9bug2LBf/modelzoo/modelzoo/fc_mnist/tf/tutorial_model_dir/cs_0863d6792f6cb5c1590f3fc48f98fd90f3eb7951cd1a5fa2049cfe56bd6d41df/plan.json:\"total_utilization\":1.71281689019136\n",
      "/tmp/tmp.VT9bug2LBf/modelzoo/modelzoo/fc_mnist/tf/tutorial_model_dir/cs_0233d1734bc58d9f1ad221730921ebe2bbcc507232f5d3bb32762a5a2baf5a31/deltat_estimate.json:\"estimated_deltat\":43264\n",
      "/tmp/tmp.VT9bug2LBf/modelzoo/modelzoo/fc_mnist/tf/tutorial_model_dir/cs_0863d6792f6cb5c1590f3fc48f98fd90f3eb7951cd1a5fa2049cfe56bd6d41df/deltat_estimate.json:\"estimated_deltat\":\n"
     ]
    }
   ],
   "source": [
    "# To get the ratio of utilized components. Theoretical peak throughput. Grep the “total_utilization” percentage from the plan.json file in the destination folder.\n",
    "!grep -o '\"total_utilization\":[0-9\\.]*' ${YOUR_ENTRY_SCRIPT_LOCATION}/tutorial_model_dir/*/plan.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059822d",
   "metadata": {},
   "source": [
    "## Closing notes\n",
    "\n",
    "Congratulations on successfully training an FC-MNIST model on the Neocortex system! Throughout this tutorial, you have gained hands-on experience in performing all of the steps required for running a reference example from scratch on a Cerebras CS-2 machine.\n",
    "\n",
    "This example is based on the [Cerebras Documentation](https://docs.cerebras.net/en/1.6.0/). \n",
    "\n",
    "Other links of interest are:\n",
    "\n",
    "* [Neocortex System](https://www.cmu.edu/psc/aibd/neocortex/)\n",
    "* [Cerebras ML Workflow](https://docs.cerebras.net/en/1.6.0/cerebras-basics/cs-ml-workflow.html)\n",
    "* [Neocortex Documentation](https://portal.neocortex.psc.edu/docs/)\n",
    "\n",
    "This material is based upon work supported by the [National Science Foundation under Grant Number 2005597](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2005597).\n",
    "\n",
    "## Next steps (optional):\n",
    "\n",
    "\n",
    "Neocortex provides a unique opportunity to access the remarkable integrated technologies of the Cerebras CS-2 and the HPE Superdome Flex Servers available in PSC's Neocortex system. \n",
    "\n",
    "We invite you to run your research on Neocortex. You could first, identify the track that your project belongs to, then answer some general questions, and then finally, apply for accessing the system over a full-fledged research grant.\n",
    "\n",
    "You can take a look at the [previous Neocortex Call for Proposals page](https://www.cmu.edu/psc/aibd/neocortex/2023-03-cfp-spring-2023.html) for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
