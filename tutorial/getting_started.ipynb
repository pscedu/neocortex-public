{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1446d37",
   "metadata": {},
   "source": [
    "# Neocortex: Hands-on FC-MNIST Example\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Welcome notes\n",
    "\n",
    "Welcome to this hands-on example of training a Fully Connected (FC) model for the MNIST dataset! In this exercise, we will explore the fascinating world of deep learning by building a neural network capable of recognizing handwritten digits. MNIST is a widely used dataset in the field of computer vision and serves as an excellent starting point for beginners.\n",
    "\n",
    "The objective of this exercise is to guide you through the process of constructing and training a simple FC model using Python and the TensorFlow deep learning framework running on top of the Cerebras Software stack on the Neocortex system. We will break down the example step by step, ensuring that you gain a clear understanding of the underlying concepts and techniques.\n",
    "\n",
    "By the end of this hands-on example, you will have a trained FC model on a Cerebras CS-2 machine that can accurately classify handwritten digits from the MNIST dataset. You will also gain valuable insights into the fundamentals of deep learning, including model architecture, training data preparation, loss functions, and optimization algorithms.\n",
    "\n",
    "Whether you are new to the Neocortex system or looking to reinforce your knowledge, this exercise will provide you with a solid foundation to explore more advanced implementations using your custom model and dataset. So, let's dive in and embark on this exciting journey of training an FC-MNIST model on the Neocortex system!\n",
    "\n",
    "### Neocortex\n",
    "\n",
    "[Neocortex](https://www.cmu.edu/psc/aibd/neocortex/) is a highly innovative resource that targets the acceleration of AI-powered scientific discovery by vastly shortening the time required for deep learning training, featuring two [Cerebras CS-2](https://www.cerebras.net/product-system/) systems and an [HPE Superdome Flex HPC server](https://buy.hpe.com/ca/en/compute/mission-critical-x86-servers/superdome-flex-servers/superdome-flex-server/hpe-superdome-flex-280-server/p/1012865453) (SDF) robustly provisioned to drive the CS-2 systems simultaneously at maximum speed and support the complementary requirements of AI and HPDA workflows.\n",
    "\n",
    "There are four types of applications currently supported on the system, divided into the following individual tracks:\n",
    "\n",
    "* **Track 1**, [Cerebras modelzoo ML models](https://portal.neocortex.psc.edu/docs/supported-applications/track1.html): models already present in version R1.6.0 of the Cerebras modelzoo ML models software.\n",
    "* **Track 2**, [Models similar to the Cerebras modelzoo models](https://portal.neocortex.psc.edu/docs/supported-applications/track2.html): a combination of the building blocks used by modelzoo models and/or the layers supported by Cerebras as listed in their documentation.\n",
    "* **Track 3**, [General purpose SDK](https://portal.neocortex.psc.edu/docs/supported-applications/track3.html): a general purpose SDK that can be used for a variety of things. This track requires you to write low-level code, similar to writing CUDA, for implementing your research.\n",
    "* **Track 4**, [WFA, WSE Field-equation API](https://portal.neocortex.psc.edu/docs/supported-applications/track4.html): for field equations, includes ML inference. This API was recently used for advancing CFP simulations at unprecedented resolution and speed ([more info](https://www.cmu.edu/psc/aibd/neocortex/2023-02-netl-psc-pioneer-first-ever-computational-fluid-dynamics-simulation-on-cerebras-wse.html)).\n",
    "This document is expected to serve as an example of how to train a (Track 1) Cerebras modelzoo ML FC-MNIST model example from scratch. \n",
    "\n",
    "This document is under continuous development. If you have any recommendations for this document, please make sure to share them with the team (see the [Feedback](https://portal.neocortex.psc.edu/docs/providing-feedback.html) page).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38396905",
   "metadata": {},
   "source": [
    "## Setup and Requirements\n",
    "\n",
    "To follow along with this hands-on tutorial on training an FC-MNIST model, you will need the following setup and requirements:\n",
    "\n",
    "1. <u>A PSC account to access the Neocortex system</u>. You should have gotten an email requesting you to create/provide a valid PSC account.\n",
    "2. <u>An SSH terminal client</u>.\n",
    "3. <u>A web browser</u>.\n",
    "4. A development environment with all of the Cerebras software stack libraries. This is already present in the Neocortex system. You don't need to download it separately.\n",
    "5. The Cerebras modelzoo repository using the 1.6.0 release tag (R_1.6.0). This repository will be downloaded as part of the tutorial. You don't need to download it separately.\n",
    "6. MNIST Dataset: The tutorial utilizes the MNIST dataset, which consists of a large collection of handwritten digit images. Fortunately, both TensorFlow and PyTorch provide convenient functions to automatically download and load the MNIST dataset. You don't need to download it separately.\n",
    "\n",
    "With these requirements in place, you are all set to start this tutorial.\n",
    "\n",
    "## Expected Steps\n",
    "\n",
    "The training is composed of different stages. We will be performing the following tasks:\n",
    "\n",
    "1. Define all of the helper variables and commands used across the tutorial steps. This way we can reuse code and focus in the logic behind the steps, i.e. setting the paths to access the Cerebras software stack.\n",
    "2. Procure the Cerebras modelzoo repository. This repository contains the example code we will be using.\n",
    "3. Navigate to the FC-MNIST code location inside the Cerebras modelzoo repository.\n",
    "4. Precompile the code, using Cerebras tools to validate everything looks good code-wise.\n",
    "5. Compile the code. This will generate the executable to use.\n",
    "6. Train the model using the generated executable.\n",
    "\n",
    "Here is a simple flow of the expected steps:\n",
    "\n",
    "```\n",
    "Set helper variables -> Get example code -> Change to FC-MNIST dir -> Validate -> Compile -> Train model\n",
    "```\n",
    "\n",
    "## Step 1: Set helper variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b54d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder path to the Cerebras directory\n",
    "import os\n",
    "\n",
    "account_id = \"ACCOUNT_ID\"  # Project allocation to use. The `projects` command shows your projects. i.e. cis123456p\n",
    "username = os.environ[\"USER\"]\n",
    "\n",
    "# Set Cerebras-related environment variables, such as the base directory containing the development environment\n",
    "cerebras_dir = \"/ocean/neocortex/cerebras\"\n",
    "os.environ[\"CEREBRAS_DIR\"] = cerebras_dir\n",
    "os.environ['CEREBRAS_CONTAINER'] = f\"{cerebras_dir}/cbcore_latest.sif\"\n",
    "\n",
    "# Set your individual code environment variables, such as the directory to be used for running the compilation\n",
    "project_path = f\"/ocean/projects/{account_id}/{username}\"\n",
    "os.environ[\"PROJECT\"] = project_path\n",
    "os.environ[\"YOUR_ENTRY_SCRIPT_LOCATION\"] = f\"{project_path}/modelzoo/modelzoo/fc_mnist/tf\"\n",
    "your_entry_script_location = os.environ[\"YOUR_ENTRY_SCRIPT_LOCATION\"]\n",
    "os.environ['BIND_LOCATIONS'] = f\"/local1/cerebras/data,/local2/cerebras/data,/local3/cerebras/data,/local4/cerebras/data,{project_path}\"\n",
    "\n",
    "# Set Slurm-related environment variables and command arguments to use for running this example\n",
    "os.environ['SLURM_GRES_ARGUMENT'] = \"--gres=cs:cerebras:1\"\n",
    "os.environ['SLURM_ARGUMENTS'] = f\"--ntasks=7 --time=0-00:15 --cpus-per-task=28 --account={account_id}\"\n",
    "\n",
    "# Define a method we will use to get some required arguments for the model training.\n",
    "def set_cs_ip_addr_value():\n",
    "    \"\"\"\n",
    "    Runs a SLURM command to retrieve the CS IP address and compute node ID to use, and sets them as environment\n",
    "    variables in the system.\n",
    "    \"\"\"\n",
    "    # Run a job while requesting a CS machine, get the assigned value for the CS_IP_ADDR environment variable.\n",
    "    cs_ip_addr_output = !salloc ${SLURM_GRES_ARGUMENT} ${SLURM_ARGUMENTS} --ntasks=1 srun /bin/bash -c set -o posix | grep CS_IP_ADDR\n",
    "    cs_ip_addr_output = [item for item in cs_ip_addr_output if item.startswith(\"CS_IP_ADDR\")]\n",
    "    os.environ[\"CS_IP_ADDR\"] = cs_ip_addr_output[0].split(\"=\")[1]\n",
    "    cs_ip_addr = os.environ[\"CS_IP_ADDR\"]\n",
    "    \n",
    "    # Execute sacct to figure out the compute node is (the SDF partition) assigned to driving that specific CS machine\n",
    "    node_id_output = !sacct --allocations --format=NodeList,AllocTRES --state=COMPLETED --parsable2 --starttime=now-1hours --endtime=now | grep \"gres/cs=1\" | tail --lines 1\n",
    "    print(node_id_output)\n",
    "    os.environ[\"NODE_ID\"] = node_id_output[0].split(\"|\")[0]\n",
    "    node_id = os.environ[\"NODE_ID\"]\n",
    "    \n",
    "    print(f\"The CS_IP_ADDR ({cs_ip_addr}) and NODE_ID ({node_id}) environment variables have been set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7a36e",
   "metadata": {},
   "source": [
    "## Step 2: Get the example code\n",
    "### Procure the Cerebras modelzoo examples repository\n",
    "\n",
    "The [Cerebras Model Zoo GitHub repository](https://github.com/Cerebras/modelzoo/tree/R_1.6.0) is public and contains examples of common deep learning models that can be trained on Cerebras hardware.\n",
    "\n",
    "Please clone the repository and then check out the R_1.6.0 tag (the current version running on Neocortex system) using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a59adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/ocean/projects/ACCOUNT_ID/USERNAME/modelzoo'...\n",
      "remote: Enumerating objects: 2158, done.\u001B[K\n",
      "remote: Counting objects: 100% (308/308), done.\u001B[K\n",
      "remote: Compressing objects: 100% (177/177), done.\u001B[K\n",
      "remote: Total 2158 (delta 181), reused 136 (delta 131), pack-reused 1850\u001B[K\n",
      "Receiving objects: 100% (2158/2158), 22.10 MiB | 31.88 MiB/s, done.\n",
      "Resolving deltas: 100% (1211/1211), done.\n",
      "/ocean/projects/ACCOUNT_ID/USERNAME/modelzoo\n",
      "Note: checking out 'tags/R_1.6.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b new_branch_name\n",
      "\n",
      "HEAD is now at 886a438... R_1.6.0\n",
      "\n",
      "OK: The reference modelzoo folder has been cloned into the /ocean/projects/ACCOUNT_ID/USERNAME directory.\n",
      "total 24K\n",
      "4.0K drwxr-xr-x  5 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 .\n",
      "4.0K drwxr-x---+ 3 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 ..\n",
      "4.0K drwxr-xr-x  8 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 .git\n",
      " 512 -rw-r--r--  1 USERNAME ACCOUNT_ID   94 Aug  4 08:15 .gitignore\n",
      " 512 -rw-r--r--  1 USERNAME ACCOUNT_ID  12K Aug  4 08:15 LICENSE\n",
      "4.0K drwxr-xr-x  6 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 modelzoo\n",
      " 512 -rw-r--r--  1 USERNAME ACCOUNT_ID 4.5K Aug  4 08:15 PYTHON-SETUP.md\n",
      " 512 -rw-r--r--  1 USERNAME ACCOUNT_ID 8.2K Aug  4 08:15 README.md\n",
      " 512 -rw-r--r--  1 USERNAME ACCOUNT_ID  27K Aug  4 08:15 RELEASE-NOTES.md\n",
      " 512 -rw-r--r--  1 USERNAME ACCOUNT_ID 1.2K Aug  4 08:15 requirements_pytorch_gpu.txt\n",
      " 512 -rw-r--r--  1 USERNAME ACCOUNT_ID 1.3K Aug  4 08:15 requirements_tensorflow_gpu.txt\n",
      "4.0K drwxr-xr-x  2 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 user_scripts\n",
      "\n",
      "OK: Successfully listed directories into the modelzoo folder /ocean/projects/ACCOUNT_ID/USERNAME/modelzoo\n"
     ]
    }
   ],
   "source": [
    "repository_exists = os.path.isdir(f\"{project_path}/modelzoo\")\n",
    "                                   \n",
    "if repository_exists:\n",
    "    !rm -rf ${PROJECT}/modelzoo\n",
    "\n",
    "!git clone https://github.com/Cerebras/modelzoo.git ${PROJECT}/modelzoo\n",
    "%cd \"$project_path/modelzoo\"\n",
    "!git checkout tags/R_1.6.0\n",
    "\n",
    "!echo -e \"\\nOK: The reference modelzoo folder has been cloned into the ${PROJECT} directory.\"\n",
    "\n",
    "!ls -lash && echo -e \"\\nOK: Successfully listed directories into the modelzoo folder\" ${PWD}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363b026",
   "metadata": {},
   "source": [
    "---\n",
    "Example of the expected output (final lines):\n",
    "\n",
    "    Cloning into '/ocean/projects/ACCOUNT_ID/USERNAME/modelzoo'...\n",
    "    [--- OUTPUT SNIPPED FOR KEEPING THIS EXAMPLE SHORT ---]\n",
    "    HEAD is now at 886a438... R_1.6.0\n",
    "\n",
    "    OK: The reference modelzoo folder has been cloned into the /ocean/projects/ACCOUNT_ID/USERNAME directory.\n",
    "---\n",
    "\n",
    "You should have the freshly checked-out folder now and it should have a modelzoo subdirectory inside as well as some other files required for running the examples. See it pointed out in the following output:\n",
    "\n",
    "    total 92K\n",
    "    4.0K drwxr-xr-x 8 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 .git\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID   94 Jun  13 12:29 .gitignore\n",
    "     12K -rw-r--r-- 1 USERNAME ACCOUNT_ID  12K Jun  13 12:29 LICENSE\n",
    "    4.0K drwxr-xr-x 6 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 modelzoo  # <- This is the one we will be using\n",
    "    8.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 4.5K Jun  13 12:29 PYTHON-SETUP.md\n",
    "     12K -rw-r--r-- 1 USERNAME ACCOUNT_ID 8.2K Jun  13 12:29 README.md\n",
    "     28K -rw-r--r-- 1 USERNAME ACCOUNT_ID  27K Jun  13 12:29 RELEASE-NOTES.md\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.2K Jun  13 12:29 requirements_pytorch_gpu.txt\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.3K Jun  13 12:29 requirements_tensorflow_gpu.txt\n",
    "    4.0K drwxr-xr-x 2 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 user_scripts\n",
    "    \n",
    "You can check if the contents of that directory look the same by running the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee5572",
   "metadata": {},
   "source": [
    "## Step 3: Change into the FC-MIST Example folder\n",
    "\n",
    "We will now change directories to the `modelzoo/fc_mnist/tf` folder for running the FC-MNIST example. We need to do it as that location has the entry script and all of the code for running the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2db49be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/ACCOUNT_ID/USERNAME/modelzoo/modelzoo/fc_mnist/tf\n",
      "total 16K\r\n",
      "4.0K drwxr-xr-x 3 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 .\r\n",
      "4.0K drwxr-xr-x 5 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 ..\r\n",
      "4.0K drwxr-xr-x 2 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 configs\r\n",
      " 512 -rw-r--r-- 1 USERNAME ACCOUNT_ID 2.6K Aug  4 08:15 data.py\r\n",
      "   0 -rw-r--r-- 1 USERNAME ACCOUNT_ID    0 Aug  4 08:15 __init__.py\r\n",
      " 512 -rw-r--r-- 1 USERNAME ACCOUNT_ID 4.3K Aug  4 08:15 model.py\r\n",
      " 512 -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.2K Aug  4 08:15 prepare_data.py\r\n",
      " 512 -rw-r--r-- 1 USERNAME ACCOUNT_ID 6.4K Aug  4 08:15 README.md\r\n",
      " 512 -rw-r--r-- 1 USERNAME ACCOUNT_ID 4.0K Aug  4 08:15 run-appliance.py\r\n",
      " 512 -rw-r--r-- 1 USERNAME ACCOUNT_ID 8.5K Aug  4 08:15 run.py\r\n",
      " 512 -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.5K Aug  4 08:15 utils.py\r\n",
      "\r\n",
      "OK: Successfully listed directories in the FC-MNIST example folder\r\n"
     ]
    }
   ],
   "source": [
    "%cd \"$your_entry_script_location\"\n",
    "!ls -lash && echo -e \"\\nOK: Successfully listed directories in the FC-MNIST example folder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210b971",
   "metadata": {},
   "source": [
    "## Structure of the code\n",
    "\n",
    "The following is the base structure that Cerebras uses for their code (the code template). If you want to run your research on their system, the suggested way to add your model and dataset is to take one of these base examples and start changing specific components.\n",
    "\n",
    "The files for this specific TensorFlow FC-MNIST directory we just switched to should look like this:\n",
    "\n",
    "    /ocean/projects/ACCOUNT_ID/USERNAME/modelzoo/modelzoo/fc_mnist/tf\n",
    "    total 56K\n",
    "    4.0K drwxr-xr-x 2 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 configs\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 2.6K Jun  13 12:29 data.py\n",
    "       0 -rw-r--r-- 1 USERNAME ACCOUNT_ID    0 Jun  13 12:29 __init__.py\n",
    "    8.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 4.3K Jun  13 12:29 model.py\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.2K Jun  13 12:29 prepare_data.py\n",
    "    8.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 6.4K Jun  13 12:29 README.md\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 4.0K Jun  13 12:29 run-appliance.py\n",
    "     12K -rw-r--r-- 1 USERNAME ACCOUNT_ID 8.5K Jun  13 12:29 run.py\n",
    "    4.0K -rw-r--r-- 1 USERNAME ACCOUNT_ID 1.5K Jun  13 12:29 utils.py\n",
    "\n",
    "Let's go over the main files in this location:\n",
    "\n",
    "* **configs/params.yaml:** YAML file containing the model configuration and the training hyperparameter settings.\n",
    "* **data.py:** where the input data pipeline is called. Additional data processor modules may be defined elsewhere (e.g., in the input folder) for data pipeline implementation.\n",
    "* **model.py:** It contains the model function definition. For information about the layers supported, please visit the Cerebras Documentation.\n",
    "* **run.py:** it contains the training/compilation/evaluation script.\n",
    "* **utils.py:** it contains the helper scripts.\n",
    "\n",
    "If you would like to modify an example to integrate your code, model, or dataset, the suggested order of files to modify is: **model.py** > **data.py** > **utils.py** > **configs/params.yaml** > **run.py**\n",
    "\n",
    "Additionally, the following diagram shows the suggested order in which the modifications should be performed for porting the code. The arrows represent the suggested order for the modification process to perform. This diagram should be read from left to right.\n",
    "\n",
    "![diagram](https://portal.neocortex.psc.edu/static/images/code_migration/code_migration_workflow.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d26e71",
   "metadata": {},
   "source": [
    "Since the Cerebras modelzoo examples are all ready to be executed, we will not make any modifications for this example as we are now ready to start running it. \n",
    "\n",
    "The execution will happen over three different steps: validate, compile, and train.\n",
    "\n",
    "1. **validate**: this step runs a fast verification (on CPU), running a light-weight compilation up to performing kernel library matching, helping you determine if you are using any TensorFlow layer or functionality that is unsupported by the Cerebras development stack.\n",
    "<br/>The argument used for running this step is `--mode train --validate_only`.\n",
    "\n",
    "2. **compile**: this steps runs the full compilation (on CPU) through all stages of the Cerebras software stack to generate a CS system executable. When the above compilation is successful, the model is guaranteed to run on the CS system. <br/>The argument used for running this step is `--mode train --compile_only`.\n",
    "\n",
    "3. **train**: this step runs the actual training job on the CS system using the compiled executable.\n",
    "<br/>The argument used for running this step is simply `--mode train` (without additional \"`_only`\" arguments).\n",
    "\n",
    "For more information, please visit the [Cerebras TensorFlow Quickstart Documentation v1.6.0](https://docs.cerebras.net/en/1.6.0/getting-started/cs-tf-quickstart.html) page.\n",
    "\n",
    "## Running the example\n",
    "\n",
    "As the development environment uses custom Cerebras libraries and it would need to be configured beforehand, Cerebras kindly offers a pre-made container with everything preconfigured and ready to execute the Cerebras modelzoo examples.\n",
    "\n",
    "The actual example code is then executed from that [Singularity/Apptainer](https://apptainer.org/) container, and multi-user access to the CS systems is handled by Slurm.\n",
    "\n",
    "The actual validation, compilation and training commands can be found below.\n",
    "\n",
    "### Step 4: Validate de code\n",
    "\n",
    "Run the code validation using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d9f388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salloc: Granted job allocation 283011\n",
      "salloc: Waiting for resource configuration\n",
      "salloc: Nodes sdf-1 are ready for job\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {}\n",
      "INFO:root:Running None on CS-2\n",
      "INFO:absl:Generating dataset mnist (./tfds/mnist/3.0.1)\n",
      "\u001B[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ./tfds/mnist/3.0.1...\u001B[0m\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001B[A\u001B[AINFO:absl:Downloading https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz into tfds/downloads/cvdf-datasets_mnist_t10k-images-idx3-ubytedDnaEPiC58ZczHNOp6ks9L4_JLids_rpvUj38kJNGMc.gz.tmp.9b190f80eccb49fe933cba7532cb28f2...\n",
      "Dl Completed...:   0%|                                  | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001B[A\u001B[AINFO:absl:Downloading https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz into tfds/downloads/cvdf-datasets_mnist_t10k-labels-idx1-ubyte4Mqf5UL1fRrpd5pIeeAh8c8ZzsY2gbIPBuKwiyfSD_I.gz.tmp.3df59d8533ae4aebbc0ac7f8fe7df0ec...\n",
      "Dl Completed...:   0%|                                  | 0/2 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001B[A\u001B[AINFO:absl:Downloading https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz into tfds/downloads/cvdf-datasets_mnist_train-images-idx3-ubyteJAsxAi0QnOBEygBw_XW2X7zp-LBZAIqqYSHN8ru4ZO4.gz.tmp.e96e22c80eae4b2fbfa2f3cee7b0f4de...\n",
      "Dl Completed...:   0%|                                  | 0/3 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001B[A\u001B[AINFO:absl:Downloading https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz into tfds/downloads/cvdf-datasets_mnist_train-labels-idx1-ubytedcDWkl3FO9T-WMEH1f1Xt51eIRmePRIMAk6X147Qw8w.gz.tmp.d25df20e563b4e0cad200e6103d4e52a...\n",
      "Dl Completed...:   0%|                                  | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:   0%|                                  | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|                                       | 0/1 [00:00<?, ? MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:   0%|                                  | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|                                      | 0/10 [00:00<?, ? MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:   0%|                                  | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|                                      | 0/10 [00:00<?, ? MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  25%|██████▌                   | 1/4 [00:00<00:00,  7.88 url/s]\n",
      "Dl Size...:   0%|                                      | 0/10 [00:00<?, ? MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  25%|██████▌                   | 1/4 [00:00<00:00,  7.88 url/s]\n",
      "Dl Size...:   0%|                                      | 0/10 [00:00<?, ? MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001B[A\u001B[A\n",
      "Dl Completed...:  25%|██████▌                   | 1/4 [00:00<00:00,  7.88 url/s]\u001B[A\n",
      "Dl Size...:  10%|███                           | 1/10 [00:00<00:01,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00,  7.88 url/s]\n",
      "Dl Size...:  10%|███                           | 1/10 [00:00<00:01,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00,  7.88 url/s]\n",
      "Dl Size...:  20%|██████                        | 2/10 [00:00<00:01,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  50%|█████████████             | 2/4 [00:00<00:00,  7.88 url/s]\n",
      "Dl Size...:  20%|██████                        | 2/10 [00:00<00:01,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  20%|██████                        | 2/10 [00:00<00:01,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  30%|█████████                     | 3/10 [00:00<00:01,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  40%|████████████                  | 4/10 [00:00<00:01,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  50%|███████████████               | 5/10 [00:00<00:00,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  60%|██████████████████            | 6/10 [00:00<00:00,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...:   0%|                         | 0/1 [00:00<?, ? file/s]\u001B[A\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  60%|██████████████████            | 6/10 [00:00<00:00,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  70%|█████████████████████         | 7/10 [00:00<00:00,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  70%|█████████████████████         | 7/10 [00:00<00:00,  5.38 MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...:  50%|████████▌        | 1/2 [00:00<00:00,  3.90 file/s]\u001B[A\u001B[A\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\n",
      "Dl Size...:  80%|████████████████████████      | 8/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...:  90%|███████████████████████████   | 9/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████▌      | 3/4 [00:00<00:00,  7.88 url/s]\u001B[A\u001B[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00,  8.82 url/s]\u001B[A\u001B[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...:  33%|█████▋           | 1/3 [00:00<00:00,  3.90 file/s]\u001B[A\u001B[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00,  8.82 url/s]\u001B[A\u001B[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00,  8.82 url/s]\u001B[A\u001B[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00,  8.82 url/s]\u001B[A\u001B[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...:  75%|████████████▊    | 3/4 [00:00<00:00,  4.65 file/s]\u001B[A\u001B[A\n",
      "\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00,  8.82 url/s]\u001B[A\u001B[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00,  7.33 MiB/s]\u001B[A\n",
      "\n",
      "Extraction completed...: 100%|█████████████████| 4/4 [00:00<00:00,  5.22 file/s]\u001B[A\u001B[A\n",
      "Dl Size...: 100%|█████████████████████████████| 10/10 [00:00<00:00, 13.06 MiB/s]\n",
      "Dl Completed...: 100%|██████████████████████████| 4/4 [00:00<00:00,  5.22 url/s]\n",
      "Generating splits...:   0%|                          | 0/2 [00:00<?, ? splits/s]\n",
      "Generating train examples...: 0 examples [00:00, ? examples/s]\u001B[A2023-08-04 08:15:38.217115: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2023-08-04 08:15:38.222686: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2700000000 Hz\n",
      "2023-08-04 08:15:38.223042: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8440800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-04 08:15:38.223059: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\n",
      "Generating train examples...: 1 examples [00:00,  9.25 examples/s]\u001B[A\n",
      "Generating train examples...: 188 examples [00:00, 13.19 examples/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...: 377 examples [00:00, 18.78 examples/s]\u001B[A\n",
      "Generating train examples...: 567 examples [00:00, 26.72 examples/s]\u001B[A\n",
      "Generating train examples...: 757 examples [00:00, 37.94 examples/s]\u001B[A\n",
      "Generating train examples...: 947 examples [00:00, 53.74 examples/s]\u001B[A\n",
      "Generating train examples...: 1137 examples [00:00, 75.85 examples/s]\u001B[A\n",
      "Generating train examples...: 1326 examples [00:00, 106.52 examples/s]\u001B[A\n",
      "Generating train examples...: 1516 examples [00:00, 148.59 examples/s]\u001B[A\n",
      "Generating train examples...: 1704 examples [00:01, 205.30 examples/s]\u001B[A\n",
      "Generating train examples...: 1892 examples [00:01, 280.16 examples/s]\u001B[A\n",
      "Generating train examples...: 2080 examples [00:01, 376.13 examples/s]\u001B[A\n",
      "Generating train examples...: 2269 examples [00:01, 494.94 examples/s]\u001B[A\n",
      "Generating train examples...: 2459 examples [00:01, 635.83 examples/s]\u001B[A\n",
      "Generating train examples...: 2649 examples [00:01, 793.92 examples/s]\u001B[A\n",
      "Generating train examples...: 2839 examples [00:01, 961.49 examples/s]\u001B[A\n",
      "Generating train examples...: 3030 examples [00:01, 1129.26 examples/s]\u001B[A\n",
      "Generating train examples...: 3220 examples [00:01, 1285.28 examples/s]\u001B[A\n",
      "Generating train examples...: 3410 examples [00:01, 1419.76 examples/s]\u001B[A\n",
      "Generating train examples...: 3601 examples [00:02, 1536.58 examples/s]\u001B[A\n",
      "Generating train examples...: 3791 examples [00:02, 1627.08 examples/s]\u001B[A\n",
      "Generating train examples...: 3981 examples [00:02, 1699.39 examples/s]\u001B[A\n",
      "Generating train examples...: 4171 examples [00:02, 1753.94 examples/s]\u001B[A\n",
      "Generating train examples...: 4361 examples [00:02, 1793.68 examples/s]\u001B[A\n",
      "Generating train examples...: 4551 examples [00:02, 1822.95 examples/s]\u001B[A\n",
      "Generating train examples...: 4741 examples [00:02, 1843.14 examples/s]\u001B[A\n",
      "Generating train examples...: 4931 examples [00:02, 1856.34 examples/s]\u001B[A\n",
      "Generating train examples...: 5121 examples [00:02, 1867.50 examples/s]\u001B[A\n",
      "Generating train examples...: 5311 examples [00:02, 1876.07 examples/s]\u001B[A\n",
      "Generating train examples...: 5501 examples [00:03, 1879.60 examples/s]\u001B[A\n",
      "Generating train examples...: 5691 examples [00:03, 1883.31 examples/s]\u001B[A\n",
      "Generating train examples...: 5881 examples [00:03, 1887.39 examples/s]\u001B[A\n",
      "Generating train examples...: 6071 examples [00:03, 1888.59 examples/s]\u001B[A\n",
      "Generating train examples...: 6261 examples [00:03, 1881.64 examples/s]\u001B[A\n",
      "Generating train examples...: 6450 examples [00:03, 1879.39 examples/s]\u001B[A\n",
      "Generating train examples...: 6640 examples [00:03, 1883.49 examples/s]\u001B[A\n",
      "Generating train examples...: 6830 examples [00:03, 1888.33 examples/s]\u001B[A\n",
      "Generating train examples...: 7020 examples [00:03, 1888.96 examples/s]\u001B[A\n",
      "Generating train examples...: 7211 examples [00:03, 1894.50 examples/s]\u001B[A\n",
      "Generating train examples...: 7401 examples [00:04, 1896.14 examples/s]\u001B[A\n",
      "Generating train examples...: 7591 examples [00:04, 1897.27 examples/s]\u001B[A\n",
      "Generating train examples...: 7781 examples [00:04, 1896.59 examples/s]\u001B[A\n",
      "Generating train examples...: 7971 examples [00:04, 1894.63 examples/s]\u001B[A\n",
      "Generating train examples...: 8161 examples [00:04, 1893.46 examples/s]\u001B[A\n",
      "Generating train examples...: 8351 examples [00:04, 1894.68 examples/s]\u001B[A\n",
      "Generating train examples...: 8541 examples [00:04, 1893.17 examples/s]\u001B[A\n",
      "Generating train examples...: 8731 examples [00:04, 1892.85 examples/s]\u001B[A\n",
      "Generating train examples...: 8921 examples [00:04, 1893.56 examples/s]\u001B[A\n",
      "Generating train examples...: 9111 examples [00:04, 1893.03 examples/s]\u001B[A\n",
      "Generating train examples...: 9301 examples [00:05, 1895.02 examples/s]\u001B[A\n",
      "Generating train examples...: 9491 examples [00:05, 1892.70 examples/s]\u001B[A\n",
      "Generating train examples...: 9681 examples [00:05, 1894.51 examples/s]\u001B[A\n",
      "Generating train examples...: 9871 examples [00:05, 1892.42 examples/s]\u001B[A\n",
      "Generating train examples...: 10061 examples [00:05, 1892.01 examples/s]\u001B[A\n",
      "Generating train examples...: 10251 examples [00:05, 1892.98 examples/s]\u001B[A\n",
      "Generating train examples...: 10441 examples [00:05, 1891.21 examples/s]\u001B[A\n",
      "Generating train examples...: 10631 examples [00:05, 1892.07 examples/s]\u001B[A\n",
      "Generating train examples...: 10821 examples [00:05, 1893.46 examples/s]\u001B[A\n",
      "Generating train examples...: 11012 examples [00:05, 1896.36 examples/s]\u001B[A\n",
      "Generating train examples...: 11202 examples [00:06, 1896.05 examples/s]\u001B[A\n",
      "Generating train examples...: 11392 examples [00:06, 1896.04 examples/s]\u001B[A\n",
      "Generating train examples...: 11582 examples [00:06, 1895.98 examples/s]\u001B[A\n",
      "Generating train examples...: 11773 examples [00:06, 1897.86 examples/s]\u001B[A\n",
      "Generating train examples...: 11963 examples [00:06, 1897.36 examples/s]\u001B[A\n",
      "Generating train examples...: 12153 examples [00:06, 1894.82 examples/s]\u001B[A\n",
      "Generating train examples...: 12343 examples [00:06, 1894.36 examples/s]\u001B[A\n",
      "Generating train examples...: 12533 examples [00:06, 1894.96 examples/s]\u001B[A\n",
      "Generating train examples...: 12723 examples [00:06, 1893.32 examples/s]\u001B[A\n",
      "Generating train examples...: 12913 examples [00:06, 1893.62 examples/s]\u001B[A\n",
      "Generating train examples...: 13103 examples [00:07, 1892.45 examples/s]\u001B[A\n",
      "Generating train examples...: 13293 examples [00:07, 1893.43 examples/s]\u001B[A\n",
      "Generating train examples...: 13483 examples [00:07, 1892.52 examples/s]\u001B[A\n",
      "Generating train examples...: 13673 examples [00:07, 1892.86 examples/s]\u001B[A\n",
      "Generating train examples...: 13863 examples [00:07, 1894.07 examples/s]\u001B[A\n",
      "Generating train examples...: 14053 examples [00:07, 1893.58 examples/s]\u001B[A\n",
      "Generating train examples...: 14243 examples [00:07, 1893.76 examples/s]\u001B[A\n",
      "Generating train examples...: 14433 examples [00:07, 1894.22 examples/s]\u001B[A\n",
      "Generating train examples...: 14623 examples [00:07, 1894.84 examples/s]\u001B[A\n",
      "Generating train examples...: 14814 examples [00:07, 1894.71 examples/s]\u001B[A\n",
      "Generating train examples...: 15006 examples [00:08, 1899.53 examples/s]\u001B[A\n",
      "Generating train examples...: 15196 examples [00:08, 1898.28 examples/s]\u001B[A\n",
      "Generating train examples...: 15386 examples [00:08, 1898.61 examples/s]\u001B[A\n",
      "Generating train examples...: 15577 examples [00:08, 1899.23 examples/s]\u001B[A\n",
      "Generating train examples...: 15767 examples [00:08, 1895.26 examples/s]\u001B[A\n",
      "Generating train examples...: 15957 examples [00:08, 1895.20 examples/s]\u001B[A\n",
      "Generating train examples...: 16147 examples [00:08, 1895.52 examples/s]\u001B[A\n",
      "Generating train examples...: 16337 examples [00:08, 1895.01 examples/s]\u001B[A\n",
      "Generating train examples...: 16527 examples [00:08, 1894.33 examples/s]\u001B[A\n",
      "Generating train examples...: 16717 examples [00:08, 1894.39 examples/s]\u001B[A\n",
      "Generating train examples...: 16907 examples [00:09, 1894.56 examples/s]\u001B[A\n",
      "Generating train examples...: 17097 examples [00:09, 1896.02 examples/s]\u001B[A\n",
      "Generating train examples...: 17287 examples [00:09, 1893.67 examples/s]\u001B[A\n",
      "Generating train examples...: 17477 examples [00:09, 1893.24 examples/s]\u001B[A\n",
      "Generating train examples...: 17667 examples [00:09, 1892.56 examples/s]\u001B[A\n",
      "Generating train examples...: 17857 examples [00:09, 1893.60 examples/s]\u001B[A\n",
      "Generating train examples...: 18047 examples [00:09, 1895.11 examples/s]\u001B[A\n",
      "Generating train examples...: 18238 examples [00:09, 1897.16 examples/s]\u001B[A\n",
      "Generating train examples...: 18428 examples [00:09, 1897.34 examples/s]\u001B[A\n",
      "Generating train examples...: 18619 examples [00:09, 1900.02 examples/s]\u001B[A\n",
      "Generating train examples...: 18810 examples [00:10, 1896.45 examples/s]\u001B[A\n",
      "Generating train examples...: 19000 examples [00:10, 1895.74 examples/s]\u001B[A\n",
      "Generating train examples...: 19190 examples [00:10, 1895.16 examples/s]\u001B[A\n",
      "Generating train examples...: 19380 examples [00:10, 1892.06 examples/s]\u001B[A\n",
      "Generating train examples...: 19570 examples [00:10, 1891.37 examples/s]\u001B[A\n",
      "Generating train examples...: 19760 examples [00:10, 1890.93 examples/s]\u001B[A\n",
      "Generating train examples...: 19950 examples [00:10, 1888.07 examples/s]\u001B[A\n",
      "Generating train examples...: 20139 examples [00:10, 1887.05 examples/s]\u001B[A\n",
      "Generating train examples...: 20329 examples [00:10, 1888.23 examples/s]\u001B[A\n",
      "Generating train examples...: 20519 examples [00:10, 1888.90 examples/s]\u001B[A\n",
      "Generating train examples...: 20708 examples [00:11, 1888.94 examples/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...: 20898 examples [00:11, 1890.54 examples/s]\u001B[A\n",
      "Generating train examples...: 21088 examples [00:11, 1891.15 examples/s]\u001B[A\n",
      "Generating train examples...: 21278 examples [00:11, 1891.12 examples/s]\u001B[A\n",
      "Generating train examples...: 21468 examples [00:11, 1889.96 examples/s]\u001B[A\n",
      "Generating train examples...: 21658 examples [00:11, 1890.64 examples/s]\u001B[A\n",
      "Generating train examples...: 21848 examples [00:11, 1891.27 examples/s]\u001B[A\n",
      "Generating train examples...: 22038 examples [00:11, 1888.29 examples/s]\u001B[A\n",
      "Generating train examples...: 22227 examples [00:11, 1886.84 examples/s]\u001B[A\n",
      "Generating train examples...: 22417 examples [00:11, 1889.21 examples/s]\u001B[A\n",
      "Generating train examples...: 22608 examples [00:12, 1894.89 examples/s]\u001B[A\n",
      "Generating train examples...: 22798 examples [00:12, 1887.45 examples/s]\u001B[A\n",
      "Generating train examples...: 22988 examples [00:12, 1890.25 examples/s]\u001B[A\n",
      "Generating train examples...: 23178 examples [00:12, 1891.45 examples/s]\u001B[A\n",
      "Generating train examples...: 23368 examples [00:12, 1893.01 examples/s]\u001B[A\n",
      "Generating train examples...: 23559 examples [00:12, 1895.28 examples/s]\u001B[A\n",
      "Generating train examples...: 23749 examples [00:12, 1896.16 examples/s]\u001B[A\n",
      "Generating train examples...: 23939 examples [00:12, 1894.64 examples/s]\u001B[A\n",
      "Generating train examples...: 24129 examples [00:12, 1894.27 examples/s]\u001B[A\n",
      "Generating train examples...: 24320 examples [00:12, 1896.16 examples/s]\u001B[A\n",
      "Generating train examples...: 24510 examples [00:13, 1897.15 examples/s]\u001B[A\n",
      "Generating train examples...: 24700 examples [00:13, 1897.19 examples/s]\u001B[A\n",
      "Generating train examples...: 24891 examples [00:13, 1898.62 examples/s]\u001B[A\n",
      "Generating train examples...: 25082 examples [00:13, 1901.15 examples/s]\u001B[A\n",
      "Generating train examples...: 25273 examples [00:13, 1898.61 examples/s]\u001B[A\n",
      "Generating train examples...: 25463 examples [00:13, 1897.78 examples/s]\u001B[A\n",
      "Generating train examples...: 25653 examples [00:13, 1896.98 examples/s]\u001B[A\n",
      "Generating train examples...: 25843 examples [00:13, 1896.80 examples/s]\u001B[A\n",
      "Generating train examples...: 26034 examples [00:13, 1898.00 examples/s]\u001B[A\n",
      "Generating train examples...: 26224 examples [00:13, 1898.40 examples/s]\u001B[A\n",
      "Generating train examples...: 26414 examples [00:14, 1892.24 examples/s]\u001B[A\n",
      "Generating train examples...: 26604 examples [00:14, 1882.74 examples/s]\u001B[A\n",
      "Generating train examples...: 26793 examples [00:14, 1874.28 examples/s]\u001B[A\n",
      "Generating train examples...: 26981 examples [00:14, 1870.42 examples/s]\u001B[A\n",
      "Generating train examples...: 27169 examples [00:14, 1868.69 examples/s]\u001B[A\n",
      "Generating train examples...: 27356 examples [00:14, 1868.96 examples/s]\u001B[A\n",
      "Generating train examples...: 27544 examples [00:14, 1870.70 examples/s]\u001B[A\n",
      "Generating train examples...: 27733 examples [00:14, 1874.20 examples/s]\u001B[A\n",
      "Generating train examples...: 27921 examples [00:14, 1874.62 examples/s]\u001B[A\n",
      "Generating train examples...: 28109 examples [00:14, 1875.77 examples/s]\u001B[A\n",
      "Generating train examples...: 28297 examples [00:15, 1874.62 examples/s]\u001B[A\n",
      "Generating train examples...: 28485 examples [00:15, 1875.73 examples/s]\u001B[A\n",
      "Generating train examples...: 28673 examples [00:15, 1876.48 examples/s]\u001B[A\n",
      "Generating train examples...: 28862 examples [00:15, 1878.93 examples/s]\u001B[A\n",
      "Generating train examples...: 29050 examples [00:15, 1878.97 examples/s]\u001B[A\n",
      "Generating train examples...: 29239 examples [00:15, 1880.12 examples/s]\u001B[A\n",
      "Generating train examples...: 29428 examples [00:15, 1879.67 examples/s]\u001B[A\n",
      "Generating train examples...: 29618 examples [00:15, 1882.68 examples/s]\u001B[A\n",
      "Generating train examples...: 29808 examples [00:15, 1887.69 examples/s]\u001B[A\n",
      "Generating train examples...: 29998 examples [00:15, 1890.23 examples/s]\u001B[A\n",
      "Generating train examples...: 30188 examples [00:16, 1892.06 examples/s]\u001B[A\n",
      "Generating train examples...: 30378 examples [00:16, 1886.99 examples/s]\u001B[A\n",
      "Generating train examples...: 30567 examples [00:16, 1886.56 examples/s]\u001B[A\n",
      "Generating train examples...: 30757 examples [00:16, 1888.22 examples/s]\u001B[A\n",
      "Generating train examples...: 30946 examples [00:16, 1887.66 examples/s]\u001B[A\n",
      "Generating train examples...: 31135 examples [00:16, 1888.02 examples/s]\u001B[A\n",
      "Generating train examples...: 31324 examples [00:16, 1888.06 examples/s]\u001B[A\n",
      "Generating train examples...: 31514 examples [00:16, 1889.17 examples/s]\u001B[A\n",
      "Generating train examples...: 31704 examples [00:16, 1889.93 examples/s]\u001B[A\n",
      "Generating train examples...: 31894 examples [00:16, 1890.06 examples/s]\u001B[A\n",
      "Generating train examples...: 32084 examples [00:17, 1889.00 examples/s]\u001B[A\n",
      "Generating train examples...: 32274 examples [00:17, 1889.34 examples/s]\u001B[A\n",
      "Generating train examples...: 32463 examples [00:17, 1889.09 examples/s]\u001B[A\n",
      "Generating train examples...: 32653 examples [00:17, 1890.31 examples/s]\u001B[A\n",
      "Generating train examples...: 32843 examples [00:17, 1890.23 examples/s]\u001B[A\n",
      "Generating train examples...: 33033 examples [00:17, 1882.34 examples/s]\u001B[A\n",
      "Generating train examples...: 33222 examples [00:17, 1881.07 examples/s]\u001B[A\n",
      "Generating train examples...: 33412 examples [00:17, 1884.18 examples/s]\u001B[A\n",
      "Generating train examples...: 33601 examples [00:17, 1877.67 examples/s]\u001B[A\n",
      "Generating train examples...: 33789 examples [00:17, 1875.90 examples/s]\u001B[A\n",
      "Generating train examples...: 33977 examples [00:18, 1872.76 examples/s]\u001B[A\n",
      "Generating train examples...: 34165 examples [00:18, 1873.83 examples/s]\u001B[A\n",
      "Generating train examples...: 34354 examples [00:18, 1877.18 examples/s]\u001B[A\n",
      "Generating train examples...: 34542 examples [00:18, 1877.24 examples/s]\u001B[A\n",
      "Generating train examples...: 34731 examples [00:18, 1879.12 examples/s]\u001B[A\n",
      "Generating train examples...: 34920 examples [00:18, 1881.16 examples/s]\u001B[A\n",
      "Generating train examples...: 35109 examples [00:18, 1883.26 examples/s]\u001B[A\n",
      "Generating train examples...: 35299 examples [00:18, 1887.43 examples/s]\u001B[A\n",
      "Generating train examples...: 35488 examples [00:18, 1886.43 examples/s]\u001B[A\n",
      "Generating train examples...: 35677 examples [00:18, 1886.27 examples/s]\u001B[A\n",
      "Generating train examples...: 35866 examples [00:19, 1886.97 examples/s]\u001B[A\n",
      "Generating train examples...: 36055 examples [00:19, 1886.51 examples/s]\u001B[A\n",
      "Generating train examples...: 36244 examples [00:19, 1887.19 examples/s]\u001B[A\n",
      "Generating train examples...: 36433 examples [00:19, 1887.28 examples/s]\u001B[A\n",
      "Generating train examples...: 36623 examples [00:19, 1889.92 examples/s]\u001B[A\n",
      "Generating train examples...: 36812 examples [00:19, 1888.96 examples/s]\u001B[A\n",
      "Generating train examples...: 37002 examples [00:19, 1890.93 examples/s]\u001B[A\n",
      "Generating train examples...: 37192 examples [00:19, 1892.30 examples/s]\u001B[A\n",
      "Generating train examples...: 37382 examples [00:19, 1892.48 examples/s]\u001B[A\n",
      "Generating train examples...: 37572 examples [00:19, 1892.91 examples/s]\u001B[A\n",
      "Generating train examples...: 37762 examples [00:20, 1882.58 examples/s]\u001B[A\n",
      "Generating train examples...: 37951 examples [00:20, 1880.29 examples/s]\u001B[A\n",
      "Generating train examples...: 38140 examples [00:20, 1880.68 examples/s]\u001B[A\n",
      "Generating train examples...: 38329 examples [00:20, 1874.25 examples/s]\u001B[A\n",
      "Generating train examples...: 38517 examples [00:20, 1873.48 examples/s]\u001B[A\n",
      "Generating train examples...: 38705 examples [00:20, 1865.80 examples/s]\u001B[A\n",
      "Generating train examples...: 38892 examples [00:20, 1865.93 examples/s]\u001B[A\n",
      "Generating train examples...: 39081 examples [00:20, 1871.83 examples/s]\u001B[A\n",
      "Generating train examples...: 39271 examples [00:20, 1878.57 examples/s]\u001B[A\n",
      "Generating train examples...: 39462 examples [00:20, 1885.17 examples/s]\u001B[A\n",
      "Generating train examples...: 39652 examples [00:21, 1889.44 examples/s]\u001B[A\n",
      "Generating train examples...: 39842 examples [00:21, 1892.22 examples/s]\u001B[A\n",
      "Generating train examples...: 40033 examples [00:21, 1896.14 examples/s]\u001B[A\n",
      "Generating train examples...: 40224 examples [00:21, 1897.91 examples/s]\u001B[A\n",
      "Generating train examples...: 40415 examples [00:21, 1899.34 examples/s]\u001B[A\n",
      "Generating train examples...: 40605 examples [00:21, 1898.44 examples/s]\u001B[A\n",
      "Generating train examples...: 40796 examples [00:21, 1899.95 examples/s]\u001B[A\n",
      "Generating train examples...: 40987 examples [00:21, 1899.92 examples/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...: 41177 examples [00:21, 1899.48 examples/s]\u001B[A\n",
      "Generating train examples...: 41367 examples [00:21, 1897.90 examples/s]\u001B[A\n",
      "Generating train examples...: 41558 examples [00:22, 1899.14 examples/s]\u001B[A\n",
      "Generating train examples...: 41748 examples [00:22, 1898.34 examples/s]\u001B[A\n",
      "Generating train examples...: 41938 examples [00:22, 1897.14 examples/s]\u001B[A\n",
      "Generating train examples...: 42128 examples [00:22, 1896.30 examples/s]\u001B[A\n",
      "Generating train examples...: 42319 examples [00:22, 1897.74 examples/s]\u001B[A\n",
      "Generating train examples...: 42509 examples [00:22, 1896.19 examples/s]\u001B[A\n",
      "Generating train examples...: 42699 examples [00:22, 1894.64 examples/s]\u001B[A\n",
      "Generating train examples...: 42889 examples [00:22, 1892.64 examples/s]\u001B[A\n",
      "Generating train examples...: 43080 examples [00:22, 1896.08 examples/s]\u001B[A\n",
      "Generating train examples...: 43270 examples [00:23, 1896.65 examples/s]\u001B[A\n",
      "Generating train examples...: 43460 examples [00:23, 1893.42 examples/s]\u001B[A\n",
      "Generating train examples...: 43650 examples [00:23, 1892.84 examples/s]\u001B[A\n",
      "Generating train examples...: 43841 examples [00:23, 1897.30 examples/s]\u001B[A\n",
      "Generating train examples...: 44031 examples [00:23, 1895.35 examples/s]\u001B[A\n",
      "Generating train examples...: 44221 examples [00:23, 1896.43 examples/s]\u001B[A\n",
      "Generating train examples...: 44412 examples [00:23, 1897.83 examples/s]\u001B[A\n",
      "Generating train examples...: 44602 examples [00:23, 1896.21 examples/s]\u001B[A\n",
      "Generating train examples...: 44792 examples [00:23, 1895.77 examples/s]\u001B[A\n",
      "Generating train examples...: 44982 examples [00:23, 1894.87 examples/s]\u001B[A\n",
      "Generating train examples...: 45173 examples [00:24, 1897.60 examples/s]\u001B[A\n",
      "Generating train examples...: 45363 examples [00:24, 1896.91 examples/s]\u001B[A\n",
      "Generating train examples...: 45553 examples [00:24, 1897.36 examples/s]\u001B[A\n",
      "Generating train examples...: 45743 examples [00:24, 1897.09 examples/s]\u001B[A\n",
      "Generating train examples...: 45933 examples [00:24, 1897.34 examples/s]\u001B[A\n",
      "Generating train examples...: 46123 examples [00:24, 1896.56 examples/s]\u001B[A\n",
      "Generating train examples...: 46314 examples [00:24, 1897.92 examples/s]\u001B[A\n",
      "Generating train examples...: 46504 examples [00:24, 1898.04 examples/s]\u001B[A\n",
      "Generating train examples...: 46694 examples [00:24, 1896.10 examples/s]\u001B[A\n",
      "Generating train examples...: 46884 examples [00:24, 1894.55 examples/s]\u001B[A\n",
      "Generating train examples...: 47074 examples [00:25, 1895.06 examples/s]\u001B[A\n",
      "Generating train examples...: 47265 examples [00:25, 1896.87 examples/s]\u001B[A\n",
      "Generating train examples...: 47455 examples [00:25, 1896.31 examples/s]\u001B[A\n",
      "Generating train examples...: 47646 examples [00:25, 1898.88 examples/s]\u001B[A\n",
      "Generating train examples...: 47836 examples [00:25, 1897.86 examples/s]\u001B[A\n",
      "Generating train examples...: 48027 examples [00:25, 1899.09 examples/s]\u001B[A\n",
      "Generating train examples...: 48217 examples [00:25, 1898.19 examples/s]\u001B[A\n",
      "Generating train examples...: 48407 examples [00:25, 1896.84 examples/s]\u001B[A\n",
      "Generating train examples...: 48597 examples [00:25, 1895.91 examples/s]\u001B[A\n",
      "Generating train examples...: 48787 examples [00:25, 1893.74 examples/s]\u001B[A\n",
      "Generating train examples...: 48978 examples [00:26, 1896.28 examples/s]\u001B[A\n",
      "Generating train examples...: 49168 examples [00:26, 1896.86 examples/s]\u001B[A\n",
      "Generating train examples...: 49358 examples [00:26, 1895.92 examples/s]\u001B[A\n",
      "Generating train examples...: 49549 examples [00:26, 1897.29 examples/s]\u001B[A\n",
      "Generating train examples...: 49739 examples [00:26, 1894.86 examples/s]\u001B[A\n",
      "Generating train examples...: 49929 examples [00:26, 1896.11 examples/s]\u001B[A\n",
      "Generating train examples...: 50119 examples [00:26, 1897.08 examples/s]\u001B[A\n",
      "Generating train examples...: 50309 examples [00:26, 1896.77 examples/s]\u001B[A\n",
      "Generating train examples...: 50499 examples [00:26, 1895.54 examples/s]\u001B[A\n",
      "Generating train examples...: 50689 examples [00:26, 1896.08 examples/s]\u001B[A\n",
      "Generating train examples...: 50879 examples [00:27, 1896.05 examples/s]\u001B[A\n",
      "Generating train examples...: 51069 examples [00:27, 1897.06 examples/s]\u001B[A\n",
      "Generating train examples...: 51260 examples [00:27, 1900.16 examples/s]\u001B[A\n",
      "Generating train examples...: 51451 examples [00:27, 1899.49 examples/s]\u001B[A\n",
      "Generating train examples...: 51641 examples [00:27, 1897.80 examples/s]\u001B[A\n",
      "Generating train examples...: 51831 examples [00:27, 1897.50 examples/s]\u001B[A\n",
      "Generating train examples...: 52021 examples [00:27, 1897.32 examples/s]\u001B[A\n",
      "Generating train examples...: 52212 examples [00:27, 1899.98 examples/s]\u001B[A\n",
      "Generating train examples...: 52403 examples [00:27, 1901.80 examples/s]\u001B[A\n",
      "Generating train examples...: 52594 examples [00:27, 1900.07 examples/s]\u001B[A\n",
      "Generating train examples...: 52785 examples [00:28, 1900.68 examples/s]\u001B[A\n",
      "Generating train examples...: 52976 examples [00:28, 1899.09 examples/s]\u001B[A\n",
      "Generating train examples...: 53166 examples [00:28, 1896.50 examples/s]\u001B[A\n",
      "Generating train examples...: 53356 examples [00:28, 1896.60 examples/s]\u001B[A\n",
      "Generating train examples...: 53546 examples [00:28, 1896.83 examples/s]\u001B[A\n",
      "Generating train examples...: 53736 examples [00:28, 1897.26 examples/s]\u001B[A\n",
      "Generating train examples...: 53926 examples [00:28, 1897.99 examples/s]\u001B[A\n",
      "Generating train examples...: 54116 examples [00:28, 1896.22 examples/s]\u001B[A\n",
      "Generating train examples...: 54307 examples [00:28, 1898.00 examples/s]\u001B[A\n",
      "Generating train examples...: 54497 examples [00:28, 1897.13 examples/s]\u001B[A\n",
      "Generating train examples...: 54687 examples [00:29, 1896.22 examples/s]\u001B[A\n",
      "Generating train examples...: 54878 examples [00:29, 1897.20 examples/s]\u001B[A\n",
      "Generating train examples...: 55070 examples [00:29, 1901.69 examples/s]\u001B[A\n",
      "Generating train examples...: 55261 examples [00:29, 1900.59 examples/s]\u001B[A\n",
      "Generating train examples...: 55452 examples [00:29, 1900.83 examples/s]\u001B[A\n",
      "Generating train examples...: 55643 examples [00:29, 1899.97 examples/s]\u001B[A\n",
      "Generating train examples...: 55833 examples [00:29, 1897.37 examples/s]\u001B[A\n",
      "Generating train examples...: 56023 examples [00:29, 1896.53 examples/s]\u001B[A\n",
      "Generating train examples...: 56214 examples [00:29, 1897.96 examples/s]\u001B[A\n",
      "Generating train examples...: 56405 examples [00:29, 1899.22 examples/s]\u001B[A\n",
      "Generating train examples...: 56595 examples [00:30, 1896.65 examples/s]\u001B[A\n",
      "Generating train examples...: 56785 examples [00:30, 1897.26 examples/s]\u001B[A\n",
      "Generating train examples...: 56975 examples [00:30, 1897.44 examples/s]\u001B[A\n",
      "Generating train examples...: 57165 examples [00:30, 1894.58 examples/s]\u001B[A\n",
      "Generating train examples...: 57355 examples [00:30, 1894.40 examples/s]\u001B[A\n",
      "Generating train examples...: 57545 examples [00:30, 1892.20 examples/s]\u001B[A\n",
      "Generating train examples...: 57735 examples [00:30, 1893.99 examples/s]\u001B[A\n",
      "Generating train examples...: 57925 examples [00:30, 1895.53 examples/s]\u001B[A\n",
      "Generating train examples...: 58116 examples [00:30, 1897.91 examples/s]\u001B[A\n",
      "Generating train examples...: 58306 examples [00:30, 1897.55 examples/s]\u001B[A\n",
      "Generating train examples...: 58496 examples [00:31, 1896.20 examples/s]\u001B[A\n",
      "Generating train examples...: 58687 examples [00:31, 1897.01 examples/s]\u001B[A\n",
      "Generating train examples...: 58878 examples [00:31, 1898.51 examples/s]\u001B[A\n",
      "Generating train examples...: 59068 examples [00:31, 1896.60 examples/s]\u001B[A\n",
      "Generating train examples...: 59258 examples [00:31, 1896.93 examples/s]\u001B[A\n",
      "Generating train examples...: 59448 examples [00:31, 1895.03 examples/s]\u001B[A\n",
      "Generating train examples...: 59638 examples [00:31, 1893.58 examples/s]\u001B[A\n",
      "Generating train examples...: 59829 examples [00:31, 1896.07 examples/s]\u001B[A\n",
      "                                                                        \u001B[A\n",
      "Shuffling mnist-train.tfrecord...:   0%|       | 0/60000 [00:00<?, ? examples/s]\u001B[A\n",
      "Shuffling mnist-train.tfrecord...:  38%|▍| 22630/60000 [00:00<00:00, 226299.14 e\u001B[A\n",
      "                                                                                \u001B[AINFO:absl:Done writing mnist-train.tfrecord. Number of examples: 60000 (shards: [60000])\n",
      "Generating splits...:  50%|█████████         | 1/2 [00:32<00:32, 32.04s/ splits]\n",
      "Generating test examples...: 0 examples [00:00, ? examples/s]\u001B[A\n",
      "Generating test examples...: 156 examples [00:00, 1556.64 examples/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test examples...: 346 examples [00:00, 1645.19 examples/s]\u001B[A\n",
      "Generating test examples...: 537 examples [00:00, 1714.91 examples/s]\u001B[A\n",
      "Generating test examples...: 727 examples [00:00, 1766.46 examples/s]\u001B[A\n",
      "Generating test examples...: 918 examples [00:00, 1805.04 examples/s]\u001B[A\n",
      "Generating test examples...: 1110 examples [00:00, 1837.11 examples/s]\u001B[A\n",
      "Generating test examples...: 1301 examples [00:00, 1856.50 examples/s]\u001B[A\n",
      "Generating test examples...: 1492 examples [00:00, 1869.75 examples/s]\u001B[A\n",
      "Generating test examples...: 1683 examples [00:00, 1879.08 examples/s]\u001B[A\n",
      "Generating test examples...: 1874 examples [00:01, 1886.36 examples/s]\u001B[A\n",
      "Generating test examples...: 2065 examples [00:01, 1891.27 examples/s]\u001B[A\n",
      "Generating test examples...: 2255 examples [00:01, 1893.23 examples/s]\u001B[A\n",
      "Generating test examples...: 2445 examples [00:01, 1893.41 examples/s]\u001B[A\n",
      "Generating test examples...: 2633 examples [00:01, 1885.65 examples/s]\u001B[A\n",
      "Generating test examples...: 2823 examples [00:01, 1889.27 examples/s]\u001B[A\n",
      "Generating test examples...: 3014 examples [00:01, 1894.21 examples/s]\u001B[A\n",
      "Generating test examples...: 3205 examples [00:01, 1896.91 examples/s]\u001B[A\n",
      "Generating test examples...: 3395 examples [00:01, 1896.85 examples/s]\u001B[A\n",
      "Generating test examples...: 3585 examples [00:01, 1897.10 examples/s]\u001B[A\n",
      "Generating test examples...: 3776 examples [00:02, 1898.64 examples/s]\u001B[A\n",
      "Generating test examples...: 3966 examples [00:02, 1897.35 examples/s]\u001B[A\n",
      "Generating test examples...: 4156 examples [00:02, 1897.68 examples/s]\u001B[A\n",
      "Generating test examples...: 4348 examples [00:02, 1901.46 examples/s]\u001B[A\n",
      "Generating test examples...: 4539 examples [00:02, 1900.31 examples/s]\u001B[A\n",
      "Generating test examples...: 4730 examples [00:02, 1901.76 examples/s]\u001B[A\n",
      "Generating test examples...: 4922 examples [00:02, 1905.89 examples/s]\u001B[A\n",
      "Generating test examples...: 5113 examples [00:02, 1903.60 examples/s]\u001B[A\n",
      "Generating test examples...: 5304 examples [00:02, 1902.62 examples/s]\u001B[A\n",
      "Generating test examples...: 5495 examples [00:02, 1900.95 examples/s]\u001B[A\n",
      "Generating test examples...: 5686 examples [00:03, 1901.02 examples/s]\u001B[A\n",
      "Generating test examples...: 5877 examples [00:03, 1899.83 examples/s]\u001B[A\n",
      "Generating test examples...: 6068 examples [00:03, 1900.91 examples/s]\u001B[A\n",
      "Generating test examples...: 6259 examples [00:03, 1897.36 examples/s]\u001B[A\n",
      "Generating test examples...: 6449 examples [00:03, 1895.81 examples/s]\u001B[A\n",
      "Generating test examples...: 6639 examples [00:03, 1896.63 examples/s]\u001B[A\n",
      "Generating test examples...: 6829 examples [00:03, 1897.25 examples/s]\u001B[A\n",
      "Generating test examples...: 7020 examples [00:03, 1898.84 examples/s]\u001B[A\n",
      "Generating test examples...: 7211 examples [00:03, 1900.14 examples/s]\u001B[A\n",
      "Generating test examples...: 7402 examples [00:03, 1901.00 examples/s]\u001B[A\n",
      "Generating test examples...: 7594 examples [00:04, 1904.03 examples/s]\u001B[A\n",
      "Generating test examples...: 7785 examples [00:04, 1901.62 examples/s]\u001B[A\n",
      "Generating test examples...: 7976 examples [00:04, 1899.71 examples/s]\u001B[A\n",
      "Generating test examples...: 8166 examples [00:04, 1899.19 examples/s]\u001B[A\n",
      "Generating test examples...: 8357 examples [00:04, 1899.56 examples/s]\u001B[A\n",
      "Generating test examples...: 8547 examples [00:04, 1898.84 examples/s]\u001B[A\n",
      "Generating test examples...: 8738 examples [00:04, 1900.27 examples/s]\u001B[A\n",
      "Generating test examples...: 8929 examples [00:04, 1899.91 examples/s]\u001B[A\n",
      "Generating test examples...: 9119 examples [00:04, 1898.36 examples/s]\u001B[A\n",
      "Generating test examples...: 9309 examples [00:04, 1897.79 examples/s]\u001B[A\n",
      "Generating test examples...: 9500 examples [00:05, 1899.37 examples/s]\u001B[A\n",
      "Generating test examples...: 9690 examples [00:05, 1898.76 examples/s]\u001B[A\n",
      "Generating test examples...: 9881 examples [00:05, 1900.47 examples/s]\u001B[A\n",
      "                                                                      \u001B[A\n",
      "Shuffling mnist-test.tfrecord...:   0%|        | 0/10000 [00:00<?, ? examples/s]\u001B[A\n",
      "                                                                                \u001B[AINFO:absl:Done writing mnist-test.tfrecord. Number of examples: 10000 (shards: [10000])\n",
      "\u001B[1mDataset mnist downloaded and prepared to ./tfds/mnist/3.0.1. Subsequent calls will reuse this data.\u001B[0m\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:root:---------- Suggestions to improve input_fn performance ----------\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3_1, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but ShuffleDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but RepeatDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "INFO:root:[input_fn] - batch(): batch_size set to 256\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but BatchDatasetV2 used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Map is called prior to Batch. Consider reversing the order and performing the map function in a batched fashion to increase the performance of the input function \n",
      "WARNING:root:[input_fn] - flat_map(): use map() instead of flat_map() to improve performance and parallelize reads. If you are not calling `flat_map` directly, check if you are using: from_generator, TextLineDataset, TFRecordDataset, or FixedLenthRecordDataset. If so, set `num_parallel_reads` to > 1 or cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE, and map() will be used automatically\n",
      "INFO:root:----------------- End of input_fn suggestions -----------------\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:cerebras.stack.tools.caching_stack:Using lair flow into stack\n",
      "=============== Starting Cerebras Compilation ===============                   \n",
      "Stack:   0%|                                         | 0/10 [00:00s, ?stages/s ]=============== Cerebras Compilation Completed ===============\n",
      "salloc: Relinquishing job allocation 283011\n"
     ]
    }
   ],
   "source": [
    "model_dir_exists = os.path.isdir(\"tutorial_model_dir\")\n",
    "if model_dir_exists:\n",
    "    !rm -rf tutorial_model_dir\n",
    "\n",
    "!salloc ${SLURM_ARGUMENTS} --ntasks=1 singularity exec --bind ${BIND_LOCATIONS} --pwd ${YOUR_ENTRY_SCRIPT_LOCATION} ${CEREBRAS_CONTAINER} python run.py --mode train --validate_only --model_dir tutorial_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21e74e",
   "metadata": {},
   "source": [
    "---\n",
    "Example of the expected output (final lines):\n",
    "\n",
    "    [--- OUTPUT SNIPPED FOR KEEPING THIS EXAMPLE SHORT ---]\n",
    "    =============== Starting Cerebras Compilation ===============                   \n",
    "    Stack:   0%|                      | 0/10 [00:00s, ?stages/s ]\n",
    "    =============== Cerebras Compilation Completed ==============\n",
    "---\n",
    "\n",
    "The command above exectuted a Slurm job using `srun` and `singularity`, both with multiple arguments.\n",
    "\n",
    "For the Slurm arguments, the following were used:\n",
    "* **--ntasks=7**: Specify  the number of tasks to run.\n",
    "* **--time=0-00:15**: Set a limit on the total run time of the job allocation. The format used is \"dd-hh:mm:mm\".\n",
    "* **--cpus-per-task=28**: Request that ncpus be allocated per process. A 28-core CPU processor is being used per task.\n",
    "* **--account=ACCOUNT_ID**: this is the AI tutorial allocation account to which resource utilization is being charged.\n",
    "* **--pty**: Execute task zero in pseudo terminal mode. Meaning, it will provide an interactive session.\n",
    "\n",
    "For the Singularity/Apptainer arguments, the following were used:\n",
    "* **exec**: the \"exec\" mode is an alternative to running the \"run\" or \"shell\" mode, and it allows running a specific command.\n",
    "* **--bind \\${BIND_LOCATIONS}**: it allows specifying the bind paths, or folders to make available to the container apps in the same (or a specific) location. The folder we are requesting to bind is the one in which the FC-MNIST example is located. Other folders with input data should also be mounted as required.\n",
    "* **--pwd \\${YOUR_ENTRY_SCRIPT_LOCATION}**: sets the working directory to use when running the commands with singularity exec. The actual value is `/ocean/projects/ACCOUNT_ID/USERNAME/modelzoo/modelzoo/fc_mnist/tf`\n",
    "* **${CEREBRAS_CONTAINER}**: this is the full path to the latest Cerebras cbcore container. The actual value is `/ocean/neocortex/cerebras/cbcore_latest.sif`.\n",
    "\n",
    "And as for the Python run.py file, the following arguments were used:\n",
    "* **--mode train**: this use the training mode on the Cerebras software stack. Other modes available are the evaluation and prediction mode. More information about this can be found in the [Cerebras Documentation](https://docs.cerebras.net/en/1.6.0/tensorflow-docs/running-a-model/train-eval-predict.html).\n",
    "* **--validate_only**: as mentioned above, this argument allow running a light-weight compilationthat helps determine any unsupported TensorFlow layer or functionality is being used in the code.\n",
    "* **--model_dir tutorial_model_dir**: this argument specifies the target directory to use for the compilation process. Using different folders for this argument allows starting from scratch when something goes wrong.\n",
    "\n",
    "\n",
    "### Step 5: Compile the code\n",
    "\n",
    "Run the full compilation using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735ce091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salloc: Granted job allocation 283012\n",
      "salloc: Waiting for resource configuration\n",
      "salloc: Nodes sdf-1 are ready for job\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {}\n",
      "INFO:root:Running None on CS-2\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:root:---------- Suggestions to improve input_fn performance ----------\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3_1, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but ShuffleDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but RepeatDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "INFO:root:[input_fn] - batch(): batch_size set to 256\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but BatchDatasetV2 used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Map is called prior to Batch. Consider reversing the order and performing the map function in a batched fashion to increase the performance of the input function \n",
      "WARNING:root:[input_fn] - flat_map(): use map() instead of flat_map() to improve performance and parallelize reads. If you are not calling `flat_map` directly, check if you are using: from_generator, TextLineDataset, TFRecordDataset, or FixedLenthRecordDataset. If so, set `num_parallel_reads` to > 1 or cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE, and map() will be used automatically\n",
      "INFO:root:----------------- End of input_fn suggestions -----------------\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:cerebras.stack.tools.caching_stack:Using lair flow into stack\n",
      "=============== Starting Cerebras Compilation ===============                   \n",
      "Estimating performance:  94%|█████████████████▉ | 33/35 [03:10s, 25.29s/stages ]=============== Cerebras Compilation Completed ===============\n",
      "salloc: Relinquishing job allocation 283012\n"
     ]
    }
   ],
   "source": [
    "!salloc ${SLURM_ARGUMENTS} --ntasks=1 singularity exec --bind ${BIND_LOCATIONS} --pwd ${YOUR_ENTRY_SCRIPT_LOCATION} ${CEREBRAS_CONTAINER} python run.py --mode train --compile_only --model_dir tutorial_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73573bb5",
   "metadata": {},
   "source": [
    "---\n",
    "Example of the expected output (final lines):\n",
    "\n",
    "    [--- OUTPUT SNIPPED FOR KEEPING THIS EXAMPLE SHORT ---]\n",
    "    =============== Starting Cerebras Compilation ===============\n",
    "    Estimating performance:  94%|█████████| 33/35 [00:50s,  3.99s/stages ]\n",
    "    =============== Cerebras Compilation Completed ===============\n",
    "---\n",
    "   \n",
    "Similar to the previous command, another job was executed using `srun` and `singularity`, but now in compilation mode.\n",
    "\n",
    "For the Python run.py file, the following argument changed:\n",
    "* **--compile_only**: as mentioned above, this steps runs the full compilation (on CPU) through all stages of the Cerebras software stack to generate a CS system executable.\n",
    "\n",
    "### Step 6: Run the training on the CS machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea184d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sdf-1|billing=98,cpu=98,gres/cs=1,node=1']\n",
      "The CS_IP_ADDR (10.8.88.10) and NODE_ID (sdf-1) environment variables have been set.\n",
      "salloc: Granted job allocation 283014\n",
      "salloc: Waiting for resource configuration\n",
      "salloc: Nodes sdf-1 are ready for job\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': ['sdf-1:21893'], 'worker': ['sdf-1:21895', 'sdf-1:21897', 'sdf-1:21899', 'sdf-1:21901', 'sdf-1:21903', 'sdf-1:21905']}, 'task': {'type': 'chief', 'index': 0}}\n",
      "INFO:root:Running train on CS-2\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "2023-08-04 08:24:56.489613: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2023-08-04 08:24:56.495056: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2700000000 Hz\n",
      "2023-08-04 08:24:56.497161: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x84e7c40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-04 08:24:56.497188: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:root:---------- Suggestions to improve input_fn performance ----------\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:[input_fn] - interleave(): in ParallelInterleaveDatasetV3_1, `cycle_length` is not being set to CS_AUTOTUNE. Currently, it is set to 16. If determinism is not required, Using CS_AUTOTUNE is likely to improve performance unless you are deliberately using a fine-tuned value.e.g. dataset = dataset.interleave(map_func, cycle_length=cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE)\n",
      "WARNING:root:AssertCardinalityDataset is not recognized by the Cerebras input_fn analyzer and cannot be evaluated for potential optimizations. Please report this to the Cerebras Support Team.\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but ShuffleDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but RepeatDataset used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "INFO:root:[input_fn] - batch(): batch_size set to 256\n",
      "WARNING:root:Tensorflow recommends that most dataset input pipelines end with a call to prefetch, but BatchDatasetV2 used in input_fn after prefetch(). Unless this is a careful design choice, consider calling prefetch last\n",
      "WARNING:root:Map is called prior to Batch. Consider reversing the order and performing the map function in a batched fashion to increase the performance of the input function \n",
      "WARNING:root:[input_fn] - flat_map(): use map() instead of flat_map() to improve performance and parallelize reads. If you are not calling `flat_map` directly, check if you are using: from_generator, TextLineDataset, TFRecordDataset, or FixedLenthRecordDataset. If so, set `num_parallel_reads` to > 1 or cerebras.tf.tools.analyze_input_fn.CS_AUTOTUNE, and map() will be used automatically\n",
      "INFO:root:----------------- End of input_fn suggestions -----------------\n",
      "INFO:absl:Load dataset info from ./tfds/mnist/3.0.1\n",
      "INFO:absl:Reusing dataset mnist (./tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from ./tfds/mnist/3.0.1\n",
      "INFO:cerebras.stack.tools.caching_stack:Using lair flow into stack\n",
      "Executing incremental compile:  34%|███▍      | 12/35 [00:10s,  3.34s/stages ]        INFO:cerebras.stack.tools.caching_stack:IncrementalCompileFlow found a cached compilation for this model configuration\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:root:Programming CS-2 fabric. This may take a couple of minutes. Please do not interrupt.\n",
      "TSKM202 08:26:23  Checkpoint callback registered!\n",
      "TSKM200 08:26:23  Getting block prebuffer numbers from cm.daemon 10.8.88.10:9001\n",
      "TSKM205 08:26:23  Waiting for block prebuffer percentage to rise above 50%, now 0%\n",
      "MSGS088 08:26:24  \u001B[0;33mWARNING: \u001B[0mMessage count for TSKM205 reached maximum 1: further instances will be not be printed\n",
      "TSKM200 08:26:32  Block prebuffer percentage is sufficient: 57\n",
      "TSKM201 08:26:32  Send block sizes:\n",
      "TSKM201 08:26:32> pre-cliff: 196607, post-cliff: 196607; using send block size: 196607\n",
      "TSKM201 08:26:32> Receive block sizes: \n",
      "TSKM201 08:26:32> pre-cliff: 50000, post-cliff: 50000; using receive block size: 50000\n",
      "Cerebras compilation completed:  37%|███▋      | 13/35 [00:30s,  2.31s/stages ]\n",
      "INFO:root:Fabric programmed: this took 44.67139029502869 seconds.\n",
      "INFO:tensorflow:Waiting for 6 streamer(s) to prime the data pipeline\n",
      "INFO:tensorflow:Streamers are ready\n",
      "INFO:root:Chief fully up. Waiting for Streaming (using 0.76% of fabric cores)\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:global step 0: loss = 2.326171875 (55.96 steps/sec)\n",
      "INFO:tensorflow:global step 100: loss = 0.261474609375 (782.07 steps/sec)\n",
      "INFO:tensorflow:global step 200: loss = 0.125 (1186.58 steps/sec)\n",
      "INFO:tensorflow:global step 300: loss = 0.1470947265625 (1524.76 steps/sec)\n",
      "INFO:tensorflow:global step 400: loss = 0.1297607421875 (1775.86 steps/sec)\n",
      "INFO:tensorflow:global step 500: loss = 0.11773681640625 (1974.6 steps/sec)\n",
      "INFO:tensorflow:global step 600: loss = 0.1068115234375 (2039.9 steps/sec)\n",
      "INFO:tensorflow:global step 700: loss = 0.08306884765625 (2172.41 steps/sec)\n",
      "INFO:tensorflow:global step 800: loss = 0.0361328125 (2281.0 steps/sec)\n",
      "INFO:tensorflow:global step 900: loss = 0.056671142578125 (2376.65 steps/sec)\n",
      "INFO:tensorflow:global step 1000: loss = 0.0655517578125 (2456.3 steps/sec)\n",
      "INFO:tensorflow:global step 1100: loss = 0.0653076171875 (2526.49 steps/sec)\n",
      "INFO:tensorflow:global step 1200: loss = 0.027862548828125 (2588.46 steps/sec)\n",
      "INFO:tensorflow:global step 1300: loss = 0.0235137939453125 (2645.34 steps/sec)\n",
      "INFO:tensorflow:global step 1400: loss = 0.08624267578125 (2693.55 steps/sec)\n",
      "INFO:tensorflow:global step 1500: loss = 0.041473388671875 (2738.17 steps/sec)\n",
      "INFO:tensorflow:global step 1600: loss = 0.03973388671875 (2776.12 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 1700: loss = 0.037811279296875 (2812.92 steps/sec)\n",
      "INFO:tensorflow:global step 1800: loss = 0.02142333984375 (2845.64 steps/sec)\n",
      "INFO:tensorflow:global step 1900: loss = 0.0208740234375 (2877.17 steps/sec)\n",
      "INFO:tensorflow:global step 2000: loss = 0.032745361328125 (2904.57 steps/sec)\n",
      "INFO:tensorflow:global step 2100: loss = 0.0225830078125 (2930.5 steps/sec)\n",
      "INFO:tensorflow:global step 2200: loss = 0.031097412109375 (2953.7 steps/sec)\n",
      "INFO:tensorflow:global step 2300: loss = 0.02081298828125 (2977.08 steps/sec)\n",
      "INFO:tensorflow:global step 2400: loss = 0.00661468505859375 (2996.56 steps/sec)\n",
      "INFO:tensorflow:global step 2500: loss = 0.01525115966796875 (3015.87 steps/sec)\n",
      "INFO:tensorflow:global step 2600: loss = 0.00873565673828125 (3032.47 steps/sec)\n",
      "INFO:tensorflow:global step 2700: loss = 0.0101470947265625 (3050.52 steps/sec)\n",
      "INFO:tensorflow:global step 2800: loss = 0.032745361328125 (3065.99 steps/sec)\n",
      "INFO:tensorflow:global step 2900: loss = 0.01284027099609375 (3081.38 steps/sec)\n",
      "INFO:tensorflow:global step 3000: loss = 0.03558349609375 (3093.78 steps/sec)\n",
      "INFO:tensorflow:global step 3100: loss = 0.00852203369140625 (3101.31 steps/sec)\n",
      "INFO:tensorflow:global step 3200: loss = 0.0279541015625 (3112.45 steps/sec)\n",
      "INFO:tensorflow:global step 3300: loss = 0.003299713134765625 (3124.26 steps/sec)\n",
      "INFO:tensorflow:global step 3400: loss = 0.0241241455078125 (3133.89 steps/sec)\n",
      "INFO:tensorflow:global step 3500: loss = 0.021728515625 (3144.75 steps/sec)\n",
      "INFO:tensorflow:global step 3600: loss = 0.0209808349609375 (3151.22 steps/sec)\n",
      "INFO:tensorflow:global step 3700: loss = 0.0166473388671875 (3161.17 steps/sec)\n",
      "INFO:tensorflow:global step 3800: loss = 0.01053619384765625 (3170.19 steps/sec)\n",
      "INFO:tensorflow:global step 3900: loss = 0.013427734375 (3179.85 steps/sec)\n",
      "INFO:tensorflow:global step 4000: loss = 0.01000213623046875 (3188.0 steps/sec)\n",
      "INFO:tensorflow:global step 4100: loss = 0.01108551025390625 (3195.77 steps/sec)\n",
      "INFO:tensorflow:global step 4200: loss = 0.0069427490234375 (3202.88 steps/sec)\n",
      "INFO:tensorflow:global step 4300: loss = 0.0298614501953125 (3210.04 steps/sec)\n",
      "INFO:tensorflow:global step 4400: loss = 0.00945281982421875 (3217.83 steps/sec)\n",
      "INFO:tensorflow:global step 4500: loss = 0.0171661376953125 (3224.38 steps/sec)\n",
      "INFO:tensorflow:global step 4600: loss = 0.0084075927734375 (3230.68 steps/sec)\n",
      "INFO:tensorflow:global step 4700: loss = 0.00814056396484375 (3236.46 steps/sec)\n",
      "INFO:tensorflow:global step 4800: loss = 0.00847625732421875 (3243.43 steps/sec)\n",
      "INFO:tensorflow:global step 4900: loss = 0.00848388671875 (3249.04 steps/sec)\n",
      "INFO:tensorflow:global step 5000: loss = 0.0282440185546875 (3255.45 steps/sec)\n",
      "INFO:tensorflow:global step 5100: loss = 0.01068115234375 (3260.08 steps/sec)\n",
      "INFO:tensorflow:global step 5200: loss = 0.0097198486328125 (3265.77 steps/sec)\n",
      "INFO:tensorflow:global step 5300: loss = 0.00363922119140625 (3270.28 steps/sec)\n",
      "INFO:tensorflow:global step 5400: loss = 0.035491943359375 (3275.89 steps/sec)\n",
      "INFO:tensorflow:global step 5500: loss = 0.0187835693359375 (3280.34 steps/sec)\n",
      "INFO:tensorflow:global step 5600: loss = 0.007122039794921875 (3284.55 steps/sec)\n",
      "INFO:tensorflow:global step 5700: loss = 0.0154571533203125 (3288.62 steps/sec)\n",
      "INFO:tensorflow:global step 5800: loss = 0.03143310546875 (3293.51 steps/sec)\n",
      "INFO:tensorflow:global step 5900: loss = 0.0033397674560546875 (3297.66 steps/sec)\n",
      "INFO:tensorflow:global step 6000: loss = 0.006473541259765625 (3302.35 steps/sec)\n",
      "INFO:tensorflow:global step 6100: loss = 0.006320953369140625 (3305.58 steps/sec)\n",
      "INFO:tensorflow:global step 6200: loss = 0.00164031982421875 (3309.72 steps/sec)\n",
      "INFO:tensorflow:global step 6300: loss = 0.0027027130126953125 (3313.4 steps/sec)\n",
      "INFO:tensorflow:global step 6400: loss = 0.0196990966796875 (3317.24 steps/sec)\n",
      "INFO:tensorflow:global step 6500: loss = 0.002079010009765625 (3320.76 steps/sec)\n",
      "INFO:tensorflow:global step 6600: loss = 0.0211639404296875 (3324.11 steps/sec)\n",
      "INFO:tensorflow:global step 6700: loss = 0.00959014892578125 (3327.21 steps/sec)\n",
      "INFO:tensorflow:global step 6800: loss = 0.0184783935546875 (3330.88 steps/sec)\n",
      "INFO:tensorflow:global step 6900: loss = 0.006061553955078125 (3333.79 steps/sec)\n",
      "INFO:tensorflow:global step 7000: loss = 0.026123046875 (3337.23 steps/sec)\n",
      "INFO:tensorflow:global step 7100: loss = 0.0045318603515625 (3339.88 steps/sec)\n",
      "INFO:tensorflow:global step 7200: loss = 0.007110595703125 (3343.5 steps/sec)\n",
      "INFO:tensorflow:global step 7300: loss = 0.016082763671875 (3346.44 steps/sec)\n",
      "INFO:tensorflow:global step 7400: loss = 0.01050567626953125 (3349.98 steps/sec)\n",
      "INFO:tensorflow:global step 7500: loss = 0.00463104248046875 (3352.63 steps/sec)\n",
      "INFO:tensorflow:global step 7600: loss = 0.000629425048828125 (3355.25 steps/sec)\n",
      "INFO:tensorflow:global step 7700: loss = 0.0202789306640625 (3357.89 steps/sec)\n",
      "INFO:tensorflow:global step 7800: loss = 0.00801849365234375 (3361.05 steps/sec)\n",
      "INFO:tensorflow:global step 7900: loss = 0.008392333984375 (3363.61 steps/sec)\n",
      "INFO:tensorflow:global step 8000: loss = 0.0218658447265625 (3366.74 steps/sec)\n",
      "INFO:tensorflow:global step 8100: loss = 0.0022144317626953125 (3368.64 steps/sec)\n",
      "INFO:tensorflow:global step 8200: loss = 0.00274658203125 (3371.34 steps/sec)\n",
      "INFO:tensorflow:global step 8300: loss = 0.0004978179931640625 (3373.36 steps/sec)\n",
      "INFO:tensorflow:global step 8400: loss = 0.00543212890625 (3375.25 steps/sec)\n",
      "INFO:tensorflow:global step 8500: loss = 0.00023818016052246094 (3377.57 steps/sec)\n",
      "INFO:tensorflow:global step 8600: loss = 0.004993438720703125 (3378.06 steps/sec)\n",
      "INFO:tensorflow:global step 8700: loss = 0.022369384765625 (3379.76 steps/sec)\n",
      "INFO:tensorflow:global step 8800: loss = 0.01287841796875 (3381.03 steps/sec)\n",
      "INFO:tensorflow:global step 8900: loss = 0.005901336669921875 (3383.16 steps/sec)\n",
      "INFO:tensorflow:global step 9000: loss = 0.0203094482421875 (3384.99 steps/sec)\n",
      "INFO:tensorflow:global step 9100: loss = 0.0087127685546875 (3386.9 steps/sec)\n",
      "INFO:tensorflow:global step 9200: loss = 0.0031223297119140625 (3387.78 steps/sec)\n",
      "INFO:tensorflow:global step 9300: loss = 0.0079498291015625 (3389.59 steps/sec)\n",
      "INFO:tensorflow:global step 9400: loss = 0.006504058837890625 (3390.93 steps/sec)\n",
      "INFO:tensorflow:global step 9500: loss = 0.021240234375 (3392.88 steps/sec)\n",
      "INFO:tensorflow:global step 9600: loss = 0.0009527206420898438 (3394.07 steps/sec)\n",
      "INFO:tensorflow:global step 9700: loss = 0.0014553070068359375 (3395.86 steps/sec)\n",
      "INFO:tensorflow:global step 9800: loss = 0.00017821788787841797 (3397.29 steps/sec)\n",
      "INFO:tensorflow:global step 9900: loss = 0.00522613525390625 (3399.09 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10000...\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10000...\n",
      "INFO:tensorflow:global step 10000: loss = 0.021331787109375 (2215.88 steps/sec)\n",
      "INFO:tensorflow:global step 10100: loss = 0.00730133056640625 (2223.64 steps/sec)\n",
      "INFO:tensorflow:global step 10200: loss = 0.0005006790161132812 (2231.59 steps/sec)\n",
      "INFO:tensorflow:global step 10300: loss = 0.01152801513671875 (2239.81 steps/sec)\n",
      "INFO:tensorflow:global step 10400: loss = 0.0010738372802734375 (2247.7 steps/sec)\n",
      "INFO:tensorflow:global step 10500: loss = 0.00942230224609375 (2255.86 steps/sec)\n",
      "INFO:tensorflow:global step 10600: loss = 0.0007548332214355469 (2263.44 steps/sec)\n",
      "INFO:tensorflow:global step 10700: loss = 0.007633209228515625 (2271.3 steps/sec)\n",
      "INFO:tensorflow:global step 10800: loss = 0.018035888671875 (2278.84 steps/sec)\n",
      "INFO:tensorflow:global step 10900: loss = 0.0012264251708984375 (2286.62 steps/sec)\n",
      "INFO:tensorflow:global step 11000: loss = 0.00153350830078125 (2294.02 steps/sec)\n",
      "INFO:tensorflow:global step 11100: loss = 0.018890380859375 (2301.46 steps/sec)\n",
      "INFO:tensorflow:global step 11200: loss = 0.00019371509552001953 (2308.72 steps/sec)\n",
      "INFO:tensorflow:global step 11300: loss = 0.0069732666015625 (2316.05 steps/sec)\n",
      "INFO:tensorflow:global step 11400: loss = 0.002513885498046875 (2323.13 steps/sec)\n",
      "INFO:tensorflow:global step 11500: loss = 0.0240325927734375 (2330.34 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 11600: loss = 0.006496429443359375 (2337.17 steps/sec)\n",
      "INFO:tensorflow:global step 11700: loss = 0.0002703666687011719 (2344.16 steps/sec)\n",
      "INFO:tensorflow:global step 11800: loss = 0.0179290771484375 (2350.86 steps/sec)\n",
      "INFO:tensorflow:global step 11900: loss = 0.003566741943359375 (2357.75 steps/sec)\n",
      "INFO:tensorflow:global step 12000: loss = 0.010162353515625 (2364.39 steps/sec)\n",
      "INFO:tensorflow:global step 12100: loss = 0.01299285888671875 (2370.74 steps/sec)\n",
      "INFO:tensorflow:global step 12200: loss = 0.0006418228149414062 (2377.15 steps/sec)\n",
      "INFO:tensorflow:global step 12300: loss = 0.0195159912109375 (2383.43 steps/sec)\n",
      "INFO:tensorflow:global step 12400: loss = 0.0037212371826171875 (2389.58 steps/sec)\n",
      "INFO:tensorflow:global step 12500: loss = 0.00751495361328125 (2395.76 steps/sec)\n",
      "INFO:tensorflow:global step 12600: loss = 0.0036296844482421875 (2401.9 steps/sec)\n",
      "INFO:tensorflow:global step 12700: loss = 0.0011892318725585938 (2407.96 steps/sec)\n",
      "INFO:tensorflow:global step 12800: loss = 0.0024166107177734375 (2414.13 steps/sec)\n",
      "INFO:tensorflow:global step 12900: loss = 0.04083251953125 (2419.95 steps/sec)\n",
      "INFO:tensorflow:global step 13000: loss = 0.0009870529174804688 (2425.97 steps/sec)\n",
      "INFO:tensorflow:global step 13100: loss = 0.0182647705078125 (2431.43 steps/sec)\n",
      "INFO:tensorflow:global step 13200: loss = 0.054718017578125 (2437.38 steps/sec)\n",
      "INFO:tensorflow:global step 13300: loss = 0.0011034011840820312 (2443.06 steps/sec)\n",
      "INFO:tensorflow:global step 13400: loss = 0.002765655517578125 (2448.77 steps/sec)\n",
      "INFO:tensorflow:global step 13500: loss = 0.00013077259063720703 (2454.22 steps/sec)\n",
      "INFO:tensorflow:global step 13600: loss = 0.01216888427734375 (2459.78 steps/sec)\n",
      "INFO:tensorflow:global step 13700: loss = 0.00193023681640625 (2465.29 steps/sec)\n",
      "INFO:tensorflow:global step 13800: loss = 0.0255584716796875 (2470.85 steps/sec)\n",
      "INFO:tensorflow:global step 13900: loss = 0.0001672506332397461 (2476.09 steps/sec)\n",
      "INFO:tensorflow:global step 14000: loss = 0.0019311904907226562 (2481.61 steps/sec)\n",
      "INFO:tensorflow:global step 14100: loss = 0.002750396728515625 (2486.75 steps/sec)\n",
      "INFO:tensorflow:global step 14200: loss = 0.0003814697265625 (2492.1 steps/sec)\n",
      "INFO:tensorflow:global step 14300: loss = 0.0011882781982421875 (2497.23 steps/sec)\n",
      "INFO:tensorflow:global step 14400: loss = 0.0019121170043945312 (2502.6 steps/sec)\n",
      "INFO:tensorflow:global step 14500: loss = 0.003704071044921875 (2507.65 steps/sec)\n",
      "INFO:tensorflow:global step 14600: loss = 0.0193939208984375 (2512.7 steps/sec)\n",
      "INFO:tensorflow:global step 14700: loss = 0.00862884521484375 (2517.7 steps/sec)\n",
      "INFO:tensorflow:global step 14800: loss = 0.011444091796875 (2522.8 steps/sec)\n",
      "INFO:tensorflow:global step 14900: loss = 0.00014925003051757812 (2527.71 steps/sec)\n",
      "INFO:tensorflow:global step 15000: loss = 0.0694580078125 (2532.71 steps/sec)\n",
      "INFO:tensorflow:global step 15100: loss = 0.0009870529174804688 (2537.5 steps/sec)\n",
      "INFO:tensorflow:global step 15200: loss = 9.316205978393555e-05 (2542.42 steps/sec)\n",
      "INFO:tensorflow:global step 15300: loss = 4.094839096069336e-05 (2547.09 steps/sec)\n",
      "INFO:tensorflow:global step 15400: loss = 9.268522262573242e-05 (2551.95 steps/sec)\n",
      "INFO:tensorflow:global step 15500: loss = 0.00830078125 (2556.68 steps/sec)\n",
      "INFO:tensorflow:global step 15600: loss = 3.9517879486083984e-05 (2561.24 steps/sec)\n",
      "INFO:tensorflow:global step 15700: loss = 0.005096435546875 (2565.71 steps/sec)\n",
      "INFO:tensorflow:global step 15800: loss = 4.792213439941406e-05 (2570.37 steps/sec)\n",
      "INFO:tensorflow:global step 15900: loss = 0.00017595291137695312 (2574.91 steps/sec)\n",
      "INFO:tensorflow:global step 16000: loss = 0.0026683807373046875 (2579.58 steps/sec)\n",
      "INFO:tensorflow:global step 16100: loss = 0.0195159912109375 (2583.87 steps/sec)\n",
      "INFO:tensorflow:global step 16200: loss = 0.00021946430206298828 (2588.45 steps/sec)\n",
      "INFO:tensorflow:global step 16300: loss = 0.0231170654296875 (2592.84 steps/sec)\n",
      "INFO:tensorflow:global step 16400: loss = 0.0009751319885253906 (2597.29 steps/sec)\n",
      "INFO:tensorflow:global step 16500: loss = 0.0013608932495117188 (2601.56 steps/sec)\n",
      "INFO:tensorflow:global step 16600: loss = 0.00202178955078125 (2605.77 steps/sec)\n",
      "INFO:tensorflow:global step 16700: loss = 0.003265380859375 (2609.96 steps/sec)\n",
      "INFO:tensorflow:global step 16800: loss = 0.0005946159362792969 (2614.14 steps/sec)\n",
      "INFO:tensorflow:global step 16900: loss = 0.00962066650390625 (2618.44 steps/sec)\n",
      "INFO:tensorflow:global step 17000: loss = 6.115436553955078e-05 (2622.53 steps/sec)\n",
      "INFO:tensorflow:global step 17100: loss = 0.0022258758544921875 (2626.58 steps/sec)\n",
      "INFO:tensorflow:global step 17200: loss = 0.00499725341796875 (2630.57 steps/sec)\n",
      "INFO:tensorflow:global step 17300: loss = 0.0060882568359375 (2634.66 steps/sec)\n",
      "INFO:tensorflow:global step 17400: loss = 1.5437602996826172e-05 (2638.53 steps/sec)\n",
      "INFO:tensorflow:global step 17500: loss = 0.00012189149856567383 (2642.6 steps/sec)\n",
      "INFO:tensorflow:global step 17600: loss = 0.0009379386901855469 (2646.3 steps/sec)\n",
      "INFO:tensorflow:global step 17700: loss = 0.0002760887145996094 (2650.24 steps/sec)\n",
      "INFO:tensorflow:global step 17800: loss = 2.3603439331054688e-05 (2653.72 steps/sec)\n",
      "INFO:tensorflow:global step 17900: loss = 0.00041866302490234375 (2657.52 steps/sec)\n",
      "INFO:tensorflow:global step 18000: loss = 0.0017242431640625 (2661.0 steps/sec)\n",
      "INFO:tensorflow:global step 18100: loss = 0.0008020401000976562 (2664.69 steps/sec)\n",
      "INFO:tensorflow:global step 18200: loss = 1.0967254638671875e-05 (2668.28 steps/sec)\n",
      "INFO:tensorflow:global step 18300: loss = 0.003940582275390625 (2671.92 steps/sec)\n",
      "INFO:tensorflow:global step 18400: loss = 0.0001964569091796875 (2675.5 steps/sec)\n",
      "INFO:tensorflow:global step 18500: loss = 7.915496826171875e-05 (2679.22 steps/sec)\n",
      "INFO:tensorflow:global step 18600: loss = 0.002353668212890625 (2682.58 steps/sec)\n",
      "INFO:tensorflow:global step 18700: loss = 0.0002396106719970703 (2686.18 steps/sec)\n",
      "INFO:tensorflow:global step 18800: loss = 0.002124786376953125 (2689.6 steps/sec)\n",
      "INFO:tensorflow:global step 18900: loss = 0.0004177093505859375 (2693.14 steps/sec)\n",
      "INFO:tensorflow:global step 19000: loss = 0.005401611328125 (2696.5 steps/sec)\n",
      "INFO:tensorflow:global step 19100: loss = 0.0057220458984375 (2699.87 steps/sec)\n",
      "INFO:tensorflow:global step 19200: loss = 0.000743865966796875 (2703.28 steps/sec)\n",
      "INFO:tensorflow:global step 19300: loss = 0.0052947998046875 (2706.82 steps/sec)\n",
      "INFO:tensorflow:global step 19400: loss = 3.4868717193603516e-05 (2710.23 steps/sec)\n",
      "INFO:tensorflow:global step 19500: loss = 0.00138092041015625 (2713.69 steps/sec)\n",
      "INFO:tensorflow:global step 19600: loss = 0.0006861686706542969 (2716.79 steps/sec)\n",
      "INFO:tensorflow:global step 19700: loss = 1.4662742614746094e-05 (2720.26 steps/sec)\n",
      "INFO:tensorflow:global step 19800: loss = 0.0015888214111328125 (2723.57 steps/sec)\n",
      "INFO:tensorflow:global step 19900: loss = 0.0026683807373046875 (2726.99 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 20000...\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into tutorial_model_dir/model.ckpt.\n",
      "WARNING:tensorflow:From /cbcore/python/python-x86_64/lib/python3.7/site-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 20000...\n",
      "INFO:tensorflow:global step 20000: loss = 0.00028252601623535156 (1834.8 steps/sec)\n",
      "INFO:tensorflow:global step 20100: loss = 0.00011211633682250977 (1839.01 steps/sec)\n",
      "INFO:tensorflow:global step 20200: loss = 0.0009288787841796875 (1843.31 steps/sec)\n",
      "INFO:tensorflow:global step 20300: loss = 0.0052490234375 (1847.73 steps/sec)\n",
      "INFO:tensorflow:global step 20400: loss = 0.0002961158752441406 (1852.05 steps/sec)\n",
      "INFO:tensorflow:global step 20500: loss = 0.01300048828125 (1856.44 steps/sec)\n",
      "INFO:tensorflow:global step 20600: loss = 0.013885498046875 (1860.7 steps/sec)\n",
      "INFO:tensorflow:global step 20700: loss = 0.0014495849609375 (1865.04 steps/sec)\n",
      "INFO:tensorflow:global step 20800: loss = 0.00019693374633789062 (1869.26 steps/sec)\n",
      "INFO:tensorflow:global step 20900: loss = 0.006072998046875 (1873.48 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 21000: loss = 0.001911163330078125 (1877.73 steps/sec)\n",
      "INFO:tensorflow:global step 21100: loss = 0.004123687744140625 (1881.86 steps/sec)\n",
      "INFO:tensorflow:global step 21200: loss = 0.00033092498779296875 (1886.09 steps/sec)\n",
      "INFO:tensorflow:global step 21300: loss = 0.0007901191711425781 (1890.21 steps/sec)\n",
      "INFO:tensorflow:global step 21400: loss = 0.0011157989501953125 (1894.36 steps/sec)\n",
      "INFO:tensorflow:global step 21500: loss = 0.000583648681640625 (1898.45 steps/sec)\n",
      "INFO:tensorflow:global step 21600: loss = 4.6133995056152344e-05 (1902.53 steps/sec)\n",
      "INFO:tensorflow:global step 21700: loss = 0.0002880096435546875 (1906.57 steps/sec)\n",
      "INFO:tensorflow:global step 21800: loss = 3.993511199951172e-05 (1910.67 steps/sec)\n",
      "INFO:tensorflow:global step 21900: loss = 0.00555419921875 (1914.7 steps/sec)\n",
      "INFO:tensorflow:global step 22000: loss = 0.01042938232421875 (1918.77 steps/sec)\n",
      "INFO:tensorflow:global step 22100: loss = 0.0073699951171875 (1922.68 steps/sec)\n",
      "INFO:tensorflow:global step 22200: loss = 0.0035858154296875 (1926.67 steps/sec)\n",
      "INFO:tensorflow:global step 22300: loss = 0.031982421875 (1930.39 steps/sec)\n",
      "INFO:tensorflow:global step 22400: loss = 0.00014448165893554688 (1934.37 steps/sec)\n",
      "INFO:tensorflow:global step 22500: loss = 0.00531768798828125 (1938.26 steps/sec)\n",
      "INFO:tensorflow:global step 22600: loss = 0.0001461505889892578 (1942.13 steps/sec)\n",
      "INFO:tensorflow:global step 22700: loss = 8.541345596313477e-05 (1945.98 steps/sec)\n",
      "INFO:tensorflow:global step 22800: loss = 0.0011625289916992188 (1949.91 steps/sec)\n",
      "INFO:tensorflow:global step 22900: loss = 0.0023899078369140625 (1953.73 steps/sec)\n",
      "INFO:tensorflow:global step 23000: loss = 0.000629425048828125 (1957.61 steps/sec)\n",
      "INFO:tensorflow:global step 23100: loss = 0.004001617431640625 (1961.29 steps/sec)\n",
      "INFO:tensorflow:global step 23200: loss = 0.0030460357666015625 (1965.14 steps/sec)\n",
      "INFO:tensorflow:global step 23300: loss = 1.895427703857422e-05 (1968.89 steps/sec)\n",
      "INFO:tensorflow:global step 23400: loss = 1.615285873413086e-05 (1972.7 steps/sec)\n",
      "INFO:tensorflow:global step 23500: loss = 3.319978713989258e-05 (1976.43 steps/sec)\n",
      "INFO:tensorflow:global step 23600: loss = 0.004230499267578125 (1980.14 steps/sec)\n",
      "INFO:tensorflow:global step 23700: loss = 0.0012273788452148438 (1983.81 steps/sec)\n",
      "INFO:tensorflow:global step 23800: loss = 0.00021457672119140625 (1987.53 steps/sec)\n",
      "INFO:tensorflow:global step 23900: loss = 0.0006070137023925781 (1991.16 steps/sec)\n",
      "INFO:tensorflow:global step 24000: loss = 0.0025081634521484375 (1994.86 steps/sec)\n",
      "INFO:tensorflow:global step 24100: loss = 0.00012493133544921875 (1998.46 steps/sec)\n",
      "INFO:tensorflow:global step 24200: loss = 6.496906280517578e-06 (2002.13 steps/sec)\n",
      "INFO:tensorflow:global step 24300: loss = 0.003631591796875 (2005.71 steps/sec)\n",
      "INFO:tensorflow:global step 24400: loss = 0.0016651153564453125 (2009.35 steps/sec)\n",
      "INFO:tensorflow:global step 24500: loss = 0.0018377304077148438 (2012.91 steps/sec)\n",
      "INFO:tensorflow:global step 24600: loss = 3.528594970703125e-05 (2016.45 steps/sec)\n",
      "INFO:tensorflow:global step 24700: loss = 0.01297760009765625 (2019.99 steps/sec)\n",
      "INFO:tensorflow:global step 24800: loss = 0.0021800994873046875 (2023.58 steps/sec)\n",
      "INFO:tensorflow:global step 24900: loss = 0.0011167526245117188 (2027.07 steps/sec)\n",
      "INFO:tensorflow:global step 25000: loss = 3.6597251892089844e-05 (2030.55 steps/sec)\n",
      "INFO:tensorflow:global step 25100: loss = 5.7578086853027344e-05 (2033.97 steps/sec)\n",
      "INFO:tensorflow:global step 25200: loss = 0.00014972686767578125 (2037.44 steps/sec)\n",
      "INFO:tensorflow:global step 25300: loss = 0.004261016845703125 (2040.93 steps/sec)\n",
      "INFO:tensorflow:global step 25400: loss = 0.0011749267578125 (2044.33 steps/sec)\n",
      "INFO:tensorflow:global step 25500: loss = 0.0154876708984375 (2047.72 steps/sec)\n",
      "INFO:tensorflow:global step 25600: loss = 1.5914440155029297e-05 (2051.0 steps/sec)\n",
      "INFO:tensorflow:global step 25700: loss = 0.001316070556640625 (2054.4 steps/sec)\n",
      "INFO:tensorflow:global step 25800: loss = 0.0013456344604492188 (2057.74 steps/sec)\n",
      "INFO:tensorflow:global step 25900: loss = 5.429983139038086e-05 (2061.12 steps/sec)\n",
      "INFO:tensorflow:global step 26000: loss = 9.495019912719727e-05 (2064.44 steps/sec)\n",
      "INFO:tensorflow:global step 26100: loss = 0.00032520294189453125 (2067.78 steps/sec)\n",
      "INFO:tensorflow:global step 26200: loss = 7.152557373046875e-06 (2071.08 steps/sec)\n",
      "INFO:tensorflow:global step 26300: loss = 8.803606033325195e-05 (2074.41 steps/sec)\n",
      "INFO:tensorflow:global step 26400: loss = 0.002056121826171875 (2077.68 steps/sec)\n",
      "INFO:tensorflow:global step 26500: loss = 0.023956298828125 (2081.01 steps/sec)\n",
      "INFO:tensorflow:global step 26600: loss = 5.5849552154541016e-05 (2084.19 steps/sec)\n",
      "INFO:tensorflow:global step 26700: loss = 0.003406524658203125 (2087.47 steps/sec)\n",
      "INFO:tensorflow:global step 26800: loss = 0.027099609375 (2090.68 steps/sec)\n",
      "INFO:tensorflow:global step 26900: loss = 0.0689697265625 (2093.95 steps/sec)\n",
      "INFO:tensorflow:global step 27000: loss = 0.0001342296600341797 (2097.03 steps/sec)\n",
      "INFO:tensorflow:global step 27100: loss = 0.0001380443572998047 (2100.16 steps/sec)\n",
      "INFO:tensorflow:global step 27200: loss = 1.5079975128173828e-05 (2103.28 steps/sec)\n",
      "INFO:tensorflow:global step 27300: loss = 0.0006699562072753906 (2106.45 steps/sec)\n",
      "INFO:tensorflow:global step 27400: loss = 0.0010023117065429688 (2109.54 steps/sec)\n",
      "INFO:tensorflow:global step 27500: loss = 6.324052810668945e-05 (2112.65 steps/sec)\n",
      "INFO:tensorflow:global step 27600: loss = 0.0020847320556640625 (2115.62 steps/sec)\n",
      "INFO:tensorflow:global step 27700: loss = 0.055999755859375 (2118.74 steps/sec)\n",
      "INFO:tensorflow:global step 27800: loss = 0.001499176025390625 (2121.8 steps/sec)\n",
      "INFO:tensorflow:global step 27900: loss = 5.9604644775390625e-05 (2124.85 steps/sec)\n",
      "INFO:tensorflow:global step 28000: loss = 0.0022869110107421875 (2127.88 steps/sec)\n",
      "INFO:tensorflow:global step 28100: loss = 0.05364990234375 (2130.84 steps/sec)\n",
      "INFO:tensorflow:global step 28200: loss = 0.0007104873657226562 (2133.86 steps/sec)\n",
      "INFO:tensorflow:global step 28300: loss = 0.0011548995971679688 (2136.92 steps/sec)\n",
      "INFO:tensorflow:global step 28400: loss = 0.00022339820861816406 (2139.91 steps/sec)\n",
      "INFO:tensorflow:global step 28500: loss = 0.0023822784423828125 (2142.95 steps/sec)\n",
      "INFO:tensorflow:global step 28600: loss = 0.0005826950073242188 (2145.87 steps/sec)\n",
      "INFO:tensorflow:global step 28700: loss = 0.0018529891967773438 (2148.9 steps/sec)\n",
      "INFO:tensorflow:global step 28800: loss = 0.00021505355834960938 (2151.83 steps/sec)\n",
      "INFO:tensorflow:global step 28900: loss = 3.635883331298828e-06 (2154.83 steps/sec)\n",
      "INFO:tensorflow:global step 29000: loss = 0.002635955810546875 (2157.75 steps/sec)\n",
      "INFO:tensorflow:global step 29100: loss = 0.00015747547149658203 (2160.67 steps/sec)\n",
      "INFO:tensorflow:global step 29200: loss = 0.00012993812561035156 (2163.57 steps/sec)\n",
      "INFO:tensorflow:global step 29300: loss = 0.00839996337890625 (2166.45 steps/sec)\n",
      "INFO:tensorflow:global step 29400: loss = 4.100799560546875e-05 (2169.4 steps/sec)\n",
      "INFO:tensorflow:global step 29500: loss = 9.781122207641602e-05 (2172.22 steps/sec)\n",
      "INFO:tensorflow:global step 29600: loss = 0.0009036064147949219 (2175.02 steps/sec)\n",
      "INFO:tensorflow:global step 29700: loss = 0.003192901611328125 (2177.86 steps/sec)\n",
      "INFO:tensorflow:global step 29800: loss = 0.001239776611328125 (2180.75 steps/sec)\n",
      "INFO:tensorflow:global step 29900: loss = 0.007602691650390625 (2183.57 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 30000...\n",
      "INFO:tensorflow:Saving checkpoints for 30000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 30000...\n",
      "INFO:tensorflow:global step 30000: loss = 0.003780364990234375 (2172.3 steps/sec)\n",
      "INFO:tensorflow:global step 30100: loss = 0.0002856254577636719 (2174.82 steps/sec)\n",
      "INFO:tensorflow:global step 30200: loss = 3.230571746826172e-05 (2177.64 steps/sec)\n",
      "INFO:tensorflow:global step 30300: loss = 4.649162292480469e-06 (2180.38 steps/sec)\n",
      "INFO:tensorflow:global step 30400: loss = 0.00042724609375 (2183.2 steps/sec)\n",
      "INFO:tensorflow:global step 30500: loss = 0.0008001327514648438 (2185.88 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 30600: loss = 0.0004100799560546875 (2188.63 steps/sec)\n",
      "INFO:tensorflow:global step 30700: loss = 0.00788116455078125 (2191.34 steps/sec)\n",
      "INFO:tensorflow:global step 30800: loss = 2.384185791015625e-06 (2194.11 steps/sec)\n",
      "INFO:tensorflow:global step 30900: loss = 0.0003185272216796875 (2196.81 steps/sec)\n",
      "INFO:tensorflow:global step 31000: loss = 7.3909759521484375e-06 (2199.58 steps/sec)\n",
      "INFO:tensorflow:global step 31100: loss = 0.026153564453125 (2202.15 steps/sec)\n",
      "INFO:tensorflow:global step 31200: loss = 0.0054931640625 (2204.86 steps/sec)\n",
      "INFO:tensorflow:global step 31300: loss = 0.00928497314453125 (2207.52 steps/sec)\n",
      "INFO:tensorflow:global step 31400: loss = 0.0001399517059326172 (2210.24 steps/sec)\n",
      "INFO:tensorflow:global step 31500: loss = 0.0077056884765625 (2212.89 steps/sec)\n",
      "INFO:tensorflow:global step 31600: loss = 5.960464477539062e-07 (2215.55 steps/sec)\n",
      "INFO:tensorflow:global step 31700: loss = 0.003238677978515625 (2218.19 steps/sec)\n",
      "INFO:tensorflow:global step 31800: loss = 0.00107574462890625 (2220.87 steps/sec)\n",
      "INFO:tensorflow:global step 31900: loss = 0.0128021240234375 (2223.46 steps/sec)\n",
      "INFO:tensorflow:global step 32000: loss = 0.00804901123046875 (2226.13 steps/sec)\n",
      "INFO:tensorflow:global step 32100: loss = 0.00015997886657714844 (2228.61 steps/sec)\n",
      "INFO:tensorflow:global step 32200: loss = 0.01152801513671875 (2231.27 steps/sec)\n",
      "INFO:tensorflow:global step 32300: loss = 0.00234222412109375 (2233.84 steps/sec)\n",
      "INFO:tensorflow:global step 32400: loss = 0.00139617919921875 (2236.45 steps/sec)\n",
      "INFO:tensorflow:global step 32500: loss = 0.0014715194702148438 (2238.99 steps/sec)\n",
      "INFO:tensorflow:global step 32600: loss = 0.000244140625 (2241.53 steps/sec)\n",
      "INFO:tensorflow:global step 32700: loss = 0.0003314018249511719 (2244.07 steps/sec)\n",
      "INFO:tensorflow:global step 32800: loss = 0.007305145263671875 (2246.65 steps/sec)\n",
      "INFO:tensorflow:global step 32900: loss = 0.0001308917999267578 (2249.16 steps/sec)\n",
      "INFO:tensorflow:global step 33000: loss = 0.00037479400634765625 (2251.69 steps/sec)\n",
      "INFO:tensorflow:global step 33100: loss = 1.4483928680419922e-05 (2254.09 steps/sec)\n",
      "INFO:tensorflow:global step 33200: loss = 0.00016820430755615234 (2256.62 steps/sec)\n",
      "INFO:tensorflow:global step 33300: loss = 0.007110595703125 (2259.08 steps/sec)\n",
      "INFO:tensorflow:global step 33400: loss = 1.150369644165039e-05 (2261.49 steps/sec)\n",
      "INFO:tensorflow:global step 33500: loss = 1.2814998626708984e-05 (2263.97 steps/sec)\n",
      "INFO:tensorflow:global step 33600: loss = 0.01092529296875 (2266.33 steps/sec)\n",
      "INFO:tensorflow:global step 33700: loss = 0.0018091201782226562 (2268.81 steps/sec)\n",
      "INFO:tensorflow:global step 33800: loss = 0.00031757354736328125 (2271.24 steps/sec)\n",
      "INFO:tensorflow:global step 33900: loss = 0.0015773773193359375 (2273.69 steps/sec)\n",
      "INFO:tensorflow:global step 34000: loss = 0.0024623870849609375 (2276.09 steps/sec)\n",
      "INFO:tensorflow:global step 34100: loss = 0.0006866455078125 (2278.48 steps/sec)\n",
      "INFO:tensorflow:global step 34200: loss = 0.00016069412231445312 (2280.82 steps/sec)\n",
      "INFO:tensorflow:global step 34300: loss = 0.007080078125 (2283.13 steps/sec)\n",
      "INFO:tensorflow:global step 34400: loss = 0.003482818603515625 (2285.43 steps/sec)\n",
      "INFO:tensorflow:global step 34500: loss = 1.8417835235595703e-05 (2287.77 steps/sec)\n",
      "INFO:tensorflow:global step 34600: loss = 3.5762786865234375e-07 (2290.04 steps/sec)\n",
      "INFO:tensorflow:global step 34700: loss = 0.0010690689086914062 (2292.44 steps/sec)\n",
      "INFO:tensorflow:global step 34800: loss = 0.0010404586791992188 (2294.75 steps/sec)\n",
      "INFO:tensorflow:global step 34900: loss = 0.00014650821685791016 (2297.15 steps/sec)\n",
      "INFO:tensorflow:global step 35000: loss = 0.0011205673217773438 (2299.48 steps/sec)\n",
      "INFO:tensorflow:global step 35100: loss = 6.145238876342773e-05 (2301.82 steps/sec)\n",
      "INFO:tensorflow:global step 35200: loss = 0.003261566162109375 (2304.11 steps/sec)\n",
      "INFO:tensorflow:global step 35300: loss = 1.043081283569336e-05 (2306.46 steps/sec)\n",
      "INFO:tensorflow:global step 35400: loss = 0.003208160400390625 (2308.74 steps/sec)\n",
      "INFO:tensorflow:global step 35500: loss = 0.00022864341735839844 (2311.07 steps/sec)\n",
      "INFO:tensorflow:global step 35600: loss = 0.00014448165893554688 (2313.3 steps/sec)\n",
      "INFO:tensorflow:global step 35700: loss = 0.0106658935546875 (2315.61 steps/sec)\n",
      "INFO:tensorflow:global step 35800: loss = 0.00013566017150878906 (2317.88 steps/sec)\n",
      "INFO:tensorflow:global step 35900: loss = 0.0036067962646484375 (2320.19 steps/sec)\n",
      "INFO:tensorflow:global step 36000: loss = 0.0011243820190429688 (2322.43 steps/sec)\n",
      "INFO:tensorflow:global step 36100: loss = 0.0011043548583984375 (2324.66 steps/sec)\n",
      "INFO:tensorflow:global step 36200: loss = 0.0212249755859375 (2326.88 steps/sec)\n",
      "INFO:tensorflow:global step 36300: loss = 7.50422477722168e-05 (2329.17 steps/sec)\n",
      "INFO:tensorflow:global step 36400: loss = 1.5079975128173828e-05 (2331.38 steps/sec)\n",
      "INFO:tensorflow:global step 36500: loss = 7.992982864379883e-05 (2333.66 steps/sec)\n",
      "INFO:tensorflow:global step 36600: loss = 5.990266799926758e-05 (2335.81 steps/sec)\n",
      "INFO:tensorflow:global step 36700: loss = 7.450580596923828e-06 (2338.06 steps/sec)\n",
      "INFO:tensorflow:global step 36800: loss = 0.0003781318664550781 (2340.25 steps/sec)\n",
      "INFO:tensorflow:global step 36900: loss = 0.0033969879150390625 (2342.49 steps/sec)\n",
      "INFO:tensorflow:global step 37000: loss = 0.022216796875 (2344.67 steps/sec)\n",
      "INFO:tensorflow:global step 37100: loss = 0.0061187744140625 (2346.82 steps/sec)\n",
      "INFO:tensorflow:global step 37200: loss = 0.006427764892578125 (2348.97 steps/sec)\n",
      "INFO:tensorflow:global step 37300: loss = 9.733438491821289e-05 (2351.16 steps/sec)\n",
      "INFO:tensorflow:global step 37400: loss = 0.0013437271118164062 (2353.3 steps/sec)\n",
      "INFO:tensorflow:global step 37500: loss = 0.0018072128295898438 (2355.44 steps/sec)\n",
      "INFO:tensorflow:global step 37600: loss = 0.0004963874816894531 (2357.58 steps/sec)\n",
      "INFO:tensorflow:global step 37700: loss = 1.0669231414794922e-05 (2359.72 steps/sec)\n",
      "INFO:tensorflow:global step 37800: loss = 8.52346420288086e-06 (2361.87 steps/sec)\n",
      "INFO:tensorflow:global step 37900: loss = 0.00623321533203125 (2363.97 steps/sec)\n",
      "INFO:tensorflow:global step 38000: loss = 8.338689804077148e-05 (2366.11 steps/sec)\n",
      "INFO:tensorflow:global step 38100: loss = 6.014108657836914e-05 (2368.13 steps/sec)\n",
      "INFO:tensorflow:global step 38200: loss = 0.0009183883666992188 (2370.18 steps/sec)\n",
      "INFO:tensorflow:global step 38300: loss = 4.6312808990478516e-05 (2372.26 steps/sec)\n",
      "INFO:tensorflow:global step 38400: loss = 7.867813110351562e-06 (2374.38 steps/sec)\n",
      "INFO:tensorflow:global step 38500: loss = 0.015380859375 (2376.44 steps/sec)\n",
      "INFO:tensorflow:global step 38600: loss = 9.202957153320312e-05 (2378.52 steps/sec)\n",
      "INFO:tensorflow:global step 38700: loss = 3.731250762939453e-05 (2380.58 steps/sec)\n",
      "INFO:tensorflow:global step 38800: loss = 1.1920928955078125e-06 (2382.68 steps/sec)\n",
      "INFO:tensorflow:global step 38900: loss = 5.435943603515625e-05 (2384.72 steps/sec)\n",
      "INFO:tensorflow:global step 39000: loss = 8.761882781982422e-05 (2386.82 steps/sec)\n",
      "INFO:tensorflow:global step 39100: loss = 0.0232391357421875 (2388.81 steps/sec)\n",
      "INFO:tensorflow:global step 39200: loss = 0.004917144775390625 (2390.88 steps/sec)\n",
      "INFO:tensorflow:global step 39300: loss = 0.00032258033752441406 (2392.9 steps/sec)\n",
      "INFO:tensorflow:global step 39400: loss = 0.0011501312255859375 (2394.96 steps/sec)\n",
      "INFO:tensorflow:global step 39500: loss = 0.0005283355712890625 (2396.95 steps/sec)\n",
      "INFO:tensorflow:global step 39600: loss = 0.0240325927734375 (2398.88 steps/sec)\n",
      "INFO:tensorflow:global step 39700: loss = 0.0034942626953125 (2400.83 steps/sec)\n",
      "INFO:tensorflow:global step 39800: loss = 1.4483928680419922e-05 (2402.88 steps/sec)\n",
      "INFO:tensorflow:global step 39900: loss = 0.00012034177780151367 (2321.67 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 40000...\n",
      "INFO:tensorflow:Saving checkpoints for 40000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 40000...\n",
      "INFO:tensorflow:global step 40000: loss = 1.0907649993896484e-05 (2306.95 steps/sec)\n",
      "INFO:tensorflow:global step 40100: loss = 0.0008211135864257812 (2308.71 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 40200: loss = 2.562999725341797e-06 (2310.73 steps/sec)\n",
      "INFO:tensorflow:global step 40300: loss = 4.214048385620117e-05 (2312.7 steps/sec)\n",
      "INFO:tensorflow:global step 40400: loss = 0.0001347064971923828 (2314.74 steps/sec)\n",
      "INFO:tensorflow:global step 40500: loss = 0.00017952919006347656 (2316.7 steps/sec)\n",
      "INFO:tensorflow:global step 40600: loss = 6.592273712158203e-05 (2318.66 steps/sec)\n",
      "INFO:tensorflow:global step 40700: loss = 0.0015697479248046875 (2320.64 steps/sec)\n",
      "INFO:tensorflow:global step 40800: loss = 1.9252300262451172e-05 (2322.63 steps/sec)\n",
      "INFO:tensorflow:global step 40900: loss = 0.03125 (2324.57 steps/sec)\n",
      "INFO:tensorflow:global step 41000: loss = 0.0307159423828125 (2326.58 steps/sec)\n",
      "INFO:tensorflow:global step 41100: loss = 0.004940032958984375 (2328.48 steps/sec)\n",
      "INFO:tensorflow:global step 41200: loss = 1.7285346984863281e-06 (2330.4 steps/sec)\n",
      "INFO:tensorflow:global step 41300: loss = 8.83936882019043e-05 (2332.29 steps/sec)\n",
      "INFO:tensorflow:global step 41400: loss = 0.00032401084899902344 (2334.22 steps/sec)\n",
      "INFO:tensorflow:global step 41500: loss = 0.0003662109375 (2336.1 steps/sec)\n",
      "INFO:tensorflow:global step 41600: loss = 0.0006070137023925781 (2337.99 steps/sec)\n",
      "INFO:tensorflow:global step 41700: loss = 2.384185791015625e-07 (2339.87 steps/sec)\n",
      "INFO:tensorflow:global step 41800: loss = 0.01331329345703125 (2341.74 steps/sec)\n",
      "INFO:tensorflow:global step 41900: loss = 1.3709068298339844e-06 (2343.66 steps/sec)\n",
      "INFO:tensorflow:global step 42000: loss = 0.0234222412109375 (2345.52 steps/sec)\n",
      "INFO:tensorflow:global step 42100: loss = 3.826618194580078e-05 (2347.4 steps/sec)\n",
      "INFO:tensorflow:global step 42200: loss = 0.0021190643310546875 (2349.27 steps/sec)\n",
      "INFO:tensorflow:global step 42300: loss = 2.956390380859375e-05 (2351.16 steps/sec)\n",
      "INFO:tensorflow:global step 42400: loss = 1.800060272216797e-05 (2352.98 steps/sec)\n",
      "INFO:tensorflow:global step 42500: loss = 0.0300750732421875 (2354.9 steps/sec)\n",
      "INFO:tensorflow:global step 42600: loss = 0.0003361701965332031 (2356.72 steps/sec)\n",
      "INFO:tensorflow:global step 42700: loss = 2.1457672119140625e-06 (2358.6 steps/sec)\n",
      "INFO:tensorflow:global step 42800: loss = 2.8312206268310547e-05 (2360.43 steps/sec)\n",
      "INFO:tensorflow:global step 42900: loss = 0.00440216064453125 (2362.29 steps/sec)\n",
      "INFO:tensorflow:global step 43000: loss = 0.0048828125 (2364.11 steps/sec)\n",
      "INFO:tensorflow:global step 43100: loss = 0.00015163421630859375 (2365.92 steps/sec)\n",
      "INFO:tensorflow:global step 43200: loss = 1.5020370483398438e-05 (2367.72 steps/sec)\n",
      "INFO:tensorflow:global step 43300: loss = 0.00012421607971191406 (2369.58 steps/sec)\n",
      "INFO:tensorflow:global step 43400: loss = 9.5367431640625e-07 (2371.39 steps/sec)\n",
      "INFO:tensorflow:global step 43500: loss = 9.78708267211914e-05 (2373.25 steps/sec)\n",
      "INFO:tensorflow:global step 43600: loss = 0.0019092559814453125 (2374.97 steps/sec)\n",
      "INFO:tensorflow:global step 43700: loss = 2.4437904357910156e-05 (2376.78 steps/sec)\n",
      "INFO:tensorflow:global step 43800: loss = 0.003246307373046875 (2378.55 steps/sec)\n",
      "INFO:tensorflow:global step 43900: loss = 0.0003097057342529297 (2380.38 steps/sec)\n",
      "INFO:tensorflow:global step 44000: loss = 0.004520416259765625 (2382.16 steps/sec)\n",
      "INFO:tensorflow:global step 44100: loss = 0.001186370849609375 (2383.89 steps/sec)\n",
      "INFO:tensorflow:global step 44200: loss = 0.00847625732421875 (2385.63 steps/sec)\n",
      "INFO:tensorflow:global step 44300: loss = 8.028745651245117e-05 (2387.43 steps/sec)\n",
      "INFO:tensorflow:global step 44400: loss = 1.5497207641601562e-06 (2389.16 steps/sec)\n",
      "INFO:tensorflow:global step 44500: loss = 0.046173095703125 (2390.95 steps/sec)\n",
      "INFO:tensorflow:global step 44600: loss = 6.794929504394531e-06 (2392.6 steps/sec)\n",
      "INFO:tensorflow:global step 44700: loss = 0.01380157470703125 (2394.38 steps/sec)\n",
      "INFO:tensorflow:global step 44800: loss = 0.0141143798828125 (2396.07 steps/sec)\n",
      "INFO:tensorflow:global step 44900: loss = 8.845329284667969e-05 (2397.81 steps/sec)\n",
      "INFO:tensorflow:global step 45000: loss = 0.0134124755859375 (2399.51 steps/sec)\n",
      "INFO:tensorflow:global step 45100: loss = 0.007503509521484375 (2401.22 steps/sec)\n",
      "INFO:tensorflow:global step 45200: loss = 2.9206275939941406e-06 (2402.92 steps/sec)\n",
      "INFO:tensorflow:global step 45300: loss = 0.0033931732177734375 (2404.66 steps/sec)\n",
      "INFO:tensorflow:global step 45400: loss = 2.980232238769531e-07 (2406.36 steps/sec)\n",
      "INFO:tensorflow:global step 45500: loss = 0.0010423660278320312 (2408.09 steps/sec)\n",
      "INFO:tensorflow:global step 45600: loss = 0.00017702579498291016 (2409.7 steps/sec)\n",
      "INFO:tensorflow:global step 45700: loss = 6.794929504394531e-06 (2411.41 steps/sec)\n",
      "INFO:tensorflow:global step 45800: loss = 0.0285186767578125 (2413.1 steps/sec)\n",
      "INFO:tensorflow:global step 45900: loss = 0.0012302398681640625 (2414.76 steps/sec)\n",
      "INFO:tensorflow:global step 46000: loss = 0.0022945404052734375 (2416.48 steps/sec)\n",
      "INFO:tensorflow:global step 46100: loss = 0.04022216796875 (2418.11 steps/sec)\n",
      "INFO:tensorflow:global step 46200: loss = 0.00010079145431518555 (2419.81 steps/sec)\n",
      "INFO:tensorflow:global step 46300: loss = 0.01971435546875 (2421.47 steps/sec)\n",
      "INFO:tensorflow:global step 46400: loss = 0.0009179115295410156 (2423.16 steps/sec)\n",
      "INFO:tensorflow:global step 46500: loss = 4.172325134277344e-06 (2424.79 steps/sec)\n",
      "INFO:tensorflow:global step 46600: loss = 5.364418029785156e-07 (2426.44 steps/sec)\n",
      "INFO:tensorflow:global step 46700: loss = 2.980232238769531e-07 (2428.08 steps/sec)\n",
      "INFO:tensorflow:global step 46800: loss = 4.947185516357422e-06 (2429.75 steps/sec)\n",
      "INFO:tensorflow:global step 46900: loss = 1.2040138244628906e-05 (2431.34 steps/sec)\n",
      "INFO:tensorflow:global step 47000: loss = 0.00018608570098876953 (2433.0 steps/sec)\n",
      "INFO:tensorflow:global step 47100: loss = 0.0001728534698486328 (2434.58 steps/sec)\n",
      "INFO:tensorflow:global step 47200: loss = 0.066650390625 (2436.23 steps/sec)\n",
      "INFO:tensorflow:global step 47300: loss = 4.744529724121094e-05 (2437.84 steps/sec)\n",
      "INFO:tensorflow:global step 47400: loss = 0.0001704692840576172 (2439.49 steps/sec)\n",
      "INFO:tensorflow:global step 47500: loss = 0.0003094673156738281 (2441.09 steps/sec)\n",
      "INFO:tensorflow:global step 47600: loss = 0.0007915496826171875 (2442.67 steps/sec)\n",
      "INFO:tensorflow:global step 47700: loss = 2.980232238769531e-07 (2444.25 steps/sec)\n",
      "INFO:tensorflow:global step 47800: loss = 0.00023984909057617188 (2445.89 steps/sec)\n",
      "INFO:tensorflow:global step 47900: loss = 0.01100921630859375 (2447.45 steps/sec)\n",
      "INFO:tensorflow:global step 48000: loss = 0.009552001953125 (2449.07 steps/sec)\n",
      "INFO:tensorflow:global step 48100: loss = 1.7821788787841797e-05 (2450.61 steps/sec)\n",
      "INFO:tensorflow:global step 48200: loss = 0.001796722412109375 (2452.22 steps/sec)\n",
      "INFO:tensorflow:global step 48300: loss = 2.6345252990722656e-05 (2453.79 steps/sec)\n",
      "INFO:tensorflow:global step 48400: loss = 0.0049896240234375 (2455.4 steps/sec)\n",
      "INFO:tensorflow:global step 48500: loss = 4.172325134277344e-06 (2456.96 steps/sec)\n",
      "INFO:tensorflow:global step 48600: loss = 8.821487426757812e-06 (2458.51 steps/sec)\n",
      "INFO:tensorflow:global step 48700: loss = 0.003826141357421875 (2460.06 steps/sec)\n",
      "INFO:tensorflow:global step 48800: loss = 4.0531158447265625e-05 (2461.66 steps/sec)\n",
      "INFO:tensorflow:global step 48900: loss = 0.00022983551025390625 (2463.19 steps/sec)\n",
      "INFO:tensorflow:global step 49000: loss = 3.5762786865234375e-07 (2464.74 steps/sec)\n",
      "INFO:tensorflow:global step 49100: loss = 3.933906555175781e-06 (2466.27 steps/sec)\n",
      "INFO:tensorflow:global step 49200: loss = 9.244680404663086e-05 (2467.85 steps/sec)\n",
      "INFO:tensorflow:global step 49300: loss = 4.64320182800293e-05 (2469.37 steps/sec)\n",
      "INFO:tensorflow:global step 49400: loss = 0.041839599609375 (2470.93 steps/sec)\n",
      "INFO:tensorflow:global step 49500: loss = 0.0303955078125 (2472.43 steps/sec)\n",
      "INFO:tensorflow:global step 49600: loss = 5.9604644775390625e-06 (2473.94 steps/sec)\n",
      "INFO:tensorflow:global step 49700: loss = 1.7881393432617188e-06 (2475.44 steps/sec)\n",
      "INFO:tensorflow:global step 49800: loss = 0.000335693359375 (2477.0 steps/sec)\n",
      "INFO:tensorflow:global step 49900: loss = 4.392862319946289e-05 (2478.51 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 50000...\n",
      "INFO:tensorflow:Saving checkpoints for 50000 into tutorial_model_dir/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 50000...\n",
      "INFO:tensorflow:global step 50000: loss = 0.01044464111328125 (2317.7 steps/sec)\n",
      "INFO:tensorflow:global step 50100: loss = 0.0011854171752929688 (2319.26 steps/sec)\n",
      "INFO:tensorflow:global step 50200: loss = 0.0003447532653808594 (2320.84 steps/sec)\n",
      "INFO:tensorflow:global step 50300: loss = 0.0009608268737792969 (2322.46 steps/sec)\n",
      "INFO:tensorflow:global step 50400: loss = 4.506111145019531e-05 (2324.04 steps/sec)\n",
      "INFO:tensorflow:global step 50500: loss = 0.0009326934814453125 (2325.65 steps/sec)\n",
      "INFO:tensorflow:global step 50600: loss = 0.00026226043701171875 (2327.19 steps/sec)\n",
      "INFO:tensorflow:global step 50700: loss = 0.006351470947265625 (2328.79 steps/sec)\n",
      "INFO:tensorflow:global step 50800: loss = 0.0030918121337890625 (2330.33 steps/sec)\n",
      "INFO:tensorflow:global step 50900: loss = 0.0010251998901367188 (2331.92 steps/sec)\n",
      "INFO:tensorflow:global step 51000: loss = 2.09808349609375e-05 (2333.46 steps/sec)\n",
      "INFO:tensorflow:global step 51100: loss = 0.0113983154296875 (2335.01 steps/sec)\n",
      "INFO:tensorflow:global step 51200: loss = 0.01137542724609375 (2336.57 steps/sec)\n",
      "INFO:tensorflow:global step 51300: loss = 0.00013303756713867188 (2338.15 steps/sec)\n",
      "INFO:tensorflow:global step 51400: loss = 0.0008711814880371094 (2339.67 steps/sec)\n",
      "INFO:tensorflow:global step 51500: loss = 1.9669532775878906e-06 (2341.2 steps/sec)\n",
      "INFO:tensorflow:global step 51600: loss = 1.1920928955078125e-07 (2342.69 steps/sec)\n",
      "INFO:tensorflow:global step 51700: loss = 2.9206275939941406e-06 (2344.25 steps/sec)\n",
      "INFO:tensorflow:global step 51800: loss = 5.900859832763672e-06 (2345.76 steps/sec)\n",
      "INFO:tensorflow:global step 51900: loss = 0.01611328125 (2347.29 steps/sec)\n",
      "INFO:tensorflow:global step 52000: loss = 0.0014171600341796875 (2348.82 steps/sec)\n",
      "INFO:tensorflow:global step 52100: loss = 4.589557647705078e-05 (2350.34 steps/sec)\n",
      "INFO:tensorflow:global step 52200: loss = 0.00012540817260742188 (2351.84 steps/sec)\n",
      "INFO:tensorflow:global step 52300: loss = 2.9802322387695312e-06 (2353.41 steps/sec)\n",
      "INFO:tensorflow:global step 52400: loss = 0.0008139610290527344 (2354.93 steps/sec)\n",
      "INFO:tensorflow:global step 52500: loss = 1.9073486328125e-06 (2356.48 steps/sec)\n",
      "INFO:tensorflow:global step 52600: loss = 0.0006389617919921875 (2357.96 steps/sec)\n",
      "INFO:tensorflow:global step 52700: loss = 0.00037860870361328125 (2359.51 steps/sec)\n",
      "INFO:tensorflow:global step 52800: loss = 9.953975677490234e-06 (2361.0 steps/sec)\n",
      "INFO:tensorflow:global step 52900: loss = 0.01373291015625 (2362.53 steps/sec)\n",
      "INFO:tensorflow:global step 53000: loss = 1.3113021850585938e-06 (2364.01 steps/sec)\n",
      "INFO:tensorflow:global step 53100: loss = 8.046627044677734e-06 (2365.5 steps/sec)\n",
      "INFO:tensorflow:global step 53200: loss = 2.390146255493164e-05 (2366.99 steps/sec)\n",
      "INFO:tensorflow:global step 53300: loss = 0.00018489360809326172 (2368.5 steps/sec)\n",
      "INFO:tensorflow:global step 53400: loss = 7.677078247070312e-05 (2369.97 steps/sec)\n",
      "INFO:tensorflow:global step 53500: loss = 6.496906280517578e-06 (2371.47 steps/sec)\n",
      "INFO:tensorflow:global step 53600: loss = 3.6954879760742188e-06 (2372.89 steps/sec)\n",
      "INFO:tensorflow:global step 53700: loss = 0.00013065338134765625 (2374.39 steps/sec)\n",
      "INFO:tensorflow:global step 53800: loss = 5.960464477539063e-08 (2375.85 steps/sec)\n",
      "INFO:tensorflow:global step 53900: loss = 0.0011644363403320312 (2377.34 steps/sec)\n",
      "INFO:tensorflow:global step 54000: loss = 3.2186508178710938e-06 (2378.8 steps/sec)\n",
      "INFO:tensorflow:global step 54100: loss = 5.364418029785156e-06 (2380.22 steps/sec)\n",
      "INFO:tensorflow:global step 54200: loss = 0.0010471343994140625 (2381.61 steps/sec)\n",
      "INFO:tensorflow:global step 54300: loss = 0.01378631591796875 (2383.04 steps/sec)\n",
      "INFO:tensorflow:global step 54400: loss = 0.0001838207244873047 (2384.52 steps/sec)\n",
      "INFO:tensorflow:global step 54500: loss = 0.000518798828125 (2385.95 steps/sec)\n",
      "INFO:tensorflow:global step 54600: loss = 0.0001531839370727539 (2387.41 steps/sec)\n",
      "INFO:tensorflow:global step 54700: loss = 0.011962890625 (2388.86 steps/sec)\n",
      "INFO:tensorflow:global step 54800: loss = 4.2557716369628906e-05 (2390.35 steps/sec)\n",
      "INFO:tensorflow:global step 54900: loss = 1.2159347534179688e-05 (2391.78 steps/sec)\n",
      "INFO:tensorflow:global step 55000: loss = 1.0669231414794922e-05 (2393.24 steps/sec)\n",
      "INFO:tensorflow:global step 55100: loss = 0.0078887939453125 (2394.62 steps/sec)\n",
      "INFO:tensorflow:global step 55200: loss = 2.5033950805664062e-06 (2396.07 steps/sec)\n",
      "INFO:tensorflow:global step 55300: loss = 1.7881393432617188e-07 (2397.49 steps/sec)\n",
      "INFO:tensorflow:global step 55400: loss = 6.324052810668945e-05 (2398.94 steps/sec)\n",
      "INFO:tensorflow:global step 55500: loss = 0.00689697265625 (2400.34 steps/sec)\n",
      "INFO:tensorflow:global step 55600: loss = 1.1920928955078125e-07 (2401.73 steps/sec)\n",
      "INFO:tensorflow:global step 55700: loss = 0.0023021697998046875 (2403.13 steps/sec)\n",
      "INFO:tensorflow:global step 55800: loss = 0.024017333984375 (2404.55 steps/sec)\n",
      "INFO:tensorflow:global step 55900: loss = 3.2186508178710938e-06 (2405.93 steps/sec)\n",
      "INFO:tensorflow:global step 56000: loss = 0.04351806640625 (2407.36 steps/sec)\n",
      "INFO:tensorflow:global step 56100: loss = 0.00030159950256347656 (2408.68 steps/sec)\n",
      "INFO:tensorflow:global step 56200: loss = 0.00042057037353515625 (2410.08 steps/sec)\n",
      "INFO:tensorflow:global step 56300: loss = 1.0073184967041016e-05 (2411.45 steps/sec)\n",
      "INFO:tensorflow:global step 56400: loss = 0.0092620849609375 (2412.86 steps/sec)\n",
      "INFO:tensorflow:global step 56500: loss = 0.0104217529296875 (2414.23 steps/sec)\n",
      "INFO:tensorflow:global step 56600: loss = 2.4437904357910156e-05 (2415.59 steps/sec)\n",
      "INFO:tensorflow:global step 56700: loss = 1.0132789611816406e-06 (2416.96 steps/sec)\n",
      "INFO:tensorflow:global step 56800: loss = 4.76837158203125e-07 (2418.34 steps/sec)\n",
      "INFO:tensorflow:global step 56900: loss = 0.00641632080078125 (2419.7 steps/sec)\n",
      "INFO:tensorflow:global step 57000: loss = 0.002166748046875 (2421.09 steps/sec)\n",
      "INFO:tensorflow:global step 57100: loss = 0.0027618408203125 (2422.39 steps/sec)\n",
      "INFO:tensorflow:global step 57200: loss = 0.021148681640625 (2423.77 steps/sec)\n",
      "INFO:tensorflow:global step 57300: loss = 1.8477439880371094e-06 (2425.09 steps/sec)\n",
      "INFO:tensorflow:global step 57400: loss = 0.0 (2426.46 steps/sec)\n",
      "INFO:tensorflow:global step 57500: loss = 0.00011068582534790039 (2427.74 steps/sec)\n",
      "INFO:tensorflow:global step 57600: loss = 0.00278472900390625 (2429.04 steps/sec)\n",
      "INFO:tensorflow:global step 57700: loss = 0.0009379386901855469 (2430.32 steps/sec)\n",
      "INFO:tensorflow:global step 57800: loss = 2.473592758178711e-05 (2431.67 steps/sec)\n",
      "INFO:tensorflow:global step 57900: loss = 0.00037288665771484375 (2432.97 steps/sec)\n",
      "INFO:tensorflow:global step 58000: loss = 6.556510925292969e-07 (2434.29 steps/sec)\n",
      "INFO:tensorflow:global step 58100: loss = 0.03143310546875 (2435.58 steps/sec)\n",
      "INFO:tensorflow:global step 58200: loss = 0.01061248779296875 (2436.92 steps/sec)\n",
      "INFO:tensorflow:global step 58300: loss = 0.0130615234375 (2438.2 steps/sec)\n",
      "INFO:tensorflow:global step 58400: loss = 0.00130462646484375 (2439.5 steps/sec)\n",
      "INFO:tensorflow:global step 58500: loss = 1.0728836059570312e-06 (2440.83 steps/sec)\n",
      "INFO:tensorflow:global step 58600: loss = 4.172325134277344e-06 (2442.07 steps/sec)\n",
      "INFO:tensorflow:global step 58700: loss = 7.152557373046875e-06 (2443.4 steps/sec)\n",
      "INFO:tensorflow:global step 58800: loss = 0.004673004150390625 (2444.69 steps/sec)\n",
      "INFO:tensorflow:global step 58900: loss = 0.01189422607421875 (2446.03 steps/sec)\n",
      "INFO:tensorflow:global step 59000: loss = 0.0030345916748046875 (2447.3 steps/sec)\n",
      "INFO:tensorflow:global step 59100: loss = 0.00023889541625976562 (2448.49 steps/sec)\n",
      "INFO:tensorflow:global step 59200: loss = 9.5367431640625e-07 (2449.77 steps/sec)\n",
      "INFO:tensorflow:global step 59300: loss = 1.8537044525146484e-05 (2451.09 steps/sec)\n",
      "INFO:tensorflow:global step 59400: loss = 7.331371307373047e-06 (2452.36 steps/sec)\n",
      "INFO:tensorflow:global step 59500: loss = 1.7285346984863281e-06 (2453.67 steps/sec)\n",
      "INFO:tensorflow:global step 59600: loss = 2.2232532501220703e-05 (2454.89 steps/sec)\n",
      "INFO:tensorflow:global step 59700: loss = 0.00435638427734375 (2456.2 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 59800: loss = 2.980232238769531e-07 (2457.46 steps/sec)\n",
      "INFO:tensorflow:global step 59900: loss = 0.00860595703125 (2458.78 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 60000...\n",
      "INFO:tensorflow:Saving checkpoints for 60000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 60000...\n",
      "INFO:tensorflow:global step 60000: loss = 5.960464477539063e-08 (2316.0 steps/sec)\n",
      "INFO:tensorflow:global step 60100: loss = 2.205371856689453e-06 (2317.24 steps/sec)\n",
      "INFO:tensorflow:global step 60200: loss = 4.172325134277344e-07 (2318.53 steps/sec)\n",
      "INFO:tensorflow:global step 60300: loss = 3.5762786865234375e-07 (2319.89 steps/sec)\n",
      "INFO:tensorflow:global step 60400: loss = 7.152557373046875e-07 (2321.09 steps/sec)\n",
      "INFO:tensorflow:global step 60500: loss = 0.00225067138671875 (2322.42 steps/sec)\n",
      "INFO:tensorflow:global step 60600: loss = 0.01532745361328125 (2323.65 steps/sec)\n",
      "INFO:tensorflow:global step 60700: loss = 0.0006518363952636719 (2324.96 steps/sec)\n",
      "INFO:tensorflow:global step 60800: loss = 1.7881393432617188e-07 (2326.26 steps/sec)\n",
      "INFO:tensorflow:global step 60900: loss = 0.0004611015319824219 (2327.57 steps/sec)\n",
      "INFO:tensorflow:global step 61000: loss = 3.2186508178710938e-06 (2328.84 steps/sec)\n",
      "INFO:tensorflow:global step 61100: loss = 0.00041556358337402344 (2330.11 steps/sec)\n",
      "INFO:tensorflow:global step 61200: loss = 2.574920654296875e-05 (2331.38 steps/sec)\n",
      "INFO:tensorflow:global step 61300: loss = 0.00689697265625 (2332.67 steps/sec)\n",
      "INFO:tensorflow:global step 61400: loss = 1.7881393432617188e-07 (2333.92 steps/sec)\n",
      "INFO:tensorflow:global step 61500: loss = 8.702278137207031e-06 (2335.22 steps/sec)\n",
      "INFO:tensorflow:global step 61600: loss = 0.00015020370483398438 (2336.46 steps/sec)\n",
      "INFO:tensorflow:global step 61700: loss = 5.960464477539062e-07 (2337.77 steps/sec)\n",
      "INFO:tensorflow:global step 61800: loss = 0.0010986328125 (2339.04 steps/sec)\n",
      "INFO:tensorflow:global step 61900: loss = 0.00070953369140625 (2340.33 steps/sec)\n",
      "INFO:tensorflow:global step 62000: loss = 4.649162292480469e-06 (2341.59 steps/sec)\n",
      "INFO:tensorflow:global step 62100: loss = 0.00019812583923339844 (2342.86 steps/sec)\n",
      "INFO:tensorflow:global step 62200: loss = 0.0 (2344.12 steps/sec)\n",
      "INFO:tensorflow:global step 62300: loss = 2.771615982055664e-05 (2345.3 steps/sec)\n",
      "INFO:tensorflow:global step 62400: loss = 9.5367431640625e-07 (2346.54 steps/sec)\n",
      "INFO:tensorflow:global step 62500: loss = 1.1563301086425781e-05 (2347.8 steps/sec)\n",
      "INFO:tensorflow:global step 62600: loss = 0.00013911724090576172 (2349.06 steps/sec)\n",
      "INFO:tensorflow:global step 62700: loss = 0.044342041015625 (2350.32 steps/sec)\n",
      "INFO:tensorflow:global step 62800: loss = 0.0001571178436279297 (2351.6 steps/sec)\n",
      "INFO:tensorflow:global step 62900: loss = 6.377696990966797e-06 (2352.85 steps/sec)\n",
      "INFO:tensorflow:global step 63000: loss = 2.6285648345947266e-05 (2354.12 steps/sec)\n",
      "INFO:tensorflow:global step 63100: loss = 0.0002923011779785156 (2355.34 steps/sec)\n",
      "INFO:tensorflow:global step 63200: loss = 2.855062484741211e-05 (2356.59 steps/sec)\n",
      "INFO:tensorflow:global step 63300: loss = 7.671117782592773e-05 (2357.81 steps/sec)\n",
      "INFO:tensorflow:global step 63400: loss = 0.005359649658203125 (2359.08 steps/sec)\n",
      "INFO:tensorflow:global step 63500: loss = 0.0022106170654296875 (2360.31 steps/sec)\n",
      "INFO:tensorflow:global step 63600: loss = 8.940696716308594e-07 (2361.53 steps/sec)\n",
      "INFO:tensorflow:global step 63700: loss = 6.556510925292969e-07 (2362.76 steps/sec)\n",
      "INFO:tensorflow:global step 63800: loss = 8.52346420288086e-06 (2364.0 steps/sec)\n",
      "INFO:tensorflow:global step 63900: loss = 0.00046253204345703125 (2365.21 steps/sec)\n",
      "INFO:tensorflow:global step 64000: loss = 1.0132789611816406e-06 (2366.47 steps/sec)\n",
      "INFO:tensorflow:global step 64100: loss = 2.980232238769531e-07 (2367.55 steps/sec)\n",
      "INFO:tensorflow:global step 64200: loss = 0.00911712646484375 (2368.79 steps/sec)\n",
      "INFO:tensorflow:global step 64300: loss = 2.384185791015625e-07 (2370.0 steps/sec)\n",
      "INFO:tensorflow:global step 64400: loss = 0.02655029296875 (2371.25 steps/sec)\n",
      "INFO:tensorflow:global step 64500: loss = 0.001434326171875 (2372.46 steps/sec)\n",
      "INFO:tensorflow:global step 64600: loss = 0.01396942138671875 (2373.63 steps/sec)\n",
      "INFO:tensorflow:global step 64700: loss = 0.00011473894119262695 (2374.83 steps/sec)\n",
      "INFO:tensorflow:global step 64800: loss = 7.903575897216797e-05 (2376.06 steps/sec)\n",
      "INFO:tensorflow:global step 64900: loss = 7.766485214233398e-05 (2377.26 steps/sec)\n",
      "INFO:tensorflow:global step 65000: loss = 2.5928020477294922e-05 (2378.5 steps/sec)\n",
      "INFO:tensorflow:global step 65100: loss = 1.3649463653564453e-05 (2379.66 steps/sec)\n",
      "INFO:tensorflow:global step 65200: loss = 0.01473236083984375 (2380.89 steps/sec)\n",
      "INFO:tensorflow:global step 65300: loss = 5.364418029785156e-07 (2382.07 steps/sec)\n",
      "INFO:tensorflow:global step 65400: loss = 3.7729740142822266e-05 (2383.26 steps/sec)\n",
      "INFO:tensorflow:global step 65500: loss = 4.708766937255859e-06 (2384.44 steps/sec)\n",
      "INFO:tensorflow:global step 65600: loss = 0.0005254745483398438 (2385.62 steps/sec)\n",
      "INFO:tensorflow:global step 65700: loss = 9.47713851928711e-06 (2386.8 steps/sec)\n",
      "INFO:tensorflow:global step 65800: loss = 0.0002467632293701172 (2388.02 steps/sec)\n",
      "INFO:tensorflow:global step 65900: loss = 0.0001175999641418457 (2389.2 steps/sec)\n",
      "INFO:tensorflow:global step 66000: loss = 9.059906005859375e-05 (2390.41 steps/sec)\n",
      "INFO:tensorflow:global step 66100: loss = 6.4373016357421875e-06 (2391.56 steps/sec)\n",
      "INFO:tensorflow:global step 66200: loss = 0.006130218505859375 (2392.76 steps/sec)\n",
      "INFO:tensorflow:global step 66300: loss = 1.1920928955078125e-07 (2393.9 steps/sec)\n",
      "INFO:tensorflow:global step 66400: loss = 0.000270843505859375 (2395.09 steps/sec)\n",
      "INFO:tensorflow:global step 66500: loss = 0.0194549560546875 (2396.27 steps/sec)\n",
      "INFO:tensorflow:global step 66600: loss = 0.0006275177001953125 (2397.45 steps/sec)\n",
      "INFO:tensorflow:global step 66700: loss = 4.458427429199219e-05 (2398.62 steps/sec)\n",
      "INFO:tensorflow:global step 66800: loss = 7.62939453125e-06 (2399.77 steps/sec)\n",
      "INFO:tensorflow:global step 66900: loss = 1.5139579772949219e-05 (2400.96 steps/sec)\n",
      "INFO:tensorflow:global step 67000: loss = 5.960464477539063e-08 (2402.11 steps/sec)\n",
      "INFO:tensorflow:global step 67100: loss = 0.0 (2403.28 steps/sec)\n",
      "INFO:tensorflow:global step 67200: loss = 3.135204315185547e-05 (2404.43 steps/sec)\n",
      "INFO:tensorflow:global step 67300: loss = 3.2782554626464844e-06 (2405.61 steps/sec)\n",
      "INFO:tensorflow:global step 67400: loss = 0.0001245737075805664 (2406.76 steps/sec)\n",
      "INFO:tensorflow:global step 67500: loss = 1.3113021850585938e-06 (2407.94 steps/sec)\n",
      "INFO:tensorflow:global step 67600: loss = 0.00023543834686279297 (2409.05 steps/sec)\n",
      "INFO:tensorflow:global step 67700: loss = 0.01404571533203125 (2410.22 steps/sec)\n",
      "INFO:tensorflow:global step 67800: loss = 1.9609928131103516e-05 (2411.36 steps/sec)\n",
      "INFO:tensorflow:global step 67900: loss = 0.0162200927734375 (2412.53 steps/sec)\n",
      "INFO:tensorflow:global step 68000: loss = 0.004608154296875 (2413.67 steps/sec)\n",
      "INFO:tensorflow:global step 68100: loss = 0.00101470947265625 (2414.81 steps/sec)\n",
      "INFO:tensorflow:global step 68200: loss = 5.364418029785156e-05 (2415.95 steps/sec)\n",
      "INFO:tensorflow:global step 68300: loss = 0.0035076141357421875 (2417.12 steps/sec)\n",
      "INFO:tensorflow:global step 68400: loss = 0.0306854248046875 (2418.24 steps/sec)\n",
      "INFO:tensorflow:global step 68500: loss = 1.2874603271484375e-05 (2419.4 steps/sec)\n",
      "INFO:tensorflow:global step 68600: loss = 0.0003981590270996094 (2420.51 steps/sec)\n",
      "INFO:tensorflow:global step 68700: loss = 2.7000904083251953e-05 (2421.65 steps/sec)\n",
      "INFO:tensorflow:global step 68800: loss = 0.0004978179931640625 (2422.77 steps/sec)\n",
      "INFO:tensorflow:global step 68900: loss = 0.0180206298828125 (2423.92 steps/sec)\n",
      "INFO:tensorflow:global step 69000: loss = 5.066394805908203e-06 (2425.04 steps/sec)\n",
      "INFO:tensorflow:global step 69100: loss = 7.152557373046875e-07 (2426.16 steps/sec)\n",
      "INFO:tensorflow:global step 69200: loss = 2.3245811462402344e-06 (2427.28 steps/sec)\n",
      "INFO:tensorflow:global step 69300: loss = 0.002071380615234375 (2428.42 steps/sec)\n",
      "INFO:tensorflow:global step 69400: loss = 0.0030612945556640625 (2429.53 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 69500: loss = 2.0742416381835938e-05 (2430.67 steps/sec)\n",
      "INFO:tensorflow:global step 69600: loss = 2.384185791015625e-07 (2431.75 steps/sec)\n",
      "INFO:tensorflow:global step 69700: loss = 7.843971252441406e-05 (2432.89 steps/sec)\n",
      "INFO:tensorflow:global step 69800: loss = 2.562999725341797e-06 (2433.99 steps/sec)\n",
      "INFO:tensorflow:global step 69900: loss = 1.2516975402832031e-06 (2435.12 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 70000...\n",
      "INFO:tensorflow:Saving checkpoints for 70000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 70000...\n",
      "INFO:tensorflow:global step 70000: loss = 0.0005993843078613281 (2317.81 steps/sec)\n",
      "INFO:tensorflow:global step 70100: loss = 2.9206275939941406e-06 (2318.87 steps/sec)\n",
      "INFO:tensorflow:global step 70200: loss = 4.750490188598633e-05 (2319.97 steps/sec)\n",
      "INFO:tensorflow:global step 70300: loss = 0.00018978118896484375 (2321.14 steps/sec)\n",
      "INFO:tensorflow:global step 70400: loss = 0.00443267822265625 (2322.26 steps/sec)\n",
      "INFO:tensorflow:global step 70500: loss = 3.49879264831543e-05 (2323.42 steps/sec)\n",
      "INFO:tensorflow:global step 70600: loss = 0.00023293495178222656 (2324.53 steps/sec)\n",
      "INFO:tensorflow:global step 70700: loss = 1.9073486328125e-06 (2325.69 steps/sec)\n",
      "INFO:tensorflow:global step 70800: loss = 5.960464477539063e-08 (2326.77 steps/sec)\n",
      "INFO:tensorflow:global step 70900: loss = 0.00173187255859375 (2327.92 steps/sec)\n",
      "INFO:tensorflow:global step 71000: loss = 4.172325134277344e-07 (2329.07 steps/sec)\n",
      "INFO:tensorflow:global step 71100: loss = 0.0004336833953857422 (2330.18 steps/sec)\n",
      "INFO:tensorflow:global step 71200: loss = 8.940696716308594e-07 (2331.35 steps/sec)\n",
      "INFO:tensorflow:global step 71300: loss = 0.0030918121337890625 (2332.47 steps/sec)\n",
      "INFO:tensorflow:global step 71400: loss = 2.1457672119140625e-06 (2333.64 steps/sec)\n",
      "INFO:tensorflow:global step 71500: loss = 0.0002338886260986328 (2334.76 steps/sec)\n",
      "INFO:tensorflow:global step 71600: loss = 0.03564453125 (2335.89 steps/sec)\n",
      "INFO:tensorflow:global step 71700: loss = 4.172325134277344e-07 (2337.0 steps/sec)\n",
      "INFO:tensorflow:global step 71800: loss = 0.0013256072998046875 (2338.15 steps/sec)\n",
      "INFO:tensorflow:global step 71900: loss = 1.9669532775878906e-06 (2339.27 steps/sec)\n",
      "INFO:tensorflow:global step 72000: loss = 0.0204010009765625 (2340.41 steps/sec)\n",
      "INFO:tensorflow:global step 72100: loss = 5.960464477539063e-08 (2341.51 steps/sec)\n",
      "INFO:tensorflow:global step 72200: loss = 0.0002593994140625 (2342.66 steps/sec)\n",
      "INFO:tensorflow:global step 72300: loss = 0.058990478515625 (2343.77 steps/sec)\n",
      "INFO:tensorflow:global step 72400: loss = 2.980232238769531e-07 (2344.91 steps/sec)\n",
      "INFO:tensorflow:global step 72500: loss = 0.0003046989440917969 (2346.02 steps/sec)\n",
      "INFO:tensorflow:global step 72600: loss = 4.029273986816406e-05 (2347.13 steps/sec)\n",
      "INFO:tensorflow:global step 72700: loss = 3.5762786865234375e-07 (2348.22 steps/sec)\n",
      "INFO:tensorflow:global step 72800: loss = 0.0 (2349.35 steps/sec)\n",
      "INFO:tensorflow:global step 72900: loss = 1.9490718841552734e-05 (2350.42 steps/sec)\n",
      "INFO:tensorflow:global step 73000: loss = 0.0489501953125 (2351.53 steps/sec)\n",
      "INFO:tensorflow:global step 73100: loss = 5.960464477539062e-07 (2352.54 steps/sec)\n",
      "INFO:tensorflow:global step 73200: loss = 3.039836883544922e-06 (2353.59 steps/sec)\n",
      "INFO:tensorflow:global step 73300: loss = 1.3947486877441406e-05 (2354.62 steps/sec)\n",
      "INFO:tensorflow:global step 73400: loss = 2.562999725341797e-06 (2355.7 steps/sec)\n",
      "INFO:tensorflow:global step 73500: loss = 5.346536636352539e-05 (2356.74 steps/sec)\n",
      "INFO:tensorflow:global step 73600: loss = 9.059906005859375e-06 (2357.77 steps/sec)\n",
      "INFO:tensorflow:global step 73700: loss = 9.441375732421875e-05 (2358.82 steps/sec)\n",
      "INFO:tensorflow:global step 73800: loss = 0.051177978515625 (2359.91 steps/sec)\n",
      "INFO:tensorflow:global step 73900: loss = 1.430511474609375e-06 (2360.94 steps/sec)\n",
      "INFO:tensorflow:global step 74000: loss = 0.0005950927734375 (2362.0 steps/sec)\n",
      "INFO:tensorflow:global step 74100: loss = 3.212690353393555e-05 (2363.0 steps/sec)\n",
      "INFO:tensorflow:global step 74200: loss = 0.018798828125 (2364.07 steps/sec)\n",
      "INFO:tensorflow:global step 74300: loss = 0.00016188621520996094 (2365.12 steps/sec)\n",
      "INFO:tensorflow:global step 74400: loss = 2.3126602172851562e-05 (2366.2 steps/sec)\n",
      "INFO:tensorflow:global step 74500: loss = 0.0026645660400390625 (2367.22 steps/sec)\n",
      "INFO:tensorflow:global step 74600: loss = 0.0 (2368.27 steps/sec)\n",
      "INFO:tensorflow:global step 74700: loss = 1.436471939086914e-05 (2369.3 steps/sec)\n",
      "INFO:tensorflow:global step 74800: loss = 3.2782554626464844e-06 (2370.37 steps/sec)\n",
      "INFO:tensorflow:global step 74900: loss = 0.0230560302734375 (2371.4 steps/sec)\n",
      "INFO:tensorflow:global step 75000: loss = 1.3709068298339844e-06 (2372.44 steps/sec)\n",
      "INFO:tensorflow:global step 75100: loss = 8.285045623779297e-06 (2373.49 steps/sec)\n",
      "INFO:tensorflow:global step 75200: loss = 2.568960189819336e-05 (2374.53 steps/sec)\n",
      "INFO:tensorflow:global step 75300: loss = 0.0 (2375.59 steps/sec)\n",
      "INFO:tensorflow:global step 75400: loss = 2.1457672119140625e-06 (2376.61 steps/sec)\n",
      "INFO:tensorflow:global step 75500: loss = 0.0169525146484375 (2377.67 steps/sec)\n",
      "INFO:tensorflow:global step 75600: loss = 0.03387451171875 (2378.66 steps/sec)\n",
      "INFO:tensorflow:global step 75700: loss = 0.033660888671875 (2379.71 steps/sec)\n",
      "INFO:tensorflow:global step 75800: loss = 5.960464477539063e-08 (2380.73 steps/sec)\n",
      "INFO:tensorflow:global step 75900: loss = 2.3305416107177734e-05 (2381.77 steps/sec)\n",
      "INFO:tensorflow:global step 76000: loss = 0.0 (2382.79 steps/sec)\n",
      "INFO:tensorflow:global step 76100: loss = 0.00763702392578125 (2383.81 steps/sec)\n",
      "INFO:tensorflow:global step 76200: loss = 0.0005946159362792969 (2384.83 steps/sec)\n",
      "INFO:tensorflow:global step 76300: loss = 0.051177978515625 (2385.87 steps/sec)\n",
      "INFO:tensorflow:global step 76400: loss = 0.0 (2386.86 steps/sec)\n",
      "INFO:tensorflow:global step 76500: loss = 0.00012683868408203125 (2387.89 steps/sec)\n",
      "INFO:tensorflow:global step 76600: loss = 0.01372528076171875 (2388.88 steps/sec)\n",
      "INFO:tensorflow:global step 76700: loss = 4.76837158203125e-07 (2389.9 steps/sec)\n",
      "INFO:tensorflow:global step 76800: loss = 0.0080108642578125 (2390.91 steps/sec)\n",
      "INFO:tensorflow:global step 76900: loss = 1.430511474609375e-06 (2391.95 steps/sec)\n",
      "INFO:tensorflow:global step 77000: loss = 1.1920928955078125e-07 (2392.94 steps/sec)\n",
      "INFO:tensorflow:global step 77100: loss = 1.7881393432617188e-07 (2393.9 steps/sec)\n",
      "INFO:tensorflow:global step 77200: loss = 3.5762786865234375e-07 (2394.91 steps/sec)\n",
      "INFO:tensorflow:global step 77300: loss = 0.00124359130859375 (2395.93 steps/sec)\n",
      "INFO:tensorflow:global step 77400: loss = 3.5762786865234375e-07 (2396.92 steps/sec)\n",
      "INFO:tensorflow:global step 77500: loss = 0.0006632804870605469 (2397.94 steps/sec)\n",
      "INFO:tensorflow:global step 77600: loss = 0.0 (2398.91 steps/sec)\n",
      "INFO:tensorflow:global step 77700: loss = 6.556510925292969e-07 (2399.92 steps/sec)\n",
      "INFO:tensorflow:global step 77800: loss = 2.980232238769531e-07 (2400.9 steps/sec)\n",
      "INFO:tensorflow:global step 77900: loss = 0.02593994140625 (2401.92 steps/sec)\n",
      "INFO:tensorflow:global step 78000: loss = 0.0 (2402.9 steps/sec)\n",
      "INFO:tensorflow:global step 78100: loss = 6.556510925292969e-06 (2403.87 steps/sec)\n",
      "INFO:tensorflow:global step 78200: loss = 3.123283386230469e-05 (2404.86 steps/sec)\n",
      "INFO:tensorflow:global step 78300: loss = 4.547834396362305e-05 (2405.86 steps/sec)\n",
      "INFO:tensorflow:global step 78400: loss = 1.8298625946044922e-05 (2406.82 steps/sec)\n",
      "INFO:tensorflow:global step 78500: loss = 1.1324882507324219e-06 (2407.83 steps/sec)\n",
      "INFO:tensorflow:global step 78600: loss = 8.344650268554688e-07 (2408.78 steps/sec)\n",
      "INFO:tensorflow:global step 78700: loss = 8.344650268554688e-07 (2409.79 steps/sec)\n",
      "INFO:tensorflow:global step 78800: loss = 2.562999725341797e-06 (2410.76 steps/sec)\n",
      "INFO:tensorflow:global step 78900: loss = 0.0100860595703125 (2411.76 steps/sec)\n",
      "INFO:tensorflow:global step 79000: loss = 0.0160675048828125 (2412.72 steps/sec)\n",
      "INFO:tensorflow:global step 79100: loss = 1.430511474609375e-06 (2413.69 steps/sec)\n",
      "INFO:tensorflow:global step 79200: loss = 0.00012540817260742188 (2414.66 steps/sec)\n",
      "INFO:tensorflow:global step 79300: loss = 5.960464477539063e-08 (2415.63 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 79400: loss = 1.6093254089355469e-06 (2416.62 steps/sec)\n",
      "INFO:tensorflow:global step 79500: loss = 0.0006542205810546875 (2417.59 steps/sec)\n",
      "INFO:tensorflow:global step 79600: loss = 2.980232238769531e-07 (2418.55 steps/sec)\n",
      "INFO:tensorflow:global step 79700: loss = 0.031982421875 (2419.53 steps/sec)\n",
      "INFO:tensorflow:global step 79800: loss = 1.7881393432617188e-07 (2420.53 steps/sec)\n",
      "INFO:tensorflow:global step 79900: loss = 9.953975677490234e-06 (2322.13 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 80000...\n",
      "INFO:tensorflow:Saving checkpoints for 80000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 80000...\n",
      "INFO:tensorflow:global step 80000: loss = 0.0166168212890625 (2317.83 steps/sec)\n",
      "INFO:tensorflow:global step 80100: loss = 0.0 (2318.68 steps/sec)\n",
      "INFO:tensorflow:global step 80200: loss = 7.152557373046875e-07 (2319.69 steps/sec)\n",
      "INFO:tensorflow:global step 80300: loss = 0.0030059814453125 (2320.69 steps/sec)\n",
      "INFO:tensorflow:global step 80400: loss = 2.384185791015625e-07 (2321.66 steps/sec)\n",
      "INFO:tensorflow:global step 80500: loss = 1.239776611328125e-05 (2322.63 steps/sec)\n",
      "INFO:tensorflow:global step 80600: loss = 8.89897346496582e-05 (2323.62 steps/sec)\n",
      "INFO:tensorflow:global step 80700: loss = 5.960464477539063e-08 (2324.59 steps/sec)\n",
      "INFO:tensorflow:global step 80800: loss = 3.075599670410156e-05 (2325.59 steps/sec)\n",
      "INFO:tensorflow:global step 80900: loss = 0.0009102821350097656 (2326.57 steps/sec)\n",
      "INFO:tensorflow:global step 81000: loss = 5.4836273193359375e-06 (2327.56 steps/sec)\n",
      "INFO:tensorflow:global step 81100: loss = 0.03436279296875 (2328.53 steps/sec)\n",
      "INFO:tensorflow:global step 81200: loss = 1.3828277587890625e-05 (2329.53 steps/sec)\n",
      "INFO:tensorflow:global step 81300: loss = 0.0001399517059326172 (2330.5 steps/sec)\n",
      "INFO:tensorflow:global step 81400: loss = 2.0623207092285156e-05 (2331.49 steps/sec)\n",
      "INFO:tensorflow:global step 81500: loss = 7.909536361694336e-05 (2332.46 steps/sec)\n",
      "INFO:tensorflow:global step 81600: loss = 0.00044727325439453125 (2333.45 steps/sec)\n",
      "INFO:tensorflow:global step 81700: loss = 0.0013866424560546875 (2334.43 steps/sec)\n",
      "INFO:tensorflow:global step 81800: loss = 0.0026683807373046875 (2335.44 steps/sec)\n",
      "INFO:tensorflow:global step 81900: loss = 0.0002598762512207031 (2336.41 steps/sec)\n",
      "INFO:tensorflow:global step 82000: loss = 0.0082244873046875 (2337.4 steps/sec)\n",
      "INFO:tensorflow:global step 82100: loss = 1.1920928955078125e-07 (2338.35 steps/sec)\n",
      "INFO:tensorflow:global step 82200: loss = 2.6226043701171875e-06 (2339.34 steps/sec)\n",
      "INFO:tensorflow:global step 82300: loss = 3.5822391510009766e-05 (2340.32 steps/sec)\n",
      "INFO:tensorflow:global step 82400: loss = 0.0002524852752685547 (2341.31 steps/sec)\n",
      "INFO:tensorflow:global step 82500: loss = 8.344650268554688e-07 (2342.27 steps/sec)\n",
      "INFO:tensorflow:global step 82600: loss = 0.0038127899169921875 (2343.22 steps/sec)\n",
      "INFO:tensorflow:global step 82700: loss = 0.018951416015625 (2344.17 steps/sec)\n",
      "INFO:tensorflow:global step 82800: loss = 1.5079975128173828e-05 (2345.16 steps/sec)\n",
      "INFO:tensorflow:global step 82900: loss = 0.00054931640625 (2346.1 steps/sec)\n",
      "INFO:tensorflow:global step 83000: loss = 5.960464477539063e-08 (2347.08 steps/sec)\n",
      "INFO:tensorflow:global step 83100: loss = 2.771615982055664e-05 (2348.01 steps/sec)\n",
      "INFO:tensorflow:global step 83200: loss = 6.318092346191406e-06 (2348.99 steps/sec)\n",
      "INFO:tensorflow:global step 83300: loss = 2.980232238769531e-07 (2349.93 steps/sec)\n",
      "INFO:tensorflow:global step 83400: loss = 0.0162353515625 (2350.88 steps/sec)\n",
      "INFO:tensorflow:global step 83500: loss = 2.384185791015625e-07 (2351.86 steps/sec)\n",
      "INFO:tensorflow:global step 83600: loss = 3.337860107421875e-06 (2352.79 steps/sec)\n",
      "INFO:tensorflow:global step 83700: loss = 1.2695789337158203e-05 (2353.75 steps/sec)\n",
      "INFO:tensorflow:global step 83800: loss = 3.325939178466797e-05 (2354.7 steps/sec)\n",
      "INFO:tensorflow:global step 83900: loss = 3.5762786865234375e-07 (2355.67 steps/sec)\n",
      "INFO:tensorflow:global step 84000: loss = 0.00013184547424316406 (2356.6 steps/sec)\n",
      "INFO:tensorflow:global step 84100: loss = 9.417533874511719e-06 (2357.51 steps/sec)\n",
      "INFO:tensorflow:global step 84200: loss = 0.00024700164794921875 (2358.45 steps/sec)\n",
      "INFO:tensorflow:global step 84300: loss = 0.0016736984252929688 (2359.42 steps/sec)\n",
      "INFO:tensorflow:global step 84400: loss = 6.0439109802246094e-05 (2360.36 steps/sec)\n",
      "INFO:tensorflow:global step 84500: loss = 0.0 (2361.33 steps/sec)\n",
      "INFO:tensorflow:global step 84600: loss = 0.00925445556640625 (2362.24 steps/sec)\n",
      "INFO:tensorflow:global step 84700: loss = 1.0132789611816406e-06 (2363.19 steps/sec)\n",
      "INFO:tensorflow:global step 84800: loss = 0.01180267333984375 (2364.11 steps/sec)\n",
      "INFO:tensorflow:global step 84900: loss = 0.0 (2365.05 steps/sec)\n",
      "INFO:tensorflow:global step 85000: loss = 4.649162292480469e-06 (2365.96 steps/sec)\n",
      "INFO:tensorflow:global step 85100: loss = 5.960464477539063e-08 (2366.89 steps/sec)\n",
      "INFO:tensorflow:global step 85200: loss = 0.00519561767578125 (2367.8 steps/sec)\n",
      "INFO:tensorflow:global step 85300: loss = 0.0001289844512939453 (2368.73 steps/sec)\n",
      "INFO:tensorflow:global step 85400: loss = 0.009429931640625 (2369.58 steps/sec)\n",
      "INFO:tensorflow:global step 85500: loss = 0.005290985107421875 (2370.47 steps/sec)\n",
      "INFO:tensorflow:global step 85600: loss = 1.7881393432617188e-07 (2371.29 steps/sec)\n",
      "INFO:tensorflow:global step 85700: loss = 0.02130126953125 (2372.18 steps/sec)\n",
      "INFO:tensorflow:global step 85800: loss = 2.980232238769531e-07 (2373.04 steps/sec)\n",
      "INFO:tensorflow:global step 85900: loss = 0.0 (2373.95 steps/sec)\n",
      "INFO:tensorflow:global step 86000: loss = 2.384185791015625e-07 (2374.8 steps/sec)\n",
      "INFO:tensorflow:global step 86100: loss = 2.6047229766845703e-05 (2375.67 steps/sec)\n",
      "INFO:tensorflow:global step 86200: loss = 0.03704833984375 (2376.53 steps/sec)\n",
      "INFO:tensorflow:global step 86300: loss = 0.00014829635620117188 (2377.44 steps/sec)\n",
      "INFO:tensorflow:global step 86400: loss = 3.5762786865234375e-07 (2378.3 steps/sec)\n",
      "INFO:tensorflow:global step 86500: loss = 0.0 (2379.2 steps/sec)\n",
      "INFO:tensorflow:global step 86600: loss = 3.159046173095703e-05 (2380.03 steps/sec)\n",
      "INFO:tensorflow:global step 86700: loss = 8.004903793334961e-05 (2380.93 steps/sec)\n",
      "INFO:tensorflow:global step 86800: loss = 0.0 (2381.77 steps/sec)\n",
      "INFO:tensorflow:global step 86900: loss = 5.960464477539062e-07 (2382.68 steps/sec)\n",
      "INFO:tensorflow:global step 87000: loss = 6.347894668579102e-05 (2383.56 steps/sec)\n",
      "INFO:tensorflow:global step 87100: loss = 1.9669532775878906e-06 (2384.44 steps/sec)\n",
      "INFO:tensorflow:global step 87200: loss = 8.702278137207031e-06 (2385.31 steps/sec)\n",
      "INFO:tensorflow:global step 87300: loss = 0.0 (2386.23 steps/sec)\n",
      "INFO:tensorflow:global step 87400: loss = 4.172325134277344e-07 (2387.1 steps/sec)\n",
      "INFO:tensorflow:global step 87500: loss = 1.329183578491211e-05 (2387.98 steps/sec)\n",
      "INFO:tensorflow:global step 87600: loss = 6.270408630371094e-05 (2388.85 steps/sec)\n",
      "INFO:tensorflow:global step 87700: loss = 6.556510925292969e-07 (2389.73 steps/sec)\n",
      "INFO:tensorflow:global step 87800: loss = 2.2172927856445312e-05 (2390.62 steps/sec)\n",
      "INFO:tensorflow:global step 87900: loss = 0.0003764629364013672 (2391.49 steps/sec)\n",
      "INFO:tensorflow:global step 88000: loss = 1.7881393432617188e-07 (2392.39 steps/sec)\n",
      "INFO:tensorflow:global step 88100: loss = 1.7881393432617188e-07 (2393.23 steps/sec)\n",
      "INFO:tensorflow:global step 88200: loss = 2.384185791015625e-07 (2394.13 steps/sec)\n",
      "INFO:tensorflow:global step 88300: loss = 0.0 (2394.99 steps/sec)\n",
      "INFO:tensorflow:global step 88400: loss = 2.0265579223632812e-06 (2395.88 steps/sec)\n",
      "INFO:tensorflow:global step 88500: loss = 2.2530555725097656e-05 (2396.74 steps/sec)\n",
      "INFO:tensorflow:global step 88600: loss = 4.291534423828125e-06 (2397.6 steps/sec)\n",
      "INFO:tensorflow:global step 88700: loss = 0.0228424072265625 (2398.47 steps/sec)\n",
      "INFO:tensorflow:global step 88800: loss = 5.364418029785156e-07 (2399.35 steps/sec)\n",
      "INFO:tensorflow:global step 88900: loss = 1.5556812286376953e-05 (2400.21 steps/sec)\n",
      "INFO:tensorflow:global step 89000: loss = 0.007251739501953125 (2401.1 steps/sec)\n",
      "INFO:tensorflow:global step 89100: loss = 6.67572021484375e-06 (2401.94 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 89200: loss = 5.364418029785156e-07 (2402.83 steps/sec)\n",
      "INFO:tensorflow:global step 89300: loss = 1.0013580322265625e-05 (2403.69 steps/sec)\n",
      "INFO:tensorflow:global step 89400: loss = 6.496906280517578e-06 (2404.56 steps/sec)\n",
      "INFO:tensorflow:global step 89500: loss = 0.0323486328125 (2405.42 steps/sec)\n",
      "INFO:tensorflow:global step 89600: loss = 9.5367431640625e-07 (2406.27 steps/sec)\n",
      "INFO:tensorflow:global step 89700: loss = 0.0002884864807128906 (2407.12 steps/sec)\n",
      "INFO:tensorflow:global step 89800: loss = 0.00012540817260742188 (2408.01 steps/sec)\n",
      "INFO:tensorflow:global step 89900: loss = 0.00981903076171875 (2321.38 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 90000...\n",
      "INFO:tensorflow:Saving checkpoints for 90000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 90000...\n",
      "INFO:tensorflow:global step 90000: loss = 0.02374267578125 (2316.67 steps/sec)\n",
      "INFO:tensorflow:global step 90100: loss = 0.0012989044189453125 (2317.36 steps/sec)\n",
      "INFO:tensorflow:global step 90200: loss = 4.76837158203125e-07 (2318.26 steps/sec)\n",
      "INFO:tensorflow:global step 90300: loss = 2.384185791015625e-07 (2319.12 steps/sec)\n",
      "INFO:tensorflow:global step 90400: loss = 1.0132789611816406e-05 (2320.03 steps/sec)\n",
      "INFO:tensorflow:global step 90500: loss = 6.556510925292969e-07 (2320.9 steps/sec)\n",
      "INFO:tensorflow:global step 90600: loss = 0.0225982666015625 (2321.78 steps/sec)\n",
      "INFO:tensorflow:global step 90700: loss = 8.52346420288086e-06 (2322.65 steps/sec)\n",
      "INFO:tensorflow:global step 90800: loss = 4.172325134277344e-07 (2323.55 steps/sec)\n",
      "INFO:tensorflow:global step 90900: loss = 2.980232238769531e-07 (2324.41 steps/sec)\n",
      "INFO:tensorflow:global step 91000: loss = 0.0 (2325.29 steps/sec)\n",
      "INFO:tensorflow:global step 91100: loss = 5.435943603515625e-05 (2326.13 steps/sec)\n",
      "INFO:tensorflow:global step 91200: loss = 0.0013885498046875 (2327.0 steps/sec)\n",
      "INFO:tensorflow:global step 91300: loss = 0.0005340576171875 (2327.87 steps/sec)\n",
      "INFO:tensorflow:global step 91400: loss = 0.0003981590270996094 (2328.74 steps/sec)\n",
      "INFO:tensorflow:global step 91500: loss = 5.2809715270996094e-05 (2329.61 steps/sec)\n",
      "INFO:tensorflow:global step 91600: loss = 3.6954879760742188e-06 (2330.47 steps/sec)\n",
      "INFO:tensorflow:global step 91700: loss = 0.0205535888671875 (2331.32 steps/sec)\n",
      "INFO:tensorflow:global step 91800: loss = 0.0002651214599609375 (2332.19 steps/sec)\n",
      "INFO:tensorflow:global step 91900: loss = 0.0145416259765625 (2333.07 steps/sec)\n",
      "INFO:tensorflow:global step 92000: loss = 1.043081283569336e-05 (2333.93 steps/sec)\n",
      "INFO:tensorflow:global step 92100: loss = 0.0206756591796875 (2334.8 steps/sec)\n",
      "INFO:tensorflow:global step 92200: loss = 0.0 (2335.67 steps/sec)\n",
      "INFO:tensorflow:global step 92300: loss = 0.0172119140625 (2336.55 steps/sec)\n",
      "INFO:tensorflow:global step 92400: loss = 0.00626373291015625 (2337.38 steps/sec)\n",
      "INFO:tensorflow:global step 92500: loss = 4.4345855712890625e-05 (2338.24 steps/sec)\n",
      "INFO:tensorflow:global step 92600: loss = 7.18235969543457e-05 (2339.04 steps/sec)\n",
      "INFO:tensorflow:global step 92700: loss = 2.384185791015625e-07 (2339.91 steps/sec)\n",
      "INFO:tensorflow:global step 92800: loss = 1.9073486328125e-06 (2340.74 steps/sec)\n",
      "INFO:tensorflow:global step 92900: loss = 0.0 (2341.61 steps/sec)\n",
      "INFO:tensorflow:global step 93000: loss = 0.007415771484375 (2342.46 steps/sec)\n",
      "INFO:tensorflow:global step 93100: loss = 0.0 (2343.3 steps/sec)\n",
      "INFO:tensorflow:global step 93200: loss = 1.430511474609375e-06 (2344.14 steps/sec)\n",
      "INFO:tensorflow:global step 93300: loss = 0.0010471343994140625 (2345.01 steps/sec)\n",
      "INFO:tensorflow:global step 93400: loss = 0.0 (2345.84 steps/sec)\n",
      "INFO:tensorflow:global step 93500: loss = 0.00015044212341308594 (2346.68 steps/sec)\n",
      "INFO:tensorflow:global step 93600: loss = 1.0669231414794922e-05 (2347.45 steps/sec)\n",
      "INFO:tensorflow:global step 93700: loss = 0.002262115478515625 (2348.29 steps/sec)\n",
      "INFO:tensorflow:global step 93800: loss = 0.0 (2349.1 steps/sec)\n",
      "INFO:tensorflow:global step 93900: loss = 1.1920928955078125e-07 (2349.95 steps/sec)\n",
      "INFO:tensorflow:global step 94000: loss = 0.001621246337890625 (2350.75 steps/sec)\n",
      "INFO:tensorflow:global step 94100: loss = 0.0 (2351.57 steps/sec)\n",
      "INFO:tensorflow:global step 94200: loss = 1.1324882507324219e-06 (2352.33 steps/sec)\n",
      "INFO:tensorflow:global step 94300: loss = 0.0 (2353.16 steps/sec)\n",
      "INFO:tensorflow:global step 94400: loss = 1.0728836059570312e-06 (2353.94 steps/sec)\n",
      "INFO:tensorflow:global step 94500: loss = 0.0323486328125 (2354.77 steps/sec)\n",
      "INFO:tensorflow:global step 94600: loss = 7.802248001098633e-05 (2355.58 steps/sec)\n",
      "INFO:tensorflow:global step 94700: loss = 6.377696990966797e-06 (2356.43 steps/sec)\n",
      "INFO:tensorflow:global step 94800: loss = 0.0007987022399902344 (2357.26 steps/sec)\n",
      "INFO:tensorflow:global step 94900: loss = 2.5033950805664062e-06 (2358.12 steps/sec)\n",
      "INFO:tensorflow:global step 95000: loss = 3.516674041748047e-06 (2358.94 steps/sec)\n",
      "INFO:tensorflow:global step 95100: loss = 2.6226043701171875e-06 (2359.78 steps/sec)\n",
      "INFO:tensorflow:global step 95200: loss = 0.0018291473388671875 (2360.61 steps/sec)\n",
      "INFO:tensorflow:global step 95300: loss = 2.980232238769531e-07 (2361.46 steps/sec)\n",
      "INFO:tensorflow:global step 95400: loss = 0.0 (2362.29 steps/sec)\n",
      "INFO:tensorflow:global step 95500: loss = 5.960464477539062e-07 (2363.12 steps/sec)\n",
      "INFO:tensorflow:global step 95600: loss = 5.960464477539063e-08 (2363.91 steps/sec)\n",
      "INFO:tensorflow:global step 95700: loss = 0.00011622905731201172 (2364.74 steps/sec)\n",
      "INFO:tensorflow:global step 95800: loss = 1.8477439880371094e-06 (2365.56 steps/sec)\n",
      "INFO:tensorflow:global step 95900: loss = 0.00013136863708496094 (2366.32 steps/sec)\n",
      "INFO:tensorflow:global step 96000: loss = 4.315376281738281e-05 (2367.14 steps/sec)\n",
      "INFO:tensorflow:global step 96100: loss = 5.960464477539063e-08 (2367.92 steps/sec)\n",
      "INFO:tensorflow:global step 96200: loss = 0.002704620361328125 (2368.73 steps/sec)\n",
      "INFO:tensorflow:global step 96300: loss = 0.00024378299713134766 (2369.52 steps/sec)\n",
      "INFO:tensorflow:global step 96400: loss = 1.1324882507324219e-06 (2370.33 steps/sec)\n",
      "INFO:tensorflow:global step 96500: loss = 7.152557373046875e-07 (2371.12 steps/sec)\n",
      "INFO:tensorflow:global step 96600: loss = 5.960464477539063e-08 (2371.89 steps/sec)\n",
      "INFO:tensorflow:global step 96700: loss = 2.6226043701171875e-06 (2372.68 steps/sec)\n",
      "INFO:tensorflow:global step 96800: loss = 5.960464477539062e-07 (2373.5 steps/sec)\n",
      "INFO:tensorflow:global step 96900: loss = 0.00016188621520996094 (2374.29 steps/sec)\n",
      "INFO:tensorflow:global step 97000: loss = 0.0 (2375.1 steps/sec)\n",
      "INFO:tensorflow:global step 97100: loss = 0.002094268798828125 (2375.87 steps/sec)\n",
      "INFO:tensorflow:global step 97200: loss = 5.543231964111328e-06 (2376.69 steps/sec)\n",
      "INFO:tensorflow:global step 97300: loss = 5.960464477539062e-07 (2377.48 steps/sec)\n",
      "INFO:tensorflow:global step 97400: loss = 1.1324882507324219e-06 (2378.29 steps/sec)\n",
      "INFO:tensorflow:global step 97500: loss = 4.76837158203125e-07 (2379.08 steps/sec)\n",
      "INFO:tensorflow:global step 97600: loss = 0.0005693435668945312 (2379.86 steps/sec)\n",
      "INFO:tensorflow:global step 97700: loss = 0.01432037353515625 (2380.65 steps/sec)\n",
      "INFO:tensorflow:global step 97800: loss = 0.0 (2381.45 steps/sec)\n",
      "INFO:tensorflow:global step 97900: loss = 0.0 (2382.23 steps/sec)\n",
      "INFO:tensorflow:global step 98000: loss = 0.0010986328125 (2383.03 steps/sec)\n",
      "INFO:tensorflow:global step 98100: loss = 0.00018274784088134766 (2383.8 steps/sec)\n",
      "INFO:tensorflow:global step 98200: loss = 4.172325134277344e-07 (2384.6 steps/sec)\n",
      "INFO:tensorflow:global step 98300: loss = 0.0048675537109375 (2385.39 steps/sec)\n",
      "INFO:tensorflow:global step 98400: loss = 1.9669532775878906e-06 (2386.18 steps/sec)\n",
      "INFO:tensorflow:global step 98500: loss = 0.00827789306640625 (2386.96 steps/sec)\n",
      "INFO:tensorflow:global step 98600: loss = 1.5497207641601562e-06 (2387.74 steps/sec)\n",
      "INFO:tensorflow:global step 98700: loss = 7.152557373046875e-07 (2388.51 steps/sec)\n",
      "INFO:tensorflow:global step 98800: loss = 1.5020370483398438e-05 (2389.31 steps/sec)\n",
      "INFO:tensorflow:global step 98900: loss = 0.0 (2390.09 steps/sec)\n",
      "INFO:tensorflow:global step 99000: loss = 8.803606033325195e-05 (2390.88 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 99100: loss = 6.496906280517578e-06 (2391.64 steps/sec)\n",
      "INFO:tensorflow:global step 99200: loss = 1.9431114196777344e-05 (2392.43 steps/sec)\n",
      "INFO:tensorflow:global step 99300: loss = 2.5391578674316406e-05 (2393.2 steps/sec)\n",
      "INFO:tensorflow:global step 99400: loss = 2.6941299438476562e-05 (2394.0 steps/sec)\n",
      "INFO:tensorflow:global step 99500: loss = 1.7881393432617188e-06 (2394.77 steps/sec)\n",
      "INFO:tensorflow:global step 99600: loss = 2.4437904357910156e-06 (2395.55 steps/sec)\n",
      "INFO:tensorflow:global step 99700: loss = 0.0016317367553710938 (2396.31 steps/sec)\n",
      "INFO:tensorflow:global step 99800: loss = 0.0008220672607421875 (2397.1 steps/sec)\n",
      "INFO:tensorflow:global step 99900: loss = 4.172325134277344e-06 (2397.86 steps/sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 100000...\n",
      "INFO:tensorflow:Saving checkpoints for 100000 into tutorial_model_dir/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 100000...\n",
      "INFO:tensorflow:Training finished with 25600000 samples in 41.691 seconds, 614039.55 samples/second.\n",
      "INFO:tensorflow:Loss for final step: 2e-07.\n",
      "=============== Starting Cerebras Compilation ===============\n",
      "=============== Cerebras Compilation Completed ===============\n",
      "salloc: Relinquishing job allocation 283014\n"
     ]
    }
   ],
   "source": [
    "set_cs_ip_addr_value()\n",
    "!salloc ${SLURM_GRES_ARGUMENT} ${SLURM_ARGUMENTS} --nodelist=${NODE_ID} srun /usr/bin/singularity exec --bind ${BIND_LOCATIONS} --pwd ${YOUR_ENTRY_SCRIPT_LOCATION} ${CEREBRAS_CONTAINER} python run.py --mode train --model_dir tutorial_model_dir --cs_ip ${CS_IP_ADDR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41bb42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example of the expected output (final lines):\n",
    "\n",
    "    SLURM environment variables have been set successfully.\n",
    "    srun: job 1234 queued and waiting for resources\n",
    "    srun: job 1234 has been allocated resources\n",
    "    INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': ['sdf:29231'], 'worker': ['sdf:29233', 'sdf:29235', 'sdf:29237', 'sdf:29239', 'sdf:29241', 'sdf:29243']}, 'task': {'type': 'chief', 'index': 0}}\n",
    "    INFO:root:Running train on CS-2\n",
    "\n",
    "    [--- OUTPUT SNIPPED FOR KEEPING THIS EXAMPLE SHORT ---]\n",
    "\n",
    "    INFO:tensorflow:global step 99700: loss = 4.76837158203125e-07 (2277.17 steps/sec)\n",
    "    INFO:tensorflow:global step 99800: loss = 0.0 (2277.98 steps/sec)\n",
    "    INFO:tensorflow:global step 99900: loss = 5.304813385009766e-06 (2278.76 steps/sec)\n",
    "    INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 100000...\n",
    "    INFO:tensorflow:Saving checkpoints for 100000 into tutorial_model_dir/model.ckpt.\n",
    "    INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 100000...\n",
    "    INFO:tensorflow:Training finished with 25600000 samples in 43.869 seconds, 583554.35 samples/second.\n",
    "    INFO:tensorflow:Loss for final step: 0.06384.\n",
    "    =============== Starting Cerebras Compilation ===============\n",
    "    =============== Cerebras Compilation Completed ===============\n",
    "\n",
    "---\n",
    "\n",
    "This third command also executed a job using `srun` and `singularity`, but now using the actual training mode that utilizes the CS system.\n",
    "\n",
    "For the Singularity/Apptainer arguments, the following argument changed:\n",
    "* **\\${SLURM_GRES_ARGUMENT}**: the actual value is `--gres=cs:cerebras:1`, and it requests a CS machine as a special resource to be used for the Slurm job training the model. If this flag were not to be used, the CS would not be allocated for the job (unavailable).\n",
    "* **--cs_ip ${CS_IP_ADDR}**: this value is set when a CS machine has been requested (using `--gres=cs:cerebras:1`). It will point to the IP address of the CS machine mapped to the specific compute node running the training. This value is dynamic and changes as required by the system administrators.\n",
    "\n",
    "For the Python run.py file, the following argument changed:\n",
    "* **--mode train**: no additional `_only` arguments were used, just the `--mode train` argument to start the training on the CS system using the compiled executable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059822d",
   "metadata": {},
   "source": [
    "## Closing notes\n",
    "\n",
    "Congratulations on successfully training an FC-MNIST model on the Neocortex system! Throughout this tutorial, you have gained hands-on experience in performing all of the steps required for running a reference example from scratch on a Cerebras CS-2 machine.\n",
    "\n",
    "This example is based on the [Cerebras Documentation](https://docs.cerebras.net/en/1.6.0/). \n",
    "\n",
    "Other links of interest are:\n",
    "\n",
    "* [Neocortex System](https://www.cmu.edu/psc/aibd/neocortex/)\n",
    "* [Cerebras ML Workflow](https://docs.cerebras.net/en/1.6.0/cerebras-basics/cs-ml-workflow.html)\n",
    "* [Neocortex Documentation](https://portal.neocortex.psc.edu/docs/)\n",
    "\n",
    "This material is based upon work supported by the [National Science Foundation under Grant Number 2005597](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2005597).\n",
    "\n",
    "## Next steps (optional):\n",
    "\n",
    "\n",
    "Neocortex provides a unique opportunity to access the remarkable integrated technologies of the Cerebras CS-2 and the HPE Superdome Flex Servers available in PSC's Neocortex system. \n",
    "\n",
    "We invite you to run your research on Neocortex. You could first, identify the track that your project belongs to, then answer some general questions, and then finally, apply for accessing the system over a full-fledged research grant.\n",
    "\n",
    "You can take a look at the [previous Neocortex Call for Proposals page](https://www.cmu.edu/psc/aibd/neocortex/2023-03-cfp-spring-2023.html) for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
